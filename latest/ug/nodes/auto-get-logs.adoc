//!!NODE_ROOT <section>
[.topic]
[[auto-get-logs,auto-get-logs.title]]
= Retrieve node logs for a managed node using kubectl and S3
:info_titleabbrev: Get node logs

include::../attributes.txt[]

[abstract]
--
Learn how to retrieve node logs for an Amazon EKS managed node that has the node monitoring agent.
--

Learn how to retrieve node logs for an Amazon EKS managed node that has the node monitoring agent.

== Prerequisites

Make sure you have the following:

* An existing Amazon EKS cluster with the node monitoring agent. For more information, see <<node-health>>.
* The `kubectl` command-line tool installed and configured to
communicate with your cluster.
* The {aws} CLI installed and logged in with sufficent permissions to
create S3 buckets and objects.
* A recent version of Python 3 installed 
* The {aws} SDK for Python 3, Boto 3, installed. 

== Step 1: Create S3 bucket destination (optional)

If you don't already have an S3 bucket to store the logs, create one. Use the following {aws} CLI command. The bucket defaults to the `private` access control list. Replace [.replaceable]`bucket-name` with your chosen unique bucket name.

[source,bash,subs="verbatim,attributes,quotes"]
----
aws s3api create-bucket --bucket [.replaceable]`bucket-name`
----

== Step 2: Create pre-signed S3 URL for HTTP Put

Amazon EKS returns the node logs by doing a HTTP PUT operation to a URL you specify. In this tutorial, we will generate a pre-signed S3 HTTP PUT URL. 

The logs will be returned as a gzip tarball, with the `.tar.gz` extension.

[NOTE]
====
You must use the {aws} API or a SDK to create the pre-signed S3 upload URL for EKS to upload the log file. You cannot create a pre-signed S3 upload URL using the {aws} CLI.  
====

. Determine where in the bucket you want to store the logs. For example, you might use `2024-11-12/logs1.tar.gz` as the key. 
. Save the following Python code to the file `presign-upload.py`. Replace `<bucket-name>` and `<key>`. The key should end with `.tar.gz`.
+
[source,python,subs="verbatim,attributes"]
----
import boto3; print(boto3.client('s3').generate_presigned_url(
   ClientMethod='put_object',
   Params={'Bucket': '<bucket-name>', 'Key': '<key>'},
   ExpiresIn=1000
))
----
. Run the script with
+
[source,cli]
----
python presign-upload.py
----
. Note the URL output. Use this value in the next step as the [.replaceacble]`http-put-destination`.


For more information, see https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-presigned-urls.html#generating-a-presigned-url-to-upload-a-file[Generate a presigned URL to upload a file] in the {aws} Boto3 SDK for Python Documentation.

== Step 3: Create NodeDiagnostic resource

Identify the name of the node you want to collect logs from.

Create a `NodeDiagnostic` manifest that uses the name of the node as the
resource's name, and providing a HTTP PUT URL destination. 

[source,bash,subs="verbatim,attributes,quotes"]
----
apiVersion: eks.amazonaws.com/v1alpha1
kind: NodeDiagnostic
metadata:
    name: [.replaceable]`node-name`
spec:
    logCapture:
        destination: [.replaceable]`http-put-destination`
----

Apply the manifest to the cluster.

[source,bash,subs="verbatim,attributes"]
----
kubectl apply -f nodediagnostic.yaml
----

You can check on the Status of the collection by describing the
`NodeDiagnostic` resource:

* A status of `Success` or `SuccessWithErrors` indicates that the task
  completed and the logs uploaded to the provided destination
  (`SuccessWithErrors` indicates that some logs might be missing)
* If the status is Failure, confirm the upload URL is well-formed and not expired.

[source,bash,subs="verbatim,attributes,quotes"]
----
kubectl describe nodediagnostics.eks.amazonaws.com/[.replaceable]`node-name`
----

== Step 4: Download logs from S3

Wait approximately one minute before attempting to download the logs. Then, use the S3 CLI to download the logs. 

[source,bash,subs="verbatim,attributes,quotes"]
----
# Once NodeDiagnostic shows Success status, download the logs
aws s3 cp s3://[.replaceable]`bucket-name`/[.replaceable]`key` ./node-logs.tar.gz
----

== Step 5: Clean up NodeDiagnostic resource

* `NodeDiagnostic` resources do not get automatically deleted. You
should clean these up on your own after you have obtained your log
artifacts

[source,bash,subs="verbatim,attributes,quotes"]
----
# Delete the NodeDiagnostic resource
kubectl delete nodediagnostics.eks.amazonaws.com/[.replaceable]`node-name`
----
