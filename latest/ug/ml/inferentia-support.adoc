//!!NODE_ROOT <section>
include::../attributes.txt[]

[.topic]
[[inferentia-support,inferentia-support.title]]
= Use {aws} [.noloc]`Inferentia` instances with Amazon EKS for Machine Learning
:info_doctype: section
:info_title: Use {aws} Inferentia instances with your EKS cluster for Machine Learning 
:info_titleabbrev: Prepare Inferentia clusters
:info_abstract: Learn how to create an Amazon EKS cluster with nodes running Amazon EC2 Inf1 instances for machine learning inference using {aws} Inferentia chips and deploy a TensorFlow Serving application.

[abstract]
--
Learn how to create an Amazon EKS cluster with nodes running Amazon EC2 Inf1 instances for machine learning inference using {aws} Inferentia chips and deploy a TensorFlow Serving application.
--

This topic describes how to create an Amazon EKS cluster with nodes running link:ec2/instance-types/inf1/[Amazon EC2 Inf1,type="marketing"] instances and (optionally) deploy a sample application. Amazon EC2 Inf1 instances are powered by link:machine-learning/inferentia/[{aws} Inferentia,type="marketing"] chips, which are custom built by {aws} to provide high performance and lowest cost inference in the cloud. Machine learning models are deployed to containers using link:machine-learning/neuron/[{aws} Neuron,type="marketing"], a specialized software development kit (SDK) consisting of a compiler, runtime, and profiling tools that optimize the machine learning inference performance of Inferentia chips. {aws} Neuron supports popular machine learning frameworks such as TensorFlow, PyTorch, and MXNet.

[NOTE]
====

Neuron device logical IDs must be contiguous. If a [.noloc]`Pod` requesting multiple Neuron devices is scheduled on an `inf1.6xlarge` or `inf1.24xlarge` instance type (which have more than one Neuron device), that [.noloc]`Pod` will fail to start if the [.noloc]`Kubernetes` scheduler selects non-contiguous device IDs. For more information, see https://github.com/aws/aws-neuron-sdk/issues/110[Device logical IDs must be contiguous] on [.noloc]`GitHub`.

====

[[inferentia-prerequisites,inferentia-prerequisites.title]]
== Prerequisites

* Have `eksctl` installed on your computer. If you don't have it installed, see https://eksctl.io/installation[Installation] in the `eksctl` documentation.
* Have `kubectl` installed on your computer. For more information, see <<install-kubectl>>.
* (Optional) Have `python3` installed on your computer. If you don't have it installed, then see https://www.python.org/downloads/[Python downloads] for installation instructions.


[[create-cluster-inferentia,create-cluster-inferentia.title]]
== Create a cluster

. Create a cluster with Inf1 Amazon EC2 instance nodes. You can replace [.replaceable]`inf1.2xlarge` with any link:ec2/instance-types/inf1/[Inf1 instance type,type="marketing"]. The `eksctl` utility detects that you are launching a node group with an `Inf1` instance type and will start your nodes using one of the Amazon EKS optimized accelerated Amazon Linux AMIs. 
+
NOTE: You can't use <<iam-roles-for-service-accounts,IAM roles for service accounts>> with TensorFlow Serving.
+
[source,bash,subs="verbatim,attributes,quotes"]
----
eksctl create cluster \
    --name inferentia \
    --region region-code \
    --nodegroup-name ng-inf1 \
    --node-type inf1.2xlarge \
    --nodes 2 \
    --nodes-min 1 \
    --nodes-max 4 \
    --ssh-access \
    --ssh-public-key your-key \
    --with-oidc
----
+
NOTE: Note the value of the following line of the output. It's used in a later (optional) step.
+
[source,bash,subs="verbatim,attributes,quotes"]
----
[9]  adding identity "{arn-aws}iam::111122223333:role/eksctl-inferentia-nodegroup-ng-in-NodeInstanceRole-FI7HIYS3BS09" to auth ConfigMap
----
+
When launching a node group with `Inf1` instances, `eksctl` automatically installs the {aws} Neuron [.noloc]`Kubernetes` device plugin. This plugin advertises Neuron devices as a system resource to the [.noloc]`Kubernetes` scheduler, which can be requested by a container. In addition to the default Amazon EKS node IAM policies, the Amazon S3 read only access policy is added so that the sample application, covered in a later step, can load a trained model from Amazon S3.
. Make sure that all [.noloc]`Pods` have started correctly.
+
[source,bash,subs="verbatim,attributes,quotes"]
----
kubectl get pods -n kube-system
----
+
Abbreviated output:
+
[source,bash,subs="verbatim,attributes,quotes"]
----
NAME                                   READY   STATUS    RESTARTS   AGE
[...]
neuron-device-plugin-daemonset-6djhp   1/1     Running   0          5m
neuron-device-plugin-daemonset-hwjsj   1/1     Running   0          5m
----


[[deploy-tensorflow-serving-application,deploy-tensorflow-serving-application.title]]
== (Optional) Deploy a TensorFlow Serving application image

A trained model must be compiled to an Inferentia target before it can be deployed on Inferentia instances. To continue, you will need a https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/tensorflow-neuron/index.html[Neuron optimized TensorFlow] model saved in Amazon S3. If you don't already have a SavedModel, please follow the tutorial for link:dlami/latest/devguide/tutorial-inferentia-tf-neuron.html[creating a Neuron compatible ResNet50 model,type="documentation"] and upload the resulting SavedModel to S3. ResNet-50 is a popular machine learning model used for image recognition tasks. For more information about compiling Neuron models, see link:dlami/latest/devguide/tutorial-inferentia.html[The {aws} Inferentia Chip With DLAMI,type="documentation"] in the {aws} Deep Learning AMIs Developer Guide.

The sample deployment manifest manages a pre-built inference serving container for TensorFlow provided by {aws} Deep Learning Containers. Inside the container is the {aws} Neuron Runtime and the TensorFlow Serving application. A complete list of pre-built Deep Learning Containers optimized for Neuron is maintained on [.noloc]`GitHub` under https://github.com/aws/deep-learning-containers/blob/master/available_images.md#neuron-inference-containers[Available Images]. At start-up, the DLC will fetch your model from Amazon S3, launch Neuron TensorFlow Serving with the saved model, and wait for prediction requests.

The number of Neuron devices allocated to your serving application can be adjusted by changing the `aws.amazon.com/neuron` resource in the deployment yaml. Please note that communication between TensorFlow Serving and the Neuron runtime happens over GRPC, which requires passing the `IPC_LOCK` capability to the container.

. Add the `AmazonS3ReadOnlyAccess` IAM policy to the node instance role that was created in step 1 of <<create-cluster-inferentia,Create a cluster>>. This is necessary so that the sample application can load a trained model from Amazon S3.
+
[source,bash,subs="verbatim,attributes,quotes"]
----
aws iam attach-role-policy \
    --policy-arn {arn-aws}iam::aws:policy/AmazonS3ReadOnlyAccess \
    --role-name eksctl-inferentia-nodegroup-ng-in-NodeInstanceRole-FI7HIYS3BS09
----
. Create a file named `rn50_deployment.yaml` with the following contents. Update the region-code and model path to match your desired settings. The model name is for identification purposes when a client makes a request to the TensorFlow server. This example uses a model name to match a sample ResNet50 client script that will be used in a later step for sending prediction requests. 
+
[source,bash,subs="verbatim,attributes,quotes"]
----
aws ecr list-images --repository-name neuron-rtd --registry-id 790709498068 --region us-west-2
----
+
[source,yaml,subs="verbatim,attributes,quotes"]
----
kind: Deployment
apiVersion: apps/v1
metadata:
  name: eks-neuron-test
  labels:
    app: eks-neuron-test
    role: master
spec:
  replicas: 2
  selector:
    matchLabels:
      app: eks-neuron-test
      role: master
  template:
    metadata:
      labels:
        app: eks-neuron-test
        role: master
    spec:
      containers:
        - name: eks-neuron-test
          image: 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference-neuron:1.15.4-neuron-py37-ubuntu18.04
          command:
            - /usr/local/bin/entrypoint.sh
          args:
            - --port=8500
            - --rest_api_port=9000
            - --model_name=resnet50_neuron
            - --model_base_path=s3://your-bucket-of-models/resnet50_neuron/
          ports:
            - containerPort: 8500
            - containerPort: 9000
          imagePullPolicy: IfNotPresent
          env:
            - name: AWS_REGION
              value: "us-east-1"
            - name: S3_USE_HTTPS
              value: "1"
            - name: S3_VERIFY_SSL
              value: "0"
            - name: S3_ENDPOINT
              value: s3.us-east-1.amazonaws.com
            - name: AWS_LOG_LEVEL
              value: "3"
          resources:
            limits:
              cpu: 4
              memory: 4Gi
              aws.amazon.com/neuron: 1
            requests:
              cpu: "1"
              memory: 1Gi
          securityContext:
            capabilities:
              add:
                - IPC_LOCK
----
. Deploy the model.
+
[source,bash,subs="verbatim,attributes,quotes"]
----
kubectl apply -f rn50_deployment.yaml
----
. Create a file named `rn50_service.yaml` with the following contents. The HTTP and gRPC ports are opened for accepting prediction requests.
+
[source,yaml,subs="verbatim,attributes,quotes"]
----
kind: Service
apiVersion: v1
metadata:
  name: eks-neuron-test
  labels:
    app: eks-neuron-test
spec:
  type: ClusterIP
  ports:
    - name: http-tf-serving
      port: 8500
      targetPort: 8500
    - name: grpc-tf-serving
      port: 9000
      targetPort: 9000
  selector:
    app: eks-neuron-test
    role: master
----
. Create a [.noloc]`Kubernetes` service for your TensorFlow model Serving application.
+
[source,bash,subs="verbatim,attributes,quotes"]
----
kubectl apply -f rn50_service.yaml
----


[[make-predictions-against-tensorflow-service,make-predictions-against-tensorflow-service.title]]
== (Optional) Make predictions against your TensorFlow Serving service
. To test locally, forward the gRPC port to the `eks-neuron-test` service.
+
[source,bash,subs="verbatim,attributes,quotes"]
----
kubectl port-forward service/eks-neuron-test 8500:8500 &
----
. Create a Python script called `tensorflow-model-server-infer.py` with the following content. This script runs inference via gRPC, which is service framework.
+
[source,python,subs="verbatim,attributes,quotes"]
----
import numpy as np
   import grpc
   import tensorflow as tf
   from tensorflow.keras.preprocessing import image
   from tensorflow.keras.applications.resnet50 import preprocess_input
   from tensorflow_serving.apis import predict_pb2
   from tensorflow_serving.apis import prediction_service_pb2_grpc
   from tensorflow.keras.applications.resnet50 import decode_predictions
   
   if __name__ == '__main__':
       channel = grpc.insecure_channel('localhost:8500')
       stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)
       img_file = tf.keras.utils.get_file(
           "./kitten_small.jpg",
           "https://raw.githubusercontent.com/awslabs/mxnet-model-server/master/docs/images/kitten_small.jpg")
       img = image.load_img(img_file, target_size=(224, 224))
       img_array = preprocess_input(image.img_to_array(img)[None, ...])
       request = predict_pb2.PredictRequest()
       request.model_spec.name = 'resnet50_inf1'
       request.inputs['input'].CopyFrom(
           tf.make_tensor_proto(img_array, shape=img_array.shape))
       result = stub.Predict(request)
       prediction = tf.make_ndarray(result.outputs['output'])
       print(decode_predictions(prediction))
----
. Run the script to submit predictions to your service.
+
[source,bash,subs="verbatim,attributes,quotes"]
----
python3 tensorflow-model-server-infer.py
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes,quotes"]
----
[[(u'n02123045', u'tabby', 0.68817204), (u'n02127052', u'lynx', 0.12701613), (u'n02123159', u'tiger_cat', 0.08736559), (u'n02124075', u'Egyptian_cat', 0.063844085), (u'n02128757', u'snow_leopard', 0.009240591)]]
----
