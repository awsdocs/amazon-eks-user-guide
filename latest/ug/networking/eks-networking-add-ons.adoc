//!!NODE_ROOT <section>
[.topic]
[[eks-networking-add-ons,eks-networking-add-ons.title]]
= Manage networking add-ons for Amazon EKS clusters
:info_doctype: section
:info_title: Manage networking add-ons for Amazon EKS \
            clusters
:info_titleabbrev: Manage networking add-ons
:info_abstract: Learn how to manage networking add-ons for your Amazon EKS cluster, including built-in \
                components like Amazon VPC CNI plugin for Kubernetes, CoreDNS, and kube-proxy, as well as optional \
                {aws} add-ons for load balancing and service mesh.


include::../attributes.txt[]

[abstract]
--
Learn how to manage networking add-ons for your Amazon EKS cluster, including built-in components like [.noloc]`Amazon VPC CNI plugin for Kubernetes`, [.noloc]`CoreDNS`, and `kube-proxy`, as well as optional {aws} add-ons for load balancing and service mesh.
--

Several networking add-ons are available for your Amazon EKS cluster.

[[eks-networking-add-ons-built-in,eks-networking-add-ons-built-in.title]]
== Built-in add-ons

[NOTE:]
====
If you create clusters in any way except by using the console, each cluster comes with the self-managed versions of the built-in add-ons. The self-managed versions can't be managed from the {aws-management-console}, {aws} Command Line Interface, or SDKs. You manage the configuration and upgrades of self-managed add-ons.

We recommend adding the Amazon EKS type of the add-on to your cluster instead of using the self-managed type of the add-on. If you create clusters in the console, the Amazon EKS type of these add-ons is installed.
====

*[.noloc]`Amazon VPC CNI plugin for Kubernetes`*::
This CNI add-on creates elastic network interfaces and attaches them to your Amazon EC2 nodes. The add-on also assigns a private `IPv4` or `IPv6` address from your VPC to each [.noloc]`Pod` and service. This add-on is installed, by default, on your cluster. For more information, see <<managing-vpc-cni>>.


*[.noloc]`CoreDNS`*::
[.noloc]`CoreDNS` is a flexible, extensible DNS server that can serve as the [.noloc]`Kubernetes` cluster DNS. [.noloc]`CoreDNS` provides name resolution for all [.noloc]`Pods` in the cluster. This add-on is installed, by default, on your cluster. For more information, see <<managing-coredns>>.


*`kube-proxy`*::
This add-on maintains network rules on your Amazon EC2 nodes and enables network communication to your [.noloc]`Pods`. This add-on is installed, by default, on your cluster. For more information, see <<managing-kube-proxy>>.


[[eks-networking-add-ons-optional,eks-networking-add-ons-optional.title]]
== Optional {aws} networking add-ons

*[.noloc]`{aws} Load Balancer Controller`*::
When you deploy [.noloc]`Kubernetes` service objects of type `loadbalancer`, the controller creates {aws} Network Load Balancers . When you create [.noloc]`Kubernetes` ingress objects, the controller creates {aws} Application Load Balancers. We recommend using this controller to provision Network Load Balancers, rather than using the https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.7/guide/service/annotations/#legacy-cloud-provider[legacy Cloud Provider] controller built-in to [.noloc]`Kubernetes`. For more information, see the https://kubernetes-sigs.github.io/aws-load-balancer-controller[{aws} Load Balancer Controller] documentation.


*{aws} Gateway API Controller*::
This controller lets you connect services across multiple [.noloc]`Kubernetes` clusters using the https://gateway-api.sigs.k8s.io/[Kubernetes gateway API]. The controller connects [.noloc]`Kubernetes` services running on Amazon EC2 instances, containers, and serverless functions by using the link:vpc-lattice/latest/ug/what-is-vpc-service-network.html[Amazon VPC Lattice,type="documentation"] service. For more information, see the https://www.gateway-api-controller.eks.aws.dev/[{aws} Gateway API Controller] documentation.

For more information about add-ons, see <<eks-add-ons>>.

[.topic]
[[managing-vpc-cni,managing-vpc-cni.title]]
== Amazon VPC CNI
:info_title: Assign IPs to [.noloc]`Pods` with the Amazon VPC CNI
:info_titleabbrev: Amazon VPC CNI
:info_abstract: Discover how the [.noloc]`Amazon VPC CNI plugin for Kubernetes` add-on works to assign private IP addresses and create network interfaces for <noloc>Pods</noloc> and services in your Amazon EKS cluster.

[abstract]
--
Discover how the [.noloc]`Amazon VPC CNI plugin for Kubernetes` add-on works to assign private IP addresses and create network interfaces for [.noloc]`Pods` and services in your Amazon EKS cluster.
--

The [.noloc]`Amazon VPC CNI plugin for Kubernetes` add-on is deployed on each Amazon EC2 node in your Amazon EKS cluster. The add-on creates link:AWSEC2/latest/UserGuide/using-eni.html[elastic network interfaces,type="documentation"] and attaches them to your Amazon EC2 nodes. The add-on also assigns a private `IPv4` or `IPv6` address from your VPC to each [.noloc]`Pod`.

A version of the add-on is deployed with each Fargate node in your cluster, but you don't update it on Fargate nodes. Other compatible CNI plugins are available for use on Amazon EKS clusters, but this is the only CNI plugin supported by Amazon EKS. For more information about the other compatible CNI plugins, see <<alternate-cni-plugins>>.

The following table lists the latest available version of the Amazon EKS add-on type for each [.noloc]`Kubernetes` version.

[[vpc-cni-latest-available-version,vpc-cni-latest-available-version.title]]
=== [.noloc]`Amazon VPC CNI` versions

[options="header"]
|===
| Kubernetes version | Amazon EKS type of VPC CNI version
| 1.31 | v1.19.0-eksbuild.1
| 1.30 | v1.19.0-eksbuild.1
| 1.29 | v1.19.0-eksbuild.1
| 1.28 | v1.19.0-eksbuild.1
| 1.27 | v1.19.0-eksbuild.1
| 1.26 | v1.19.0-eksbuild.1
| 1.25 | v1.19.0-eksbuild.1
| 1.24 | v1.19.0-eksbuild.1
| 1.23 | v1.18.5-eksbuild.1
|===

[IMPORTANT]
====

If you're self-managing this add-on, the versions in the table might not be the same as the available self-managed versions. For more information about updating the self-managed type of this add-on, see <<vpc-add-on-self-managed-update>>.

====

[IMPORTANT]
====

To upgrade to VPC CNI v1.12.0 or later, you must upgrade to VPC CNI v1.7.0 first. We recommend that you update one minor version at a time.

====

[[manage-vpc-cni-add-on-on-considerations,manage-vpc-cni-add-on-on-considerations.title]]
=== Considerations

The following are considerations for using the feature.



* Versions are specified as `major-version.minor-version.patch-version-eksbuild.build-number`.
* Check version compatibility for each feature. Some features of each release of the [.noloc]`Amazon VPC CNI plugin for Kubernetes` require certain [.noloc]`Kubernetes` versions. When using different Amazon EKS features, if a specific version of the add-on is required, then it's noted in the feature documentation. Unless you have a specific reason for running an earlier version, we recommend running the latest version.


[.topic]
[[vpc-add-on-create,vpc-add-on-create.title]]
=== Creating the Amazon VPC CNI (Amazon EKS add-on)

Use the following steps to create the [.noloc]`Amazon VPC CNI plugin for Kubernetes` Amazon EKS add-on.

Before you begin, review the considerations. For more information, see <<manage-vpc-cni-add-on-on-considerations>>.

[[vpc-add-on-create-prerequisites,vpc-add-on-create-prerequisites.title]]
==== Prerequisites

The following are prerequisites for the [.noloc]`Amazon VPC CNI plugin for Kubernetes` Amazon EKS add-on.



* An existing Amazon EKS cluster. To deploy one, see <<getting-started>>.
* An existing {aws} Identity and Access Management (IAM) [.noloc]`OpenID Connect` ([.noloc]`OIDC`) provider for your cluster. To determine whether you already have one, or to create one, see <<enable-iam-roles-for-service-accounts>>.
* An IAM role with the  link:aws-managed-policy/latest/reference/AmazonEKS_CNI_Policy.html[AmazonEKS_CNI_Policy,type="documentation"] IAM policy (if your cluster uses the `IPv4` family) or an IPv6 policy (if your cluster uses the `IPv6` family) attached to it. For more information about the VPC CNI role, see <<cni-iam-role>>. For information about the IPv6 policy, see <<cni-iam-role-create-ipv6-policy>>.
* If you're using version `1.7.0` or later of the [.noloc]`Amazon VPC CNI plugin for Kubernetes` and you use custom [.noloc]`Pod` security policies, see <<psp-delete-default>> and <<pod-security-policy>>.

[IMPORTANT]
====

[.noloc]`Amazon VPC CNI plugin for Kubernetes` versions `v1.16.0` to `v1.16.1` removed compatibility with [.noloc]`Kubernetes` versions `1.23` and earlier. VPC CNI version `v1.16.2` restores compatibility with [.noloc]`Kubernetes` versions `1.23` and earlier and CNI spec `v0.4.0`.

[.noloc]`Amazon VPC CNI plugin for Kubernetes` versions `v1.16.0` to `v1.16.1` implement CNI specification version `v1.0.0`. CNI spec `v1.0.0` is supported on EKS clusters that run the [.noloc]`Kubernetes` versions `v1.24` or later. VPC CNI version `v1.16.0` to `v1.16.1` and CNI spec `v1.0.0` aren't supported on [.noloc]`Kubernetes` version `v1.23` or earlier. For more information about `v1.0.0` of the CNI spec, see https://github.com/containernetworking/cni/blob/spec-v1.0.0/SPEC.md[Container Network Interface (CNI) Specification] on [.noloc]`GitHub`.

====


[[vpc-add-on-create-procedure,vpc-add-on-create-procedure.title]]
==== Procedure

After you complete the prerequisites, use the following steps to create the add-on.

. See which version of the add-on is installed on your cluster.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl describe daemonset aws-node --namespace kube-system | grep amazon-k8s-cni: | cut -d : -f 3
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
v1.16.4-eksbuild.2
----
. See which type of the add-on is installed on your cluster. Depending on the tool that you created your cluster with, you might not currently have the Amazon EKS add-on type installed on your cluster. Replace [.replaceable]`my-cluster` with the name of your cluster.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks describe-addon --cluster-name my-cluster --addon-name vpc-cni --query addon.addonVersion --output text
----
+
If a version number is returned, you have the Amazon EKS type of the add-on installed on your cluster and don't need to complete the remaining steps in this procedure. If an error is returned, you don't have the Amazon EKS type of the add-on installed on your cluster. Complete the remaining steps of this procedure to install it.
. Save the configuration of your currently installed add-on.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl get daemonset aws-node -n kube-system -o yaml > aws-k8s-cni-old.yaml
----
. Create the add-on using the {aws} CLI. If you want to use the {aws-management-console} or `eksctl` to create the add-on, see <<creating-an-add-on>> and specify `vpc-cni` for the add-on name. Copy the command that follows to your device. Make the following modifications to the command, as needed, and then run the modified command.
+
** Replace [.replaceable]`my-cluster` with the name of your cluster.
** Replace [.replaceable]`v1.19.0-eksbuild.1` with the latest version listed in the latest version table for your cluster version. For the latest version table, see <<vpc-cni-latest-available-version>>.
** Replace [.replaceable]`111122223333` with your account ID and [.replaceable]`AmazonEKSVPCCNIRole` with the name of an <<cni-iam-role-create-role,existing IAM role>> that you've created. Specifying a role requires that you have an IAM [.noloc]`OpenID Connect` ([.noloc]`OIDC`) provider for your cluster. To determine whether you have one for your cluster, or to create one, see <<enable-iam-roles-for-service-accounts>>. 
+
[source,bash,subs="verbatim,attributes"]
----
aws eks create-addon --cluster-name my-cluster --addon-name vpc-cni --addon-version v1.19.0-eksbuild.1 \
    --service-account-role-arn {arn-aws}iam::111122223333:role/AmazonEKSVPCCNIRole
----
+
If you've applied custom settings to your current add-on that conflict with the default settings of the Amazon EKS add-on, creation might fail. If creation fails, you receive an error that can help you resolve the issue. Alternatively, you can add `--resolve-conflicts OVERWRITE` to the previous command. This allows the add-on to overwrite any existing custom settings. Once you've created the add-on, you can update it with your custom settings.
. Confirm that the latest version of the add-on for your cluster's [.noloc]`Kubernetes` version was added to your cluster. Replace [.replaceable]`my-cluster` with the name of your cluster.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks describe-addon --cluster-name my-cluster --addon-name vpc-cni --query addon.addonVersion --output text
----
+
It might take several seconds for add-on creation to complete.
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
v1.19.0-eksbuild.1
----
. If you made custom settings to your original add-on, before you created the Amazon EKS add-on, use the configuration that you saved in a previous step to update the EKS add-on with your custom settings. Follow the steps in <<vpc-add-on-update>>.
. (Optional) Install the `cni-metrics-helper` to your cluster. It scrapes elastic network interface and IP address information, aggregates it at a cluster level, and publishes the metrics to Amazon CloudWatch. For more information, see https://github.com/aws/amazon-vpc-cni-k8s/blob/master/cmd/cni-metrics-helper/README.md[cni-metrics-helper] on GitHub.


[.topic]
[[vpc-add-on-update,vpc-add-on-update.title]]
=== Updating the Amazon VPC CNI (Amazon EKS add-on)

Update the Amazon EKS type of the [.noloc]`Amazon VPC CNI plugin for Kubernetes` add-on. If you haven't added the Amazon EKS type of the add-on to your cluster, you can install it by following <<vpc-add-on-create>>. Or, update the other type of VPC CNI installation by following <<vpc-add-on-self-managed-update>>.

. See which version of the add-on is installed on your cluster. Replace [.replaceable]`my-cluster` with your cluster name.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks describe-addon --cluster-name my-cluster --addon-name vpc-cni --query "addon.addonVersion" --output text
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
v1.16.4-eksbuild.2
----
+
Compare the version with the table of latest versions at <<vpc-cni-latest-available-version>>. If the version returned is the same as the version for your cluster's [.noloc]`Kubernetes` version in the latest version table, then you already have the latest version installed on your cluster and don't need to complete the rest of this procedure. If you receive an error, instead of a version number in your output, then you don't have the Amazon EKS type of the add-on installed on your cluster. You need to create the add-on before you can update it with this procedure. To create the Amazon EKS type of the VPC CNI add-on, you can follow <<vpc-add-on-create>>.
. Save the configuration of your currently installed add-on.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl get daemonset aws-node -n kube-system -o yaml > aws-k8s-cni-old.yaml
----
. Update your add-on using the {aws} CLI. If you want to use the {aws-management-console} or `eksctl` to update the add-on, see <<updating-an-add-on>>. Copy the command that follows to your device. Make the following modifications to the command, as needed, and then run the modified command.
+
** Replace [.replaceable]`my-cluster` with the name of your cluster.
** Replace [.replaceable]`v1.19.0-eksbuild.1` with the latest version listed in the latest version table for your cluster version.
** Replace [.replaceable]`111122223333` with your account ID and [.replaceable]`AmazonEKSVPCCNIRole` with the name of an existing IAM role that you've created. To create an IAM role for the VPC CNI, see <<cni-iam-role-create-role>>. Specifying a role requires that you have an IAM [.noloc]`OpenID Connect` ([.noloc]`OIDC`) provider for your cluster. To determine whether you have one for your cluster, or to create one, see <<enable-iam-roles-for-service-accounts>>. 
** The `--resolve-conflicts PRESERVE` option preserves existing configuration values for the add-on. If you've set custom values for add-on settings, and you don't use this option, Amazon EKS overwrites your values with its default values. If you use this option, then we recommend testing any field and value changes on a non-production cluster before updating the add-on on your production cluster. If you change this value to `OVERWRITE`, all settings are changed to Amazon EKS default values. If you've set custom values for any settings, they might be overwritten with Amazon EKS default values. If you change this value to `none`, Amazon EKS doesn't change the value of any settings, but the update might fail. If the update fails, you receive an error message to help you resolve the conflict. 
** If you're not updating a configuration setting, remove ``--configuration-values '{[.replaceable]`"env":{"AWS_VPC_K8S_CNI_EXTERNALSNAT":"true"}`}'`` from the command. If you're updating a configuration setting, replace [.replaceable]`"env":{"AWS_VPC_K8S_CNI_EXTERNALSNAT":"true"}` with the setting that you want to set. In this example, the `AWS_VPC_K8S_CNI_EXTERNALSNAT` environment variable is set to `true`. The value that you specify must be valid for the configuration schema. If you don't know the configuration schema, run ``aws eks describe-addon-configuration --addon-name vpc-cni --addon-version [.replaceable]`v1.19.0-eksbuild.1```, replacing [.replaceable]`v1.19.0-eksbuild.1` with the version number of the add-on that you want to see the configuration for. The schema is returned in the output. If you have any existing custom configuration, want to remove it all, and set the values for all settings back to Amazon EKS defaults, remove [.replaceable]`"env":{"AWS_VPC_K8S_CNI_EXTERNALSNAT":"true"}` from the command, so that you have empty `{}`. For an explanation of each setting, see https://github.com/aws/amazon-vpc-cni-k8s#cni-configuration-variables[CNI Configuration Variables] on GitHub.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks update-addon --cluster-name my-cluster --addon-name vpc-cni --addon-version v1.19.0-eksbuild.1 \
    --service-account-role-arn {arn-aws}iam::111122223333:role/AmazonEKSVPCCNIRole \
    --resolve-conflicts PRESERVE --configuration-values '{"env":{"AWS_VPC_K8S_CNI_EXTERNALSNAT":"true"}}'
----
+
It might take several seconds for the update to complete.
. Confirm that the add-on version was updated. Replace [.replaceable]`my-cluster` with the name of your cluster.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks describe-addon --cluster-name my-cluster --addon-name vpc-cni
----
+
It might take several seconds for the update to complete.
+
An example output is as follows.
+
[source,json,subs="verbatim,attributes"]
----
{
    "addon": {
        "addonName": "vpc-cni",
        "clusterName": "my-cluster",
        "status": "ACTIVE",
        "addonVersion": "v1.19.0-eksbuild.1",
        "health": {
            "issues": []
        },
        "addonArn": "{arn-aws}eks:region:111122223333:addon/my-cluster/vpc-cni/74c33d2f-b4dc-8718-56e7-9fdfa65d14a9",
        "createdAt": "2023-04-12T18:25:19.319000+00:00",
        "modifiedAt": "2023-04-12T18:40:28.683000+00:00",
        "serviceAccountRoleArn": "{arn-aws}iam::111122223333:role/AmazonEKSVPCCNIRole",
        "tags": {},
        "configurationValues": "{\"env\":{\"AWS_VPC_K8S_CNI_EXTERNALSNAT\":\"true\"}}"
    }
}
----


[.topic]
[[vpc-add-on-self-managed-update,vpc-add-on-self-managed-update.title]]
=== Updating the Amazon VPC CNI (self-managed add-on)

[IMPORTANT]
====

We recommend adding the Amazon EKS type of the add-on to your cluster instead of using the self-managed type of the add-on. If you're not familiar with the difference between the types, see <<eks-add-ons>>. For more information about adding an Amazon EKS add-on to your cluster, see <<creating-an-add-on>>. If you're unable to use the Amazon EKS add-on, we encourage you to submit an issue about why you can't to the https://github.com/aws/containers-roadmap/issues[Containers roadmap GitHub repository].

====
. Confirm that you don't have the Amazon EKS type of the add-on installed on your cluster. Replace [.replaceable]`my-cluster` with the name of your cluster.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks describe-addon --cluster-name my-cluster --addon-name vpc-cni --query addon.addonVersion --output text
----
+
If an error message is returned, you don't have the Amazon EKS type of the add-on installed on your cluster. To self-manage the add-on, complete the remaining steps in this procedure to update the add-on. If a version number is returned, you have the Amazon EKS type of the add-on installed on your cluster. To update it, use the procedure in  <<updating-an-add-on>>, rather than using this procedure. If you're not familiar with the differences between the add-on types, see <<eks-add-ons>>.
. See which version of the container image is currently installed on your cluster.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl describe daemonset aws-node --namespace kube-system | grep amazon-k8s-cni: | cut -d : -f 3
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
v1.16.4-eksbuild.2
----
+
Your output might not include the build number.
. Backup your current settings so you can configure the same settings once you've updated your version.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl get daemonset aws-node -n kube-system -o yaml > aws-k8s-cni-old.yaml
----
To review the available versions and familiarize yourself with the changes in the version that you want to update to, see https://github.com/aws/amazon-vpc-cni-k8s/releases[releases] on [.noloc]`GitHub`. Note that we recommend updating to the same `major`.``minor``.``patch`` version listed in the latest available versions table, even if later versions are available on GitHub. For the latest available version table, see <<vpc-cni-latest-available-version>>. The build versions listed in the table aren't specified in the self-managed versions listed on GitHub. Update your version by completing the tasks in one of the following options:
+
** If you don't have any custom settings for the add-on, then run the command under the `To apply this release:` heading on GitHub for the https://github.com/aws/amazon-vpc-cni-k8s/releases[release] that you're updating to.
** If you have custom settings, download the manifest file with the following command. Change [.replaceable]`https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/v1.19.0/config/master/aws-k8s-cni.yaml` to the URL for the release on GitHub that you're updating to.
+
[source,bash,subs="verbatim,attributes"]
----
curl -O https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/v1.19.0/config/master/aws-k8s-cni.yaml
----
+
If necessary, modify the manifest with the custom settings from the backup you made in a previous step and then apply the modified manifest to your cluster. If your nodes don't have access to the private Amazon EKS Amazon ECR repositories that the images are pulled from (see the lines that start with `image:` in the manifest), then you'll have to download the images, copy them to your own repository, and modify the manifest to pull the images from your repository. For more information, see <<copy-image-to-repository>>.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl apply -f aws-k8s-cni.yaml
----
. Confirm that the new version is now installed on your cluster.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl describe daemonset aws-node --namespace kube-system | grep amazon-k8s-cni: | cut -d : -f 3
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
v1.19.0
----
. (Optional) Install the `cni-metrics-helper` to your cluster. It scrapes elastic network interface and IP address information, aggregates it at a cluster level, and publishes the metrics to Amazon CloudWatch. For more information, see https://github.com/aws/amazon-vpc-cni-k8s/blob/master/cmd/cni-metrics-helper/README.md[cni-metrics-helper] on GitHub.


[.topic]
[[cni-iam-role,cni-iam-role.title]]
=== Configure Amazon VPC CNI plugin to use IRSA
:info_doctype: section
:info_title: Configure Amazon VPC CNI plugin to use IRSA
:info_titleabbrev: Configure VPC CNI for IRSA
:info_abstract: Learn how to configure the [.noloc]`Amazon VPC CNI plugin for Kubernetes` to use IAM roles for service accounts (IRSA) for [.noloc]`Pod` networking in Amazon EKS clusters.

[abstract]
--
Learn how to configure the [.noloc]`Amazon VPC CNI plugin for Kubernetes` to use IAM roles for service accounts (IRSA) for [.noloc]`Pod` networking in Amazon EKS clusters.
--

The https://github.com/aws/amazon-vpc-cni-k8s[Amazon VPC CNI plugin for Kubernetes] is the networking plugin for [.noloc]`Pod` networking in Amazon EKS clusters. The plugin is responsible for allocating VPC IP addresses to [.noloc]`Kubernetes` nodes and configuring the necessary networking for [.noloc]`Pods` on each node. The plugin:


* Requires {aws} Identity and Access Management (IAM) permissions. If your cluster uses the `IPv4` family, the permissions are specified in the ` link:aws-managed-policy/latest/reference/AmazonEKS_CNI_Policy.html[AmazonEKS_CNI_Policy,type="documentation"]` {aws} managed policy.If your cluster uses the `IPv6` family, then the permissions must be added to an IAM policy that you create; for instructions, see <<cni-iam-role-create-ipv6-policy>>. You can attach the policy to the Amazon EKS node IAM role, or to a separate IAM role. For instructions to attach the policy to the Amazon EKS node IAM role, see <<create-node-role>>. We recommend that you assign it to a separate role, as detailed in this topic.
* Creates and is configured to use a [.noloc]`Kubernetes` service account named `aws-node` when it's deployed. The service account is bound to a [.noloc]`Kubernetes` `clusterrole` named `aws-node`, which is assigned the required [.noloc]`Kubernetes` permissions.


[NOTE]
====

The [.noloc]`Pods` for the [.noloc]`Amazon VPC CNI plugin for Kubernetes` have access to the permissions assigned to the  <<create-node-role,Amazon EKS node IAM role>>, unless you block access to IMDS. For more information, see https://aws.github.io/aws-eks-best-practices/security/docs/iam/#restrict-access-to-the-instance-profile-assigned-to-the-worker-node[Restrict access to the instance profile assigned to the worker node].

====

* An existing Amazon EKS cluster. To deploy one, see <<getting-started>>.
* An existing {aws} Identity and Access Management (IAM) [.noloc]`OpenID Connect` ([.noloc]`OIDC`) provider for your cluster. To determine whether you already have one, or to create one, see <<enable-iam-roles-for-service-accounts>>.


[[cni-iam-role-create-role,cni-iam-role-create-role.title]]
==== Step 1: Create the [.noloc]`Amazon VPC CNI plugin for Kubernetes` IAM role
. Determine the IP family of your cluster.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks describe-cluster --name my-cluster | grep ipFamily
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
"ipFamily": "ipv4"
----
+
The output may return `ipv6` instead.
. Create the IAM role. You can use `eksctl` or `kubectl` and the {aws} CLI to create your IAM role.
+
eksctl:::
** Create an IAM role and attach the IAM policy to the role with the command that matches the IP family of your cluster. The command creates and deploys an {aws} CloudFormation stack that creates an IAM role, attaches the policy that you specify to it, and annotates the existing `aws-node` [.noloc]`Kubernetes` service account with the ARN of the IAM role that is created.
+
*** `IPv4`
+
Replace [.replaceable]`my-cluster` with your own value.
+
[source,bash,subs="verbatim,attributes"]
----
eksctl create iamserviceaccount \
    --name aws-node \
    --namespace kube-system \
    --cluster my-cluster \
    --role-name AmazonEKSVPCCNIRole \
    --attach-policy-arn {arn-aws}iam::aws:policy/AmazonEKS_CNI_Policy \
    --override-existing-serviceaccounts \
    --approve
----
*** `IPv6`
+
Replace [.replaceable]`my-cluster` with your own value. Replace [.replaceable]`111122223333` with your account ID and replace [.replaceable]`AmazonEKS_CNI_IPv6_Policy` with the name of your `IPv6` policy. If you don't have an `IPv6` policy, see <<cni-iam-role-create-ipv6-policy>> to create one. To use `IPv6` with your cluster, it must meet several requirements. For more information, see <<cni-ipv6>>.
+
[source,bash,subs="verbatim,attributes"]
----
eksctl create iamserviceaccount \
    --name aws-node \
    --namespace kube-system \
    --cluster my-cluster \    
    --role-name AmazonEKSVPCCNIRole \
    --attach-policy-arn {arn-aws}iam::111122223333:policy/AmazonEKS_CNI_IPv6_Policy \
    --override-existing-serviceaccounts \
    --approve
----


kubectl and the {aws} CLI:::
... View your cluster's OIDC provider URL.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks describe-cluster --name my-cluster --query "cluster.identity.oidc.issuer" --output text
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
https://oidc.eks.region-code.amazonaws.com/id/EXAMPLED539D4633E53DE1B71EXAMPLE
----
+
If no output is returned, then you must  <<enable-iam-roles-for-service-accounts,create an IAM OIDC provider for your cluster>>.
... Copy the following contents to a file named [.replaceable]`vpc-cni-trust-policy.json`. Replace [.replaceable]`111122223333` with your account ID and [.replaceable]`EXAMPLED539D4633E53DE1B71EXAMPLE` with the output returned in the previous step. Replace [.replaceable]`region-code` with the {aws} Region that your cluster is in.
+
[source,json,subs="verbatim,attributes"]
----
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Federated": "{arn-aws}iam::111122223333:oidc-provider/oidc.eks.region-code.amazonaws.com/id/EXAMPLED539D4633E53DE1B71EXAMPLE"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.region-code.amazonaws.com/id/EXAMPLED539D4633E53DE1B71EXAMPLE:aud": "sts.amazonaws.com",
                    "oidc.eks.region-code.amazonaws.com/id/EXAMPLED539D4633E53DE1B71EXAMPLE:sub": "system:serviceaccount:kube-system:aws-node"
                }
            }
        }
    ]
}
----
... Create the role. You can replace [.replaceable]`AmazonEKSVPCCNIRole` with any name that you choose.
+
[source,bash,subs="verbatim,attributes"]
----
aws iam create-role \
  --role-name AmazonEKSVPCCNIRole \
  --assume-role-policy-document file://"vpc-cni-trust-policy.json"
----
... Attach the required IAM policy to the role. Run the command that matches the IP family of your cluster.
+
**** `IPv4`
+
[source,bash,subs="verbatim,attributes"]
----
aws iam attach-role-policy \
  --policy-arn {arn-aws}iam::aws:policy/AmazonEKS_CNI_Policy \
  --role-name AmazonEKSVPCCNIRole
----
**** `IPv6`
+
Replace [.replaceable]`111122223333` with your account ID and [.replaceable]`AmazonEKS_CNI_IPv6_Policy` with the name of your `IPv6` policy. If you don't have an `IPv6` policy, see <<cni-iam-role-create-ipv6-policy>> to create one. To use `IPv6` with your cluster, it must meet several requirements. For more information, see <<cni-ipv6>>.
+
[source,bash,subs="verbatim,attributes"]
----
aws iam attach-role-policy \
  --policy-arn {arn-aws}iam::111122223333:policy/AmazonEKS_CNI_IPv6_Policy \
  --role-name AmazonEKSVPCCNIRole
----
... Run the following command to annotate the `aws-node` service account with the ARN of the IAM role that you created previously. Replace the [.replaceable]`example values` with your own values.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl annotate serviceaccount \
    -n kube-system aws-node \
    eks.amazonaws.com/role-arn={arn-aws}iam::111122223333:role/AmazonEKSVPCCNIRole
----
. (Optional) Configure the {aws} Security Token Service endpoint type used by your [.noloc]`Kubernetes` service account. For more information, see <<configure-sts-endpoint>>.


[[cni-iam-role-redeploy-pods,cni-iam-role-redeploy-pods.title]]
==== Step 2: Re-deploy [.noloc]`Amazon VPC CNI plugin for Kubernetes` [.noloc]`Pods`
. Delete and re-create any existing [.noloc]`Pods` that are associated with the service account to apply the credential environment variables. The annotation is not applied to [.noloc]`Pods` that are currently running without the annotation. The following command deletes the existing `aws-node` [.noloc]`DaemonSet` [.noloc]`Pods` and deploys them with the service account annotation.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl delete Pods -n kube-system -l k8s-app=aws-node
----
. Confirm that the [.noloc]`Pods` all restarted.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl get pods -n kube-system -l k8s-app=aws-node
----
. Describe one of the [.noloc]`Pods` and verify that the `AWS_WEB_IDENTITY_TOKEN_FILE` and `AWS_ROLE_ARN` environment variables exist. Replace [.replaceable]`cpjw7` with the name of one of your [.noloc]`Pods` returned in the output of the previous step.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl describe pod -n kube-system aws-node-cpjw7 | grep 'AWS_ROLE_ARN:\|AWS_WEB_IDENTITY_TOKEN_FILE:'
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
AWS_ROLE_ARN:                 {arn-aws}iam::111122223333:role/AmazonEKSVPCCNIRole
      AWS_WEB_IDENTITY_TOKEN_FILE:  /var/run/secrets/eks.amazonaws.com/serviceaccount/token
      AWS_ROLE_ARN:                           {arn-aws}iam::111122223333:role/AmazonEKSVPCCNIRole
      AWS_WEB_IDENTITY_TOKEN_FILE:            /var/run/secrets/eks.amazonaws.com/serviceaccount/token
----
+
Two sets of duplicate results are returned because the [.noloc]`Pod` contains two containers. Both containers have the same values.
+
If your [.noloc]`Pod` is using the {aws} Regional endpoint, then the following line is also returned in the previous output.
+
[source,bash,subs="verbatim,attributes"]
----
AWS_STS_REGIONAL_ENDPOINTS=regional
----


[[remove-cni-policy-node-iam-role,remove-cni-policy-node-iam-role.title]]
==== Step 3: Remove the CNI policy from the node IAM role

If your  <<create-node-role,Amazon EKS node IAM role>> currently has the `AmazonEKS_CNI_Policy` IAM (`IPv4`) policyor an <<cni-iam-role-create-ipv6-policy,IPv6 policy>>attached to it, and you've created a separate IAM role, attached the policy to it instead, and assigned it to the `aws-node` [.noloc]`Kubernetes` service account, then we recommend that you remove the policy from your node role with the {aws} CLI command that matches the IP family of your cluster. Replace [.replaceable]`AmazonEKSNodeRole` with the name of your node role.



* `IPv4`
+
[source,bash,subs="verbatim,attributes"]
----
aws iam detach-role-policy --role-name AmazonEKSNodeRole --policy-arn {arn-aws}iam::aws:policy/AmazonEKS_CNI_Policy
----
* `IPv6`
+
Replace [.replaceable]`111122223333` with your account ID and [.replaceable]`AmazonEKS_CNI_IPv6_Policy` with the name of your `IPv6` policy.
+
[source,bash,subs="verbatim,attributes"]
----
aws iam detach-role-policy --role-name AmazonEKSNodeRole --policy-arn {arn-aws}iam::111122223333:policy/AmazonEKS_CNI_IPv6_Policy
----


[[cni-iam-role-create-ipv6-policy,cni-iam-role-create-ipv6-policy.title]]
==== Create IAM policy for clusters that use the `IPv6` family

If you created a cluster that uses the `IPv6` family and the cluster has version `1.10.1` or later of the [.noloc]`Amazon VPC CNI plugin for Kubernetes` add-on configured, then you need to create an IAM policy that you can assign to an IAM role. If you have an existing cluster that you didn't configure with the `IPv6` family when you created it, then to use `IPv6`, you must create a new cluster. For more information about using `IPv6` with your cluster, see <<cni-ipv6>>.

. Copy the following text and save it to a file named `vpc-cni-ipv6-policy.json`.
+
[source,json,subs="verbatim,attributes"]
----
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "ec2:AssignIpv6Addresses",
                "ec2:DescribeInstances",
                "ec2:DescribeTags",
                "ec2:DescribeNetworkInterfaces",
                "ec2:DescribeInstanceTypes"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "ec2:CreateTags"
            ],
            "Resource": [
                "{arn-aws}ec2:*:*:network-interface/*"
            ]
        }
    ]
}
----
. Create the IAM policy.
+
[source,bash,subs="verbatim,attributes"]
----
aws iam create-policy --policy-name AmazonEKS_CNI_IPv6_Policy --policy-document file://vpc-cni-ipv6-policy.json
----


[.topic]
[[pod-networking-use-cases,pod-networking-use-cases.title]]
=== Learn about VPC CNI modes and configuration

[abstract]
--
Discover how [.noloc]`Amazon VPC CNI plugin for Kubernetes` provides pod networking capabilities and settings for different Amazon EKS node types and use cases, including security groups, [.noloc]`Kubernetes` network policies, custom networking, IPv4, and IPv6 support.
--

The [.noloc]`Amazon VPC CNI plugin for Kubernetes` provides networking for [.noloc]`Pods`. Use the following table to learn more about the available networking features.

[cols="1,1", options="header"]
|===
|Networking feature
|Learn more


|Configure your cluster to assign IPv6 addresses to clusters, [.noloc]`Pods`, and services
|<<cni-ipv6>>

|Use IPv4 Source Network Address Translation for [.noloc]`Pods`
|<<external-snat>>

|Restrict network traffic to and from your [.noloc]`Pods`
|<<cni-network-policy-configure>>

|Customize the secondary network interface in nodes
|<<cni-custom-network>>

|Increase IP addresses for your node
|<<cni-increase-ip-addresses>>

|Use security groups for [.noloc]`Pod` network traffic
|<<security-groups-for-pods>>

|Use multiple network interfaces for [.noloc]`Pods`
|<<pod-multiple-network-interfaces>>
|===

[.topic]
[[cni-ipv6,cni-ipv6.title]]
==== Learn about IPv6 addresses to clusters, [.noloc]`pods`, and services

[abstract]
--
Learn how to deploy an `IPv6` cluster and nodes with Amazon EKS for assigning `IPv6` addresses to [.noloc]`Pods` and [.noloc]`services` instead of `IPv4`, leveraging IP prefix delegation and the latest [.noloc]`Amazon VPC CNI` plugin.
--

*Applies to*: [.noloc]`Pods` with Amazon EC2 instances and Fargate [.noloc]`Pods`

By default, [.noloc]`Kubernetes` assigns `IPv4` addresses to your [.noloc]`Pods` and [.noloc]`services`. Instead of assigning `IPv4` addresses to your [.noloc]`Pods` and [.noloc]`services`, you can configure your cluster to assign `IPv6` addresses to them. Amazon EKS doesn't support dual-stacked [.noloc]`Pods` or [.noloc]`services`, even though [.noloc]`Kubernetes` does in version `1.23` and later. As a result, you can't assign both `IPv4` and `IPv6` addresses to your [.noloc]`Pods` and [.noloc]`services`.  

You select which IP family you want to use for your cluster when you create it. You can't change the family after you create the cluster.

For a tutorial to deploy an Amazon EKS `IPv6` cluster, see <<deploy-ipv6-cluster>>.

//[[ipv6-considerations,ipv6-considerations.title]]
//===== Considerations 

The following are considerations for using the feature:

===== `IPv6` Feature support

* *No [.noloc]`Windows` support*: [.noloc]`Windows` [.noloc]`Pods` and [.noloc]`services` aren't supported.
* *Nitro-based EC2 nodes required*: You can only use `IPv6` with {aws} Nitro-based Amazon EC2 or Fargate nodes.
* *EC2 and Fargate nodes supported*: You can use `IPv6` with <<security-groups-for-pods>> with Amazon EC2 nodes and Fargate nodes.
* *Outposts not supported*: You can't use `IPv6` with <<eks-outposts>>.
* *FSx for Lustre is not supported*: The <<fsx-csi>> is not supported.
* *Instance Metadata Service not supported*: Use of the Amazon EC2 link:AWSEC2/latest/UserGuide/configuring-instance-metadata-service.html[Instance Metadata Service,type="documentation"] `IPv6` endpoint is not supported with Amazon EKS.
* *Custom networking not supported*: If you previously used <<cni-custom-network>> to help alleviate IP address exhaustion, you can use `IPv6` instead. You can't use custom networking with `IPv6`. If you use custom networking for network isolation, then you might need to continue to use custom networking and the `IPv4` family for your clusters.


===== IP address assignments

* *Kubernetes services*: Kubernetes services are only assigned an `IPv6` addresses. They aren't assigned IPv4 addresses. 
* *Pods*: Pods are assigned an IPv6 address and a host-local IPv4 address. The host-local IPv4 address is assigned by using a host-local CNI plugin chained with VPC CNI and the address is not reported to the Kubernetes control plane. It is only used when a pod needs to communicate with an external IPv4 resources in another Amazon VPC or the internet. The host-local IPv4 address gets SNATed (by VPC CNI) to the primary IPv4 address of the primary ENI of the worker node. 
* *Pods and services*: [.noloc]`Pods` and [.noloc]`services` are only assigned an `IPv6` address. They aren't assigned an `IPv4` address. Because [.noloc]`Pods` are able to communicate to `IPv4` endpoints through NAT on the instance itself,  link:vpc/latest/userguide/vpc-nat-gateway.html#nat-gateway-nat64-dns64[DNS64 and NAT64,type="documentation"] aren't needed. If the traffic needs a public IP address, the traffic is then source network address translated to a public IP.
* *Routing addresses*: The source `IPv6` address of a [.noloc]`Pod` isn't source network address translated to the `IPv6` address of the node when communicating outside of the VPC. It is routed using an internet gateway or egress-only internet gateway.
* *Nodes*: All nodes are assigned an `IPv4` and `IPv6` address.
* *Fargate [.noloc]`Pods`*: Each Fargate [.noloc]`Pod` receives an `IPv6` address from the CIDR that's specified for the subnet that it's deployed in. The underlying hardware unit that runs Fargate [.noloc]`Pods` gets a unique `IPv4` and `IPv6` address from the CIDRs that are assigned to the subnet that the hardware unit is deployed in.


===== How to use `IPv6` with EKS

* *Create new cluster*: You must create a new cluster and specify that you want to use the `IPv6` family for that cluster. You can't enable the `IPv6` family for a cluster that you updated from a previous version. For instructions on how to create a new cluster, see Considerations .
* *Use recent VPC CNI*: Deploy Amazon VPC CNI version `1.10.1` or later. This version or later is deployed by default. After you deploy the add-on, you can't downgrade your Amazon VPC CNI add-on to a version lower than `1.10.1` without first removing all nodes in all node groups in your cluster.
* *Configure VPC CNI for `IPv6`*: If you use Amazon EC2 nodes, you must configure the Amazon VPC CNI add-on with IP prefix delegation and `IPv6`. If you choose the `IPv6` family when creating your cluster, the `1.10.1` version of the add-on defaults to this configuration. This is the case for both a self-managed or Amazon EKS add-on. For more information about IP prefix delegation, see <<cni-increase-ip-addresses>>.
* *Configure `IPv4` and `IPv6` addresses*: When you create a cluster, the VPC and subnets that you specify must have an `IPv6` CIDR block that's assigned to the VPC and subnets that you specify. They must also have an `IPv4` CIDR block assigned to them. This is because, even if you only want to use `IPv6`, a VPC still requires an `IPv4` CIDR block to function. For more information, see  link:vpc/latest/userguide/working-with-vpcs.html#vpc-associate-ipv6-cidr[Associate an IPv6 CIDR block with your VPC,type="documentation"] in the Amazon VPC User Guide.
* *Auto-assign IPv6 addresses to nodes:* When you create your nodes, you must specify subnets that are configured to auto-assign `IPv6` addresses. Otherwise, you can't deploy your nodes. By default, this configuration is disabled. For more information, see  link:vpc/latest/userguide/vpc-ip-addressing.html#subnet-ipv6[Modify the IPv6 addressing attribute for your subnet,type="documentation"] in the Amazon VPC User Guide.
* *Set route tables to use `IPv6`*: The route tables that are assigned to your subnets must have routes for `IPv6` addresses. For more information, see link:vpc/latest/userguide/vpc-migrate-ipv6.html[Migrate to IPv6,type="documentation"] in the Amazon VPC User Guide.
* *Set security groups for `IPv6`*: Your security groups must allow `IPv6` addresses. For more information, see link:vpc/latest/userguide/vpc-migrate-ipv6.html[Migrate to IPv6,type="documentation"] in the Amazon VPC User Guide.
* *Set up load balancer*: Use version `2.3.1` or later of the {aws} Load Balancer Controller to load balance HTTP applications using the <<alb-ingress>> or network traffic using the <<network-load-balancing>> to `IPv6` [.noloc]`Pods` with either load balancer in IP mode, but not instance mode. For more information, see <<aws-load-balancer-controller>>.
* *Add `IPv6` IAM policy*: You must attach an `IPv6` IAM policy to your node IAM or CNI IAM role. Between the two, we recommend that you attach it to a CNI IAM role. For more information, see <<cni-iam-role-create-ipv6-policy>> and <<cni-iam-role-create-role>>.
* *Evaluate all components*: Perform a thorough evaluation of your applications, Amazon EKS add-ons, and {aws} services that you integrate with before deploying `IPv6` clusters. This is to ensure that everything works as expected with `IPv6`.
* *Add `BootstrapArguments` self-managed node groups*: When creating a self-managed node group in a cluster that uses the `IPv6` family, user-data must include the following `BootstrapArguments` for the https://github.com/awslabs/amazon-eks-ami/blob/main/templates/al2/runtime/bootstrap.sh[bootstrap.sh] file that runs at node start up. Replace [.replaceable]`your-cidr` with the `IPv6` [.noloc]`CIDR` range of your cluster's VPC.
+
[source,bash,subs="verbatim,attributes"]
----
--ip-family ipv6 --service-ipv6-cidr your-cidr
----
+
If you don't know the `IPv6` `CIDR` range for your cluster, you can see it with the following command (requires the {aws} CLI version `2.4.9` or later).
+
[source,bash,subs="verbatim,attributes"]
----
aws eks describe-cluster --name my-cluster --query cluster.kubernetesNetworkConfig.serviceIpv6Cidr --output text
----


[.topic]
[[deploy-ipv6-cluster,deploy-ipv6-cluster.title]]
===== Deploying an Amazon EKS `IPv6` cluster and managed Amazon Linux nodes

In this tutorial, you deploy an `IPv6` Amazon VPC, an Amazon EKS cluster with the `IPv6` family, and a managed node group with Amazon EC2 Amazon Linux nodes. You can't deploy Amazon EC2 [.noloc]`Windows` nodes in an `IPv6` cluster. You can also deploy Fargate nodes to your cluster, though those instructions aren't provided in this topic for simplicity. 

====== Prerequisites

Complete the following before you start the tutorial:

Install and configure the following tools and resources that you need to create and manage an Amazon EKS cluster.

* We recommend that you familiarize yourself with all settings and deploy a cluster with the settings that meet your requirements. For more information, see <<create-cluster>>, <<managed-node-groups>>, and the <<cni-ipv6,Considerations>> for this topic. You can only enable some settings when creating your cluster.
* The `kubectl` command line tool is installed on your device or {aws} CloudShell. The version can be the same as or up to one minor version earlier or later than the [.noloc]`Kubernetes` version of your cluster. For example, if your cluster version is `1.29`, you can use `kubectl` version `1.28`, `1.29`, or `1.30` with it. To install or upgrade `kubectl`, see <<install-kubectl>>.
* The IAM security principal that you're using must have permissions to work with Amazon EKS IAM roles, service linked roles, {aws} CloudFormation, a VPC, and related resources. For more information, see  link:service-authorization/latest/reference/list_amazonelastickubernetesservice.html[Actions, resources, and condition keys for Amazon Elastic Kubernetes Service,type="documentation"] and link:IAM/latest/UserGuide/using-service-linked-roles.html[Using service-linked roles,type="documentation"] in the IAM User Guide.
* If you use the [.noloc]`eksctl`, install version `{eksctl-min-version}` or later on your computer. To install or update to it, see https://eksctl.io/installation[Installation] in the `eksctl` documentation.
* Version `2.12.3` or later or version `1.27.160` or later of the {aws} Command Line Interface ({aws} CLI) installed and configured on your device or {aws} CloudShell. To check your current version, use `aws --version | cut -d / -f2 | cut -d ' ' -f1`. Package managers such `yum`, `apt-get`, or [.noloc]`Homebrew` for [.noloc]`macOS` are often several versions behind the latest version of the {aws} CLI. To install the latest version, see link:cli/latest/userguide/cli-chap-install.html[Installing, updating, and uninstalling the {aws} CLI,type="documentation"] and  link:cli/latest/userguide/cli-configure-quickstart.html#cli-configure-quickstart-config[Quick configuration with aws configure,type="documentation"] in the _{aws} Command Line Interface User Guide_. The {aws} CLI version that is installed in {aws} CloudShell might also be several versions behind the latest version. To update it, see  link:cloudshell/latest/userguide/vm-specs.html#install-cli-software[Installing {aws} CLI to your home directory,type="documentation"] in the _{aws} CloudShell User Guide_. If you use the {aws} CloudShell, you may need to  link:cloudshell/latest/userguide/vm-specs.html#install-cli-software[install version 2.12.3 or later or 1.27.160 or later of the {aws} CLI,type="documentation"], because the default {aws} CLI version installed in the {aws} CloudShell may be an earlier version.


//[[deploy-ipv6-cluster-procedure,deploy-ipv6-cluster-procedure.title]]
//====== Procedure

You can use the [.noloc]`eksctl` or CLI to  deploy an `IPv6` cluster.


====== Deploy an IPv6 cluster with [.noloc]`eksctl`

.. Create the `ipv6-cluster.yaml` file. Copy the command that follows to your device. Make the following modifications to the command as needed and then run the modified command:
+
*** Replace [.replaceable]`my-cluster` with a name for your cluster. The name can contain only alphanumeric characters (case-sensitive) and hyphens. It must start with an alphanumeric character and can't be longer than 100 characters. The name must be unique within the {aws} Region and {aws} account that you're creating the cluster in.
*** Replace [.replaceable]`region-code` with any {aws} Region that is supported by Amazon EKS. For a list of {aws} Regions, see link:general/latest/gr/eks.html[Amazon EKS endpoints and quotas,type="documentation"] in the {aws} General Reference guide.
*** The value for `version` with the version of your cluster. For more information, see <<kubernetes-versions>>.
*** Replace [.replaceable]`my-nodegroup` with a name for your node group. The node group name can't be longer than 63 characters. It must start with letter or digit, but can also include hyphens and underscores for the remaining characters.
*** Replace [.replaceable]`t3.medium` with any  link:AWSEC2/latest/UserGuide/instance-types.html#ec2-nitro-instances[{aws} Nitro System instance type,type="documentation"].
+
[source,yaml,subs="verbatim,attributes"]
----
cat >ipv6-cluster.yaml <<EOF
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: my-cluster
  region: region-code
  version: "X.XX"

kubernetesNetworkConfig:
  ipFamily: IPv6

addons:
  - name: vpc-cni
    version: latest
  - name: coredns
    version: latest
  - name: kube-proxy
    version: latest

iam:
  withOIDC: true

managedNodeGroups:
  - name: my-nodegroup
    instanceType: t3.medium
EOF
----
.. Create your cluster.
+
[source,bash,subs="verbatim,attributes"]
----
eksctl create cluster -f ipv6-cluster.yaml
----
+
Cluster creation takes several minutes. Don't proceed until you see the last line of output, which looks similar to the following output.
+
[literal]
----
[...]
[]  EKS cluster "my-cluster" in "region-code" region is ready
----
.. Confirm that default [.noloc]`Pods` are assigned `IPv6` addresses.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl get pods -n kube-system -o wide
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
NAME                       READY   STATUS    RESTARTS   AGE     IP                                       NODE                                            NOMINATED NODE   READINESS GATES
aws-node-rslts             1/1     Running   1          5m36s   2600:1f13:b66:8200:11a5:ade0:c590:6ac8   ip-192-168-34-75.region-code.compute.internal   <none>           <none>
aws-node-t74jh             1/1     Running   0          5m32s   2600:1f13:b66:8203:4516:2080:8ced:1ca9   ip-192-168-253-70.region-code.compute.internal  <none>           <none>
coredns-85d5b4454c-cw7w2   1/1     Running   0          56m     2600:1f13:b66:8203:34e5::                ip-192-168-253-70.region-code.compute.internal  <none>           <none>
coredns-85d5b4454c-tx6n8   1/1     Running   0          56m     2600:1f13:b66:8203:34e5::1               ip-192-168-253-70.region-code.compute.internal  <none>           <none>
kube-proxy-btpbk           1/1     Running   0          5m36s   2600:1f13:b66:8200:11a5:ade0:c590:6ac8   ip-192-168-34-75.region-code.compute.internal   <none>           <none>
kube-proxy-jjk2g           1/1     Running   0          5m33s   2600:1f13:b66:8203:4516:2080:8ced:1ca9   ip-192-168-253-70.region-code.compute.internal  <none>           <none>
----
.. Confirm that default services are assigned `IPv6` addresses.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl get services -n kube-system -o wide
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
NAME       TYPE        CLUSTER-IP          EXTERNAL-IP   PORT(S)         AGE   SELECTOR
kube-dns   ClusterIP   fd30:3087:b6c2::a   <none>        53/UDP,53/TCP   57m   k8s-app=kube-dns
----
.. (Optional) <<sample-deployment,Deploy a sample application>> or deploy the <<aws-load-balancer-controller,{aws} Load Balancer Controller>> and a sample application to load balance HTTP applications with <<alb-ingress>> or network traffic with <<network-load-balancing>> to `IPv6` [.noloc]`Pods`.
.. After you've finished with the cluster and nodes that you created for this tutorial, you should clean up the resources that you created with the following command.
+
[source,bash,subs="verbatim,attributes"]
----
eksctl delete cluster my-cluster
----


====== Deploy an IPv6 cluster with {aws} CLI

[IMPORTANT]
==== 
** You must complete all steps in this procedure as the same user. To check the current user, run the following command:
+
[source,bash,subs="verbatim,attributes"]
----
aws sts get-caller-identity
----
** You must complete all steps in this procedure in the same shell. Several steps use variables set in previous steps. Steps that use variables won't function properly if the variable values are set in a different shell. If you use the link:cloudshell/latest/userguide/welcome.html[{aws} CloudShell,type="documentation"] to complete the following procedure, remember that if you don't interact with it using your keyboard or pointer for approximately 2030 minutes, your shell session ends. Running processes do not count as interactions.
** The instructions are written for the Bash shell, and may need adjusting in other shells.
====


Replace all [.replaceable]`example values` in the steps of this procedure with your own values.

.. Run the following commands to set some variables used in later steps. Replace [.replaceable]`region-code` with the {aws} Region that you want to deploy your resources in. The value can be any {aws} Region that is supported by Amazon EKS. For a list of {aws} Regions, see link:general/latest/gr/eks.html[Amazon EKS endpoints and quotas,type="documentation"] in the {aws} General Reference guide. Replace [.replaceable]`my-cluster` with a name for your cluster. The name can contain only alphanumeric characters (case-sensitive) and hyphens. It must start with an alphanumeric character and can't be longer than 100 characters. The name must be unique within the {aws} Region and {aws} account that you're creating the cluster in. Replace [.replaceable]`my-nodegroup` with a name for your node group. The node group name can't be longer than 63 characters. It must start with letter or digit, but can also include hyphens and underscores for the remaining characters. Replace [.replaceable]`111122223333` with your account ID.
+
[source,bash,subs="verbatim,attributes"]
----
export region_code=region-code
export cluster_name=my-cluster
export nodegroup_name=my-nodegroup
export account_id=111122223333
----
.. Create an Amazon VPC with public and private subnets that meets Amazon EKS and `IPv6` requirements.
+
... Run the following command to set a variable for your {aws} CloudFormation stack name. You can replace [.replaceable]`my-eks-ipv6-vpc` with any name you choose.
+
[source,bash,subs="verbatim,attributes"]
----
export vpc_stack_name=my-eks-ipv6-vpc
----
... Create an `IPv6` VPC using an {aws} CloudFormation template.
+
[source,bash,subs="verbatim,attributes"]
----
aws cloudformation create-stack --region $region_code --stack-name $vpc_stack_name \
  --template-url https://s3.us-west-2.amazonaws.com/amazon-eks/cloudformation/2020-10-29/amazon-eks-ipv6-vpc-public-private-subnets.yaml
----
+
The stack takes a few minutes to create. Run the following command. Don't continue to the next step until the output of the command is `CREATE_COMPLETE`.
+
[source,bash,subs="verbatim,attributes"]
----
aws cloudformation describe-stacks --region $region_code --stack-name $vpc_stack_name --query Stacks[].StackStatus --output text
----
... Retrieve the IDs of the public subnets that were created.
+
[source,bash,subs="verbatim,attributes"]
----
aws cloudformation describe-stacks --region $region_code --stack-name $vpc_stack_name \
    --query='Stacks[].Outputs[?OutputKey==`SubnetsPublic`].OutputValue' --output text
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
subnet-0a1a56c486EXAMPLE,subnet-099e6ca77aEXAMPLE
----
... Enable the auto-assign `IPv6` address option for the public subnets that were created.
+
[source,bash,subs="verbatim,attributes"]
----
aws ec2 modify-subnet-attribute --region $region_code --subnet-id subnet-0a1a56c486EXAMPLE --assign-ipv6-address-on-creation
aws ec2 modify-subnet-attribute --region $region_code --subnet-id subnet-099e6ca77aEXAMPLE --assign-ipv6-address-on-creation
----
... Retrieve the names of the subnets and security groups created by the template from the deployed {aws} CloudFormation stack and store them in variables for use in a later step.
+
[source,bash,subs="verbatim,attributes"]
----
security_groups=$(aws cloudformation describe-stacks --region $region_code --stack-name $vpc_stack_name \
    --query='Stacks[].Outputs[?OutputKey==`SecurityGroups`].OutputValue' --output text)

public_subnets=$(aws cloudformation describe-stacks --region $region_code --stack-name $vpc_stack_name \
    --query='Stacks[].Outputs[?OutputKey==`SubnetsPublic`].OutputValue' --output text)

private_subnets=$(aws cloudformation describe-stacks --region $region_code --stack-name $vpc_stack_name \
    --query='Stacks[].Outputs[?OutputKey==`SubnetsPrivate`].OutputValue' --output text)

subnets=${public_subnets},${private_subnets}
----
.. Create a cluster IAM role and attach the required Amazon EKS IAM managed policy to it. [.noloc]`Kubernetes` clusters managed by Amazon EKS make calls to other {aws} services on your behalf to manage the resources that you use with the service.
+
... Run the following command to create the `eks-cluster-role-trust-policy.json` file.
+
[source,json,subs="verbatim,attributes"]
----
cat >eks-cluster-role-trust-policy.json <<EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "eks.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF
----
... Run the following command to set a variable for your role name. You can replace [.replaceable]`myAmazonEKSClusterRole` with any name you choose.
+
[source,bash,subs="verbatim,attributes"]
----
export cluster_role_name=myAmazonEKSClusterRole
----
... Create the role.
+
[source,bash,subs="verbatim,attributes"]
----
aws iam create-role --role-name $cluster_role_name --assume-role-policy-document file://"eks-cluster-role-trust-policy.json"
----
... Retrieve the ARN of the IAM role and store it in a variable for a later step.
+
[source,bash,subs="verbatim,attributes"]
----
CLUSTER_IAM_ROLE=$(aws iam get-role --role-name $cluster_role_name --query="Role.Arn" --output text)
----
... Attach the required Amazon EKS managed IAM policy to the role.
+
[source,bash,subs="verbatim,attributes"]
----
aws iam attach-role-policy --policy-arn {arn-aws}iam::aws:policy/AmazonEKSClusterPolicy --role-name $cluster_role_name
----
.. Create your cluster.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks create-cluster --region $region_code --name $cluster_name --kubernetes-version 1.XX \
   --role-arn $CLUSTER_IAM_ROLE --resources-vpc-config subnetIds=$subnets,securityGroupIds=$security_groups \
   --kubernetes-network-config ipFamily=ipv6
----
+
... NOTE: You might receive an error that one of the Availability Zones in your request doesn't have sufficient capacity to create an Amazon EKS cluster. If this happens, the error output contains the Availability Zones that can support a new cluster. Retry creating your cluster with at least two subnets that are located in the supported Availability Zones for your account. For more information, see <<ice>>.
+
The cluster takes several minutes to create. Run the following command. Don't continue to the next step until the output from the command is `ACTIVE`.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks describe-cluster --region $region_code --name $cluster_name --query cluster.status
----
.. Create or update a `kubeconfig` file for your cluster so that you can communicate with your cluster.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks update-kubeconfig --region $region_code --name $cluster_name
----
+
By default, the `config` file is created in `~/.kube` or the new cluster's configuration is added to an existing `config` file in `~/.kube`.
.. Create a node IAM role.
+
... Run the following command to create the `vpc-cni-ipv6-policy.json` file.
+
[source,json,subs="verbatim,attributes"]
----
cat >vpc-cni-ipv6-policy <<EOF
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "ec2:AssignIpv6Addresses",
                "ec2:DescribeInstances",
                "ec2:DescribeTags",
                "ec2:DescribeNetworkInterfaces",
                "ec2:DescribeInstanceTypes"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "ec2:CreateTags"
            ],
            "Resource": [
                "{arn-aws}ec2:*:*:network-interface/*"
            ]
        }
    ]
}
EOF
----
... Create the IAM policy.
+
[source,bash,subs="verbatim,attributes"]
----
aws iam create-policy --policy-name AmazonEKS_CNI_IPv6_Policy --policy-document file://vpc-cni-ipv6-policy.json
----
... Run the following command to create the `node-role-trust-relationship.json` file.
+
[source,json,subs="verbatim,attributes"]
----
cat >node-role-trust-relationship.json <<EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "ec2.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF
----
... Run the following command to set a variable for your role name. You can replace [.replaceable]`AmazonEKSNodeRole` with any name you choose.
+
[source,bash,subs="verbatim,attributes"]
----
export node_role_name=AmazonEKSNodeRole
----
... Create the IAM role.
+
[source,bash,subs="verbatim,attributes"]
----
aws iam create-role --role-name $node_role_name --assume-role-policy-document file://"node-role-trust-relationship.json"
----
... Attach the IAM policy to the IAM role.
+
[source,bash,subs="verbatim,attributes"]
----
aws iam attach-role-policy --policy-arn {arn-aws}iam::$account_id:policy/AmazonEKS_CNI_IPv6_Policy \
    --role-name $node_role_name
----
+
IMPORTANT: For simplicity in this tutorial, the policy is attached to this IAM role. In a production cluster however, we recommend attaching the policy to a separate IAM role. For more information, see <<cni-iam-role>>.
... Attach two required IAM managed policies to the IAM role.
+
[source,bash,subs="verbatim,attributes"]
----
aws iam attach-role-policy --policy-arn {arn-aws}iam::aws:policy/AmazonEKSWorkerNodePolicy \
  --role-name $node_role_name
aws iam attach-role-policy --policy-arn {arn-aws}iam::aws:policy/AmazonEC2ContainerRegistryReadOnly \
  --role-name $node_role_name
----
... Retrieve the ARN of the IAM role and store it in a variable for a later step.
+
[source,bash,subs="verbatim,attributes"]
----
node_iam_role=$(aws iam get-role --role-name $node_role_name --query="Role.Arn" --output text)
----
.. Create a managed node group.
+
... View the IDs of the subnets that you created in a previous step.
+
[source,bash,subs="verbatim,attributes"]
----
echo $subnets
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
subnet-0a1a56c486EXAMPLE,subnet-099e6ca77aEXAMPLE,subnet-0377963d69EXAMPLE,subnet-0c05f819d5EXAMPLE
----
... Create the node group. Replace [.replaceable]`0a1a56c486EXAMPLE`, [.replaceable]`099e6ca77aEXAMPLE`, [.replaceable]`0377963d69EXAMPLE`, and [.replaceable]`0c05f819d5EXAMPLE` with the values returned in the output of the previous step. Be sure to remove the commas between subnet IDs from the previous output in the following command. You can replace [.replaceable]`t3.medium` with any  link:AWSEC2/latest/UserGuide/instance-types.html#ec2-nitro-instances[{aws} Nitro System instance type,type="documentation"].
+
[source,bash,subs="verbatim,attributes"]
----
aws eks create-nodegroup --region $region_code --cluster-name $cluster_name --nodegroup-name $nodegroup_name \
    --subnets subnet-0a1a56c486EXAMPLE subnet-099e6ca77aEXAMPLE subnet-0377963d69EXAMPLE subnet-0c05f819d5EXAMPLE \
    --instance-types t3.medium --node-role $node_iam_role
----
+
The node group takes a few minutes to create. Run the following command. Don't proceed to the next step until the output returned is `ACTIVE`.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks describe-nodegroup --region $region_code --cluster-name $cluster_name --nodegroup-name $nodegroup_name \
    --query nodegroup.status --output text
----
.. Confirm that the default [.noloc]`Pods` are assigned `IPv6` addresses in the `IP` column.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl get pods -n kube-system -o wide
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
NAME                       READY   STATUS    RESTARTS   AGE     IP                                       NODE                                            NOMINATED NODE   READINESS GATES
aws-node-rslts             1/1     Running   1          5m36s   2600:1f13:b66:8200:11a5:ade0:c590:6ac8   ip-192-168-34-75.region-code.compute.internal   <none>           <none>
aws-node-t74jh             1/1     Running   0          5m32s   2600:1f13:b66:8203:4516:2080:8ced:1ca9   ip-192-168-253-70.region-code.compute.internal  <none>           <none>
coredns-85d5b4454c-cw7w2   1/1     Running   0          56m     2600:1f13:b66:8203:34e5::                ip-192-168-253-70.region-code.compute.internal  <none>           <none>
coredns-85d5b4454c-tx6n8   1/1     Running   0          56m     2600:1f13:b66:8203:34e5::1               ip-192-168-253-70.region-code.compute.internal  <none>           <none>
kube-proxy-btpbk           1/1     Running   0          5m36s   2600:1f13:b66:8200:11a5:ade0:c590:6ac8   ip-192-168-34-75.region-code.compute.internal   <none>           <none>
kube-proxy-jjk2g           1/1     Running   0          5m33s   2600:1f13:b66:8203:4516:2080:8ced:1ca9   ip-192-168-253-70.region-code.compute.internal  <none>           <none>
----
.. Confirm that the default services are assigned `IPv6` addresses in the `IP` column.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl get services -n kube-system -o wide
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
NAME       TYPE        CLUSTER-IP          EXTERNAL-IP   PORT(S)         AGE   SELECTOR
kube-dns   ClusterIP   fd30:3087:b6c2::a   <none>        53/UDP,53/TCP   57m   k8s-app=kube-dns
----
.. (Optional) <<sample-deployment,Deploy a sample application>> or deploy the <<aws-load-balancer-controller,{aws} Load Balancer Controller>> and a sample application to load balance HTTP applications with <<alb-ingress>> or network traffic with <<network-load-balancing>> to `IPv6` [.noloc]`Pods`.
.. After you've finished with the cluster and nodes that you created for this tutorial, you should clean up the resources that you created with the following commands. Make sure that you're not using any of the resources outside of this tutorial before deleting them.
+
... If you're completing this step in a different shell than you completed the previous steps in, set the values of all the variables used in previous steps, replacing the [.replaceable]`example values` with the values you specified when you completed the previous steps. If you're completing this step in the same shell that you completed the previous steps in, skip to the next step.
+
[source,bash,subs="verbatim,attributes"]
----
export region_code=region-code
export vpc_stack_name=my-eks-ipv6-vpc
export cluster_name=my-cluster
export nodegroup_name=my-nodegroup
export account_id=111122223333
export node_role_name=AmazonEKSNodeRole
export cluster_role_name=myAmazonEKSClusterRole
----
... Delete your node group.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks delete-nodegroup --region $region_code --cluster-name $cluster_name --nodegroup-name $nodegroup_name
----
+
Deletion takes a few minutes. Run the following command. Don't proceed to the next step if any output is returned.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks list-nodegroups --region $region_code --cluster-name $cluster_name --query nodegroups --output text
----
... Delete the cluster.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks delete-cluster --region $region_code --name $cluster_name
----
+
The cluster takes a few minutes to delete. Before continuing make sure that the cluster is deleted with the following command.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks describe-cluster --region $region_code --name $cluster_name
----
+
Don't proceed to the next step until your output is similar to the following output.
+
[source,bash,subs="verbatim,attributes"]
----
An error occurred (ResourceNotFoundException) when calling the DescribeCluster operation: No cluster found for name: my-cluster.
----
... Delete the IAM resources that you created. Replace [.replaceable]`AmazonEKS_CNI_IPv6_Policy` with the name you chose, if you chose a different name than the one used in previous steps.
+
[source,bash,subs="verbatim,attributes"]
----
aws iam detach-role-policy --role-name $cluster_role_name --policy-arn {arn-aws}iam::aws:policy/AmazonEKSClusterPolicy
aws iam detach-role-policy --role-name $node_role_name --policy-arn {arn-aws}iam::aws:policy/AmazonEKSWorkerNodePolicy
aws iam detach-role-policy --role-name $node_role_name --policy-arn {arn-aws}iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
aws iam detach-role-policy --role-name $node_role_name --policy-arn {arn-aws}iam::$account_id:policy/AmazonEKS_CNI_IPv6_Policy
aws iam delete-policy --policy-arn {arn-aws}iam::$account_id:policy/AmazonEKS_CNI_IPv6_Policy
aws iam delete-role --role-name $cluster_role_name
aws iam delete-role --role-name $node_role_name
----
... Delete the {aws} CloudFormation stack that created the VPC.
+
[source,bash,subs="verbatim,attributes"]
----
aws cloudformation delete-stack --region $region_code --stack-name $vpc_stack_name
----


[.topic]
[[external-snat,external-snat.title]]
==== Enable outbound internet access for [.noloc]`pods`

[abstract]
--
Learn how Amazon EKS manages external communication for [.noloc]`Pods` using Source Network Address Translation (SNAT), allowing Pods to access internet resources or networks connected via VPC peering, Transit Gateway, or {aws} Direct Connect.
--

*Applies to*: [.noloc]`Linux` `IPv4` Fargate nodes, [.noloc]`Linux` nodes with Amazon EC2 instances  

If you deployed your cluster using the `IPv6` family, then the information in this topic isn't applicable to your cluster, because `IPv6` addresses are not network translated. For more information about using `IPv6` with your cluster, see <<cni-ipv6>>.

By default, each [.noloc]`Pod` in your cluster is assigned a   link:AWSEC2/latest/UserGuide/using-instance-addressing.html#concepts-private-addresses[private,type="documentation"]``IPv4`` address from a classless inter-domain routing (CIDR) block that is associated with the VPC that the [.noloc]`Pod` is deployed in. [.noloc]`Pods` in the same VPC communicate with each other using these private IP addresses as end points. When a [.noloc]`Pod` communicates to any `IPv4` address that isn't within a CIDR block that's associated to your VPC, the Amazon VPC CNI plugin (for both https://github.com/aws/amazon-vpc-cni-k8s#amazon-vpc-cni-k8s[Linux] or https://github.com/aws/amazon-vpc-cni-plugins/tree/master/plugins/vpc-bridge[Windows]) translates the [.noloc]`Pod's` `IPv4` address to the primary private `IPv4` address of the primary  link:AWSEC2/latest/UserGuide/using-eni.html#eni-basics[elastic network interface,type="documentation"] of the node that the [.noloc]`Pod` is running on, by default  ^^<<snat-exception,*>>^^.

[NOTE]
====

For [.noloc]`Windows` nodes, there are additional details to consider. By default, the https://github.com/aws/amazon-vpc-cni-plugins/tree/master/plugins/vpc-bridge[VPC CNI plugin for Windows] is defined with a networking configuration in which the traffic to a destination within the same VPC is excluded for SNAT. This means that internal VPC communication has SNAT disabled and the IP address allocated to a [.noloc]`Pod` is routable inside the VPC. But traffic to a destination outside of the VPC has the source [.noloc]`Pod` IP SNAT'ed to the instance ENI's primary IP address. This default configuration for [.noloc]`Windows` ensures that the pod can access networks outside of your VPC in the same way as the host instance.

====

Due to this behavior:



* Your [.noloc]`Pods` can communicate with internet resources only if the node that they're running on has a   link:AWSEC2/latest/UserGuide/using-instance-addressing.html#concepts-public-addresses[public,type="documentation"] or link:vpc/latest/userguide/vpc-eips.html[elastic,type="documentation"] IP address assigned to it and is in a  link:vpc/latest/userguide/configure-subnets.html#subnet-basics[public subnet,type="documentation"]. A public subnet's associated  link:vpc/latest/userguide/VPC_Route_Tables.html[route table,type="documentation"] has a route to an internet gateway. We recommend deploying nodes to private subnets, whenever possible.
* For versions of the plugin earlier than `1.8.0`, resources that are in networks or VPCs that are connected to your cluster VPC using link:vpc/latest/peering/what-is-vpc-peering.html[VPC peering,type="documentation"], a link:whitepapers/latest/aws-vpc-connectivity-options/transit-vpc-option.html[transit VPC,type="documentation"], or link:directconnect/latest/UserGuide/Welcome.html[{aws} Direct Connect,type="documentation"] can't initiate communication to your [.noloc]`Pods` behind secondary elastic network interfaces. Your [.noloc]`Pods` can initiate communication to those resources and receive responses from them, though.

If either of the following statements are true in your environment, then change the default configuration with the command that follows.



* You have resources in networks or VPCs that are connected to your cluster VPC using link:vpc/latest/peering/what-is-vpc-peering.html[VPC peering,type="documentation"], a link:whitepapers/latest/aws-vpc-connectivity-options/transit-vpc-option.html[transit VPC,type="documentation"], or link:directconnect/latest/UserGuide/Welcome.html[{aws} Direct Connect,type="documentation"] that need to initiate communication with your [.noloc]`Pods` using an `IPv4` address and your plugin version is earlier than `1.8.0`.
* Your [.noloc]`Pods` are in a   link:vpc/latest/userguide/configure-subnets.html#subnet-basics[private subnet,type="documentation"] and need to communicate outbound to the internet. The subnet has a route to a link:vpc/latest/userguide/vpc-nat-gateway.html[NAT gateway,type="documentation"].


[source,bash,subs="verbatim,attributes"]
----
kubectl set env daemonset -n kube-system aws-node AWS_VPC_K8S_CNI_EXTERNALSNAT=true
----

[NOTE]
====

The `AWS_VPC_K8S_CNI_EXTERNALSNAT` and `AWS_VPC_K8S_CNI_EXCLUDE_SNAT_CIDRS` CNI configuration variables aren't applicable to [.noloc]`Windows` nodes. Disabling SNAT isn't supported for [.noloc]`Windows`. As for excluding a list of `IPv4` CIDRs from SNAT, you can define this by specifying the `ExcludedSnatCIDRs` parameter in the [.noloc]`Windows` bootstrap script. For more information on using this parameter, see <<bootstrap-script-configuration-parameters>>.

====

[[snat-exception,snat-exception.title]]
===== Host networking

^^*^^If a [.noloc]`Pod's` spec contains `hostNetwork=true` (default is `false`), then its IP address isn't translated to a different address. This is the case for the `kube-proxy` and [.noloc]`Amazon VPC CNI plugin for Kubernetes` [.noloc]`Pods` that run on your cluster, by default. For these [.noloc]`Pods`, the IP address is the same as the node's primary IP address, so the [.noloc]`Pod's` IP address isn't translated. For more information about a [.noloc]`Pod's` `hostNetwork` setting, see https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.30/#podspec-v1-core[PodSpec v1 core] in the [.noloc]`Kubernetes` API reference.

[.topic]
[[cni-network-policy,cni-network-policy.title]]
==== Limit [.noloc]`pod` traffic with [.noloc]`Kubernetes` network policies

[abstract]
--
Learn how to configure your Amazon EKS cluster to use [.noloc]`Kubernetes` network policies with the [.noloc]`Amazon VPC CNI` plugin. Control network traffic to and from pods using network policies for enhanced security. Covers network policy considerations, requirements, setup instructions, and troubleshooting tips.
--

By default, there are no restrictions in [.noloc]`Kubernetes` for IP addresses, ports, or connections between any [.noloc]`Pods` in your cluster or between your [.noloc]`Pods` and resources in any other network. You can use [.noloc]`Kubernetes` _network policy_ to restrict network traffic to and from your [.noloc]`Pods`. For more information, see https://kubernetes.io/docs/concepts/services-networking/network-policies/[Network Policies] in the [.noloc]`Kubernetes` documentation.

If you have version `1.13` or earlier of the [.noloc]`Amazon VPC CNI plugin for Kubernetes` on your cluster, you need to implement a third party solution to apply [.noloc]`Kubernetes` network policies to your cluster. Version `1.14` or later of the plugin can implement network policies, so you don't need to use a third party solution. In this topic, you learn how to configure your cluster to use [.noloc]`Kubernetes` network policy on your cluster without using a third party add-on.

Network policies in the [.noloc]`Amazon VPC CNI plugin for Kubernetes` are supported in the following configurations.



* Amazon EKS clusters of version `1.25` and later.
* Version 1.14 or later of the [.noloc]`Amazon VPC CNI plugin for Kubernetes` on your cluster.
* Cluster configured for `IPv4` or `IPv6` addresses.
* You can use network policies with <<security-groups-for-pods,security groups for Pods>>. With network policies, you can control all in-cluster communication. With security groups for [.noloc]`Pods`, you can control access to {aws} services from applications within a [.noloc]`Pod`.
* You can use network policies with _custom networking_ and _prefix delegation_.


[[cni-network-policy-considerations,cni-network-policy-considerations.title]]
===== Considerations

*Architecture*

* When applying [.noloc]`Amazon VPC CNI plugin for Kubernetes` network policies to your cluster with the [.noloc]`Amazon VPC CNI plugin for Kubernetes` , you can apply the policies to Amazon EC2 Linux nodes only. You can't apply the policies to Fargate or Windows nodes.
* Network policies only apply either `IPv4` or `IPv6` addresses, but not both. In an `IPv4` cluster, the VPC CNI assigns `IPv4` address to pods and applies `IPv4` policies. In an `IPv6` cluster, the VPC CNI assigns `IPv6` address to pods and applies `IPv6` policies. Any `IPv4` network policy rules applied to an `IPv6` cluster are ignored. Any `IPv6` network policy rules applied to an `IPv4` cluster are ignored.

*Network Policies*

* Network Policies are only applied to [.noloc]`Pods` that are part of a [.noloc]`Deployment`. Standalone [.noloc]`Pods` that don't have a `metadata.ownerReferences` set can't have network policies applied to them.
* You can apply multiple network policies to the same [.noloc]`Pod`. When two or more policies that select the same [.noloc]`Pod` are configured, all policies are applied to the [.noloc]`Pod`.
* The maximum number of unique combinations of ports for each protocol in each `ingress:` or `egress:` selector in a network policy is 24.
* For any of your [.noloc]`Kubernetes` services, the service port must be the same as the container port. If you're using named ports, use the same name in the service spec too.

*Migration*

* If your cluster is currently using a third party solution to manage [.noloc]`Kubernetes` network policies, you can use those same policies with the [.noloc]`Amazon VPC CNI plugin for Kubernetes`. However you must remove your existing solution so that it isn't managing the same policies.

*Installation*

* The network policy feature creates and requires a `PolicyEndpoint` Custom Resource Definition (CRD) called `policyendpoints.networking.k8s.aws`. `PolicyEndpoint` objects of the Custom Resource are managed by Amazon EKS. You shouldn't modify or delete these resources.
* If you run pods that use the instance role IAM credentials or connect to the EC2 IMDS, be careful to check for network policies that would block access to the EC2 IMDS. You may need to add a network policy to allow access to EC2 IMDS. For more information, see link:AWSEC2/latest/UserGuide/ec2-instance-metadata.html[Instance metadata and user data,type="documentation"] in the Amazon EC2 User Guide.
+
Pods that use _IAM roles for service accounts_ or _EKS Pod Identity_ don't access EC2 IMDS.
* The [.noloc]`Amazon VPC CNI plugin for Kubernetes` doesn't apply network policies to additional network interfaces for each pod, only the primary interface for each pod (`eth0`). This affects the following architectures:
+
** `IPv6` pods with the `ENABLE_V4_EGRESS` variable set to `true`. This variable enables the `IPv4` egress feature to connect the IPv6 pods to `IPv4` endpoints such as those outside the cluster. The `IPv4` egress feature works by creating an additional network interface with a local loopback IPv4 address.
** When using chained network plugins such as [.noloc]`Multus`. Because these plugins add network interfaces to each pod, network policies aren't applied to the chained network plugins.


[.topic]
[[cni-network-policy-configure,cni-network-policy-configure.title]]
===== Restrict Pod network traffic with [.noloc]`Kubernetes` network policies

[abstract]
--
Learn how to deploy [.noloc]`Kubernetes` network policies on your Amazon EKS cluster.
--

You can use a [.noloc]`Kubernetes` network policy to restrict network traffic to and from your [.noloc]`Pods`. For more information, see https://kubernetes.io/docs/concepts/services-networking/network-policies/[Network Policies] in the [.noloc]`Kubernetes` documentation.

You must configure the following in order to use this feature:

. Set up policy enforcement at [.noloc]`Pod` startup. You do this in the `aws-node` container of the VPC CNI `DaemonSet`.
. Enable the network policy parameter for the add-on.
. Configure your cluster to use the [.noloc]`Kubernetes` network policy

Before you begin, review the considerations. For more information, see <<cni-network-policy-considerations>>.

[[cni-network-policy-prereqs,cni-network-policy-prereqs.title]]
====== Prerequisites 

The following are prerequisites for the feature:



* 
.Minimum cluster version
An existing Amazon EKS cluster. To deploy one, see <<getting-started>>. The cluster must be [.noloc]`Kubernetes` version `1.25` or later. The cluster must be running one of the [.noloc]`Kubernetes` versions and platform versions listed in the following table. Note that any [.noloc]`Kubernetes` and platform versions later than those listed are also supported. You can check your current [.noloc]`Kubernetes` version by replacing [.replaceable]`my-cluster` in the following command with the name of your cluster and then running the modified command:
+
[source,bash,subs="verbatim,attributes"]
----
aws eks describe-cluster
              --name my-cluster --query cluster.version --output
              text
----
+
[cols="1,1", options="header"]
|===
|Kubernetes version
|Platform version


|`1.27.4`
|`eks.5`

|`1.26.7`
|`eks.6`

|`1.25.12`
|`eks.7`
|===
* 
.Minimum VPC CNI version
Version `1.14` or later of the [.noloc]`Amazon VPC CNI plugin for Kubernetes` on your cluster. You can see which version that you currently have with the following command.
+
[source,shell,subs="verbatim,attributes"]
----
kubectl describe daemonset aws-node --namespace kube-system | grep amazon-k8s-cni: | cut -d : -f 3
----
+
If your version is earlier than `1.14`, see <<vpc-add-on-update>> to upgrade to version `1.14` or later.
* 
.Minimum Linux kernel version
Your nodes must have Linux kernel version `5.10` or later. You can check your kernel version with `uname -r`. If you're using the latest versions of the Amazon EKS optimized Amazon Linux, Amazon EKS optimized accelerated Amazon Linux AMIs, and Bottlerocket AMIs, they already have the required kernel version.
+
The Amazon EKS optimized accelerated Amazon Linux AMI version `v20231116` or later have kernel version `5.10`.


[[cni-network-policy-configure-policy,cni-network-policy-configure-policy.title]]
====== Step 1: Set up policy enforcement at [.noloc]`Pod` startup


The [.noloc]`Amazon VPC CNI plugin for Kubernetes` configures network policies for pods in parallel with the pod provisioning. Until all of the policies are configured for the new pod, containers in the new pod will start with a  _default allow policy_. This is called _standard mode_. A default allow policy means that all ingress and egress traffic is allowed to and from the new pods. For example, the pods will not have any firewall rules enforced (all traffic is allowed) until the new pod is updated with the active policies. 

With the `NETWORK_POLICY_ENFORCING_MODE` variable set to `strict`, pods that use the VPC CNI start with a _default deny policy_, then policies are configured. This is called _strict mode_. In strict mode, you must have a network policy for every endpoint that your pods need to access in your cluster. Note that this requirement applies to the [.noloc]`CoreDNS` pods. The default deny policy isn't configured for pods with Host networking.

You can change the default network policy by setting the environment variable `NETWORK_POLICY_ENFORCING_MODE` to `strict` in the `aws-node` container of the VPC CNI `DaemonSet`.

[source,yaml,subs="verbatim,attributes"]
----
env:
  - name: NETWORK_POLICY_ENFORCING_MODE
    value: "strict"
----


[[enable-network-policy-parameter,enable-network-policy-parameter.title]]
====== Step 2: Enable the network policy parameter for the add-on

The network policy feature uses port `8162` on the node for metrics by default. Also, the feature used port `8163` for health probes. If you run another application on the nodes or inside pods that needs to use these ports, the app fails to run. In VPC CNI version `v1.14.1` or later, you can change these ports.

Use the following procedure to enable the network policy parameter for the add-on.



{aws-management-console}::
.. Open the link:eks/home#/clusters[Amazon EKS console,type="console"].
.. In the left navigation pane, select *Clusters*, and then select the name of the cluster that you want to configure the Amazon VPC CNI add-on for.
.. Choose the *Add-ons* tab.
.. Select the box in the top right of the add-on box and then choose *Edit*.
.. On the *Configure [.replaceable]`name of add-on`* page:
+
... Select a `v1.14.0-eksbuild.3` or later version in the *Version* list.
... Expand the *Optional configuration settings*.
... Enter the JSON key `"enableNetworkPolicy":` and value `"true"` in *Configuration values*. The resulting text must be a valid JSON object. If this key and value are the only data in the text box, surround the key and value with curly braces `{ }`.
+
The following example has network policy feature enabled and metrics and health probes are set to the default port numbers:
+
[source,json,subs="verbatim,attributes"]
----
{
    "enableNetworkPolicy": "true",
    "nodeAgent": {
        "healthProbeBindAddr": "8163",
        "metricsBindAddr": "8162"
    }
}
----


Helm::

If you have installed the [.noloc]`Amazon VPC CNI plugin for Kubernetes` through `helm`, you can update the configuration to change the ports.

.. Run the following command to change the ports. Set the port number in the value for either key `nodeAgent.metricsBindAddr` or key `nodeAgent.healthProbeBindAddr`, respectively.
+
[source,shell,subs="verbatim,attributes"]
----
helm upgrade --set nodeAgent.metricsBindAddr=8162 --set nodeAgent.healthProbeBindAddr=8163 aws-vpc-cni --namespace kube-system eks/aws-vpc-cni
----


[.noloc]`kubectl`::
.. Open the `aws-node` `DaemonSet` in your editor.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl edit daemonset -n kube-system aws-node
----
.. Replace the port numbers in the following command arguments in the `args:` in the `aws-network-policy-agent` container in the VPC CNI `aws-node` daemonset manifest.
+
[source,yaml,subs="verbatim,attributes"]
----
    - args:
            - --metrics-bind-addr=:8162
            - --health-probe-bind-addr=:8163
----


[[cni-mount-bpf,cni-mount-bpf.title]]
====== Step 3: Mount the Berkeley Packet Filter (BPF) file system on your nodes

You must mount the Berkeley Packet Filter (BPF) file system on each of your nodes.

[NOTE]
====

If your cluster is version `1.27` or later, you can skip this step as all Amazon EKS optimized Amazon Linux and Bottlerocket AMIs for `1.27` or later have this feature already.

For all other cluster versions, if you upgrade the Amazon EKS optimized Amazon Linux to version `v20230703` or later or you upgrade the Bottlerocket AMI to version `v1.0.2` or later, you can skip this step.

====
. Mount the Berkeley Packet Filter (BPF) file system on each of your nodes.
+
[source,shell,subs="verbatim,attributes"]
----
sudo mount -t bpf bpffs /sys/fs/bpf
----
. Then, add the same command to your user data in your launch template for your Amazon EC2 Auto Scaling Groups.


[[cni-network-policy-setup,cni-network-policy-setup.title]]
====== Step 4: Configure your cluster to use [.noloc]`Kubernetes` network policies

Configure the cluster to use [.noloc]`Kubernetes` network policies. You can set this for an Amazon EKS add-on or self-managed add-on.


[[cni-network-policy-setup-procedure-add-on,cni-network-policy-setup-procedure-add-on.title]]
.Amazon EKS add-on
[%collapsible]
====

{aws-management-console}::
.. Open the link:eks/home#/clusters[Amazon EKS console,type="console"].
.. In the left navigation pane, select *Clusters*, and then select the name of the cluster that you want to configure the Amazon VPC CNI add-on for.
.. Choose the *Add-ons* tab.
.. Select the box in the top right of the add-on box and then choose *Edit*.
.. On the *Configure [.replaceable]`name of addon`* page:
+
... Select a `v1.14.0-eksbuild.3` or later version in the *Version* list.
... Expand the *Optional configuration settings*.
... Enter the JSON key `"enableNetworkPolicy":` and value `"true"` in *Configuration values*. The resulting text must be a valid JSON object. If this key and value are the only data in the text box, surround the key and value with curly braces `{ }`. The following example shows network policy is enabled:
+
[source,json,subs="verbatim,attributes"]
----
{ "enableNetworkPolicy": "true" }
----
+
The following screenshot shows an example of this scenario.
+
image::images/console-cni-config-network-policy.png[{aws-management-console} showing the VPC CNI add-on with network policy in the optional configuration.,scaledwidth=80%]


{aws} CLI::
.. Run the following {aws} CLI command. Replace `my-cluster` with the name of your cluster and the IAM role ARN with the role that you are using.
+
[source,shell,subs="verbatim,attributes"]
----
aws eks update-addon --cluster-name my-cluster --addon-name vpc-cni --addon-version v1.14.0-eksbuild.3 \
    --service-account-role-arn {arn-aws}iam::123456789012:role/AmazonEKSVPCCNIRole \
    --resolve-conflicts PRESERVE --configuration-values '{"enableNetworkPolicy": "true"}'
----

====

[[cni-network-policy-setup-procedure-self-managed-add-on,cni-network-policy-setup-procedure-self-managed-add-on.title]]
.Self-managed add-on
[%collapsible]
====

Helm::

If you have installed the [.noloc]`Amazon VPC CNI plugin for Kubernetes` through `helm`, you can update the configuration to enable network policy.

.. Run the following command to enable network policy.
+
[source,shell,subs="verbatim,attributes"]
----
helm upgrade --set enableNetworkPolicy=true aws-vpc-cni --namespace kube-system eks/aws-vpc-cni
----


[.noloc]`kubectl`::
.. Open the `amazon-vpc-cni` `ConfigMap` in your editor.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl edit configmap -n kube-system amazon-vpc-cni -o yaml
----
.. Add the following line to the `data` in the `ConfigMap`.
+
[source,bash,subs="verbatim,attributes"]
----
enable-network-policy-controller: "true"
----
+
Once you've added the line, your `ConfigMap` should look like the following example.
+
[source,yaml,subs="verbatim,attributes"]
----
apiVersion: v1
 kind: ConfigMap
 metadata:
  name: amazon-vpc-cni
  namespace: kube-system
 data:
  enable-network-policy-controller: "true"
----
.. Open the `aws-node` `DaemonSet` in your editor.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl edit daemonset -n kube-system aws-node
----
.. Replace the `false` with `true` in the command argument `--enable-network-policy=false` in the `args:` in the `aws-network-policy-agent` container in the VPC CNI `aws-node` daemonset manifest.
+
[source,yaml,subs="verbatim,attributes"]
----
     - args:
        - --enable-network-policy=true
----

====

[[cni-network-policy-setup-procedure-confirm,cni-network-policy-setup-procedure-confirm.title]]
====== Step 5. Next steps

After you complete the configuration, confirm that the `aws-node` pods are running on your cluster.

[source,bash,subs="verbatim,attributes"]
----
kubectl get pods -n kube-system | grep 'aws-node\|amazon'
----

An example output is as follows.

[source,bash,subs="verbatim,attributes"]
----
aws-node-gmqp7                                          2/2     Running   1 (24h ago)   24h
aws-node-prnsh                                          2/2     Running   1 (24h ago)   24h
----

There are 2 containers in the `aws-node` pods in versions `1.14` and later. In previous versions and if network policy is disabled, there is only a single container in the `aws-node` pods.

You can now deploy [.noloc]`Kubernetes` network policies to your cluster.  

To implement [.noloc]`Kubernetes` network policies you create [.noloc]`Kubernetes` `NetworkPolicy` objects and deploy them to your cluster. `NetworkPolicy` objects are scoped to a namespace. You implement policies to allow or deny traffic between [.noloc]`Pods` based on label selectors, namespaces, and IP address ranges. For more information about creating `NetworkPolicy` objects, see https://kubernetes.io/docs/concepts/services-networking/network-policies/#networkpolicy-resource[Network Policies] in the [.noloc]`Kubernetes` documentation.

Enforcement of [.noloc]`Kubernetes` `NetworkPolicy` objects is implemented using the [.noloc]`Extended Berkeley Packet Filter` ([.noloc]`eBPF`). Relative to `iptables` based implementations, it offers lower latency and performance characteristics, including reduced CPU utilization and avoiding sequential lookups. Additionally, [.noloc]`eBPF` probes provide access to context rich data that helps debug complex kernel level issues and improve observability. Amazon EKS supports an [.noloc]`eBPF`-based exporter that leverages the probes to log policy results on each node and export the data to external log collectors to aid in debugging. For more information, see the https://ebpf.io/what-is-ebpf/#what-is-ebpf[eBPF documentation].

[.topic]
[[network-policy-disable,network-policy-disable.title]]
===== Disable [.noloc]`Kubernetes` network policies for Amazon EKS Pod network traffic

[abstract]
--
Learn how to disable [.noloc]`Kubernetes` network policies for Amazon EKS Pod network traffic.
--

Disable [.noloc]`Kubernetes` network policies to stop restricting Amazon EKS Pod network traffic

. List all Kubernetes network policies.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl get netpol -A
----
. Delete each Kubernetes network policy. You must delete all network policies before disabling network policies. 
+
[source,bash,subs="verbatim,attributes"]
----
kubectl delete netpol <policy-name>
----
. Open the aws-node DaemonSet in your editor.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl edit daemonset -n kube-system aws-node
----
. Replace the `true` with `false` in the command argument `--enable-network-policy=true` in the `args:` in the `aws-network-policy-agent` container in the VPC CNI `aws-node` daemonset manifest.
+
[source,yaml,subs="verbatim,attributes"]
----
     - args:
        - --enable-network-policy=true
----


include::network-policies-troubleshooting.adoc[leveloffset=+1]


include::network-policy-stars-demo.adoc[leveloffset=+1]


[.topic]
[[cni-custom-network,cni-custom-network.title]]
==== Deploy [.noloc]`pods` in alternate subnets with custom networking

[abstract]
--
Learn how to enable custom networking for Amazon EKS [.noloc]`Pods` to deploy them in different subnets or use different security groups than the node's primary network interface, increasing IP address availability and network isolation.
--

*Applies to*: [.noloc]`Linux` `IPv4` Fargate nodes, [.noloc]`Linux` nodes with Amazon EC2 instances  

By default, when the [.noloc]`Amazon VPC CNI plugin for Kubernetes` creates secondary link:AWSEC2/latest/UserGuide/using-eni.html[elastic network interfaces,type="documentation"] (network interfaces) for your Amazon EC2 node, it creates them in the same subnet as the node's primary network interface. It also associates the same security groups to the secondary network interface that are associated to the primary network interface. For one or more of the following reasons, you might want the plugin to create secondary network interfaces in a different subnet or want to associate different security groups to the secondary network interfaces, or both: 



* There's a limited number of `IPv4` addresses that are available in the subnet that the primary network interface is in. This might limit the number of [.noloc]`Pods` that you can create in the subnet. By using a different subnet for secondary network interfaces, you can increase the number of available `IPv4` addresses available for [.noloc]`Pods`.
* For security reasons, your [.noloc]`Pods` might need to use a different subnet or security groups than the node's primary network interface.
* The nodes are configured in public subnets, and you want to place the [.noloc]`Pods` in private subnets. The route table associated to a public subnet includes a route to an internet gateway. The route table associated to a private subnet doesn't include a route to an internet gateway.


[[cni-custom-network-considerations,cni-custom-network-considerations.title]]
===== Considerations

The following are considerations for using the feature.



* With custom networking enabled, no IP addresses assigned to the primary network interface are assigned to [.noloc]`Pods`. Only IP addresses from secondary network interfaces are assigned to [.noloc]`Pods`.
* If your cluster uses the `IPv6` family, you can't use custom networking.
* If you plan to use custom networking only to help alleviate `IPv4` address exhaustion, you can create a cluster using the `IPv6` family instead. For more information, see <<cni-ipv6>>.
* Even though [.noloc]`Pods` deployed to subnets specified for secondary network interfaces can use different subnet and security groups than the node's primary network interface, the subnets and security groups must be in the same VPC as the node.
* For Fargate, subnets are controlled through the Fargate profile. For more information, see <<fargate-profile>>.


[.topic]
[[cni-custom-network-tutorial,cni-custom-network-tutorial.title]]
===== Customizing the secondary network interface in Amazon EKS nodes

[abstract]
--
Learn how your [.noloc]`Pods` can use different security groups and subnets than the primary elastic network interface of the Amazon EC2 node that they run on.
--

Complete the following before you start the tutorial:



* Review the considerations
* Familiarity with how the [.noloc]`Amazon VPC CNI plugin for Kubernetes` creates secondary network interfaces and assigns IP addresses to [.noloc]`Pods`. For more information, see https://github.com/aws/amazon-vpc-cni-k8s#eni-allocation[ENI Allocation] on [.noloc]`GitHub`.
* Version `2.12.3` or later or version `1.27.160` or later of the {aws} Command Line Interface ({aws} CLI) installed and configured on your device or {aws} CloudShell. To check your current version, use `aws --version | cut -d / -f2 | cut -d ' ' -f1`. Package managers such `yum`, `apt-get`, or [.noloc]`Homebrew` for [.noloc]`macOS` are often several versions behind the latest version of the {aws} CLI. To install the latest version, see link:cli/latest/userguide/cli-chap-install.html[Installing, updating, and uninstalling the {aws} CLI,type="documentation"] and  link:cli/latest/userguide/cli-configure-quickstart.html#cli-configure-quickstart-config[Quick configuration with aws configure,type="documentation"] in the _{aws} Command Line Interface User Guide_. The {aws} CLI version that is installed in {aws} CloudShell might also be several versions behind the latest version. To update it, see  link:cloudshell/latest/userguide/vm-specs.html#install-cli-software[Installing {aws} CLI to your home directory,type="documentation"] in the _{aws} CloudShell User Guide_.
* The `kubectl` command line tool is installed on your device or {aws} CloudShell. The version can be the same as or up to one minor version earlier or later than the [.noloc]`Kubernetes` version of your cluster. For example, if your cluster version is `1.29`, you can use `kubectl` version `1.28`, `1.29`, or `1.30` with it. To install or upgrade `kubectl`, see <<install-kubectl>>.
* We recommend that you complete the steps in this topic in a Bash shell. If you aren't using a Bash shell, some script commands such as line continuation characters and the way variables are set and used require adjustment for your shell. Additionally, the quoting and escaping rules for your shell might be different. For more information, see link:cli/latest/userguide/cli-usage-parameters-quoting-strings.html[Using quotation marks with strings in the {aws} CLI,type="documentation"] in the {aws} Command Line Interface User Guide.

For this tutorial, we recommend using the [.replaceable]`example values`, except where it's noted to replace them. You can replace any [.replaceable]`example value` when completing the steps for a production cluster. We recommend completing all steps in the same terminal. This is because variables are set and used throughout the steps and won't exist in different terminals.

The commands in this topic are formatted using the conventions listed in link:cli/latest/userguide/welcome-examples.html[Using the {aws} CLI examples,type="documentation"]. If you're running commands from the command line against resources that are in a different {aws} Region than the default {aws} Region defined in the {aws} CLI  link:cli/latest/userguide/cli-configure-quickstart.html#cli-configure-quickstart-profiles[profile,type="documentation"] that you're using, then you need to add `--region [.replaceable]``region-code``` to the commands.

When you want to deploy custom networking to your production cluster, skip to <<custom-networking-configure-vpc,Step 2: Configure your VPC>>.

[[custom-networking-create-cluster,custom-networking-create-cluster.title]]
====== Step 1: Create a test VPC and cluster

The following procedures help you create a test VPC and cluster and configure custom networking for that cluster. We don't recommend using the test cluster for production workloads because several unrelated features that you might use on your production cluster aren't covered in this topic. For more information, see <<create-cluster>>.

. Define the `cluster_name` and `account_id` variables..
+
[source,bash,subs="verbatim,attributes"]
----
export cluster_name=my-custom-networking-cluster
account_id=$(aws sts get-caller-identity --query Account --output text)
----
. Create a VPC.
+
.. If you are deploying to a test system, create a VPC using an Amazon EKS {aws} CloudFormation template.
+
[source,bash,subs="verbatim,attributes"]
----
aws cloudformation create-stack --stack-name my-eks-custom-networking-vpc \
  --template-url https://s3.us-west-2.amazonaws.com/amazon-eks/cloudformation/2020-10-29/amazon-eks-vpc-private-subnets.yaml \
  --parameters ParameterKey=VpcBlock,ParameterValue=192.168.0.0/24 \
  ParameterKey=PrivateSubnet01Block,ParameterValue=192.168.0.64/27 \
  ParameterKey=PrivateSubnet02Block,ParameterValue=192.168.0.96/27 \
  ParameterKey=PublicSubnet01Block,ParameterValue=192.168.0.0/27 \
  ParameterKey=PublicSubnet02Block,ParameterValue=192.168.0.32/27
----
+
The {aws} CloudFormation stack takes a few minutes to create. To check on the stack's deployment status, run the following command.
+
[source,bash,subs="verbatim,attributes"]
----
aws cloudformation describe-stacks --stack-name my-eks-custom-networking-vpc --query Stacks\[\].StackStatus  --output text
----
+
Don't continue to the next step until the output of the command is `CREATE_COMPLETE`.
.. Define variables with the values of the private subnet IDs created by the template.
+
[source,bash,subs="verbatim,attributes"]
----
subnet_id_1=$(aws cloudformation describe-stack-resources --stack-name my-eks-custom-networking-vpc \
    --query "StackResources[?LogicalResourceId=='PrivateSubnet01'].PhysicalResourceId" --output text)
subnet_id_2=$(aws cloudformation describe-stack-resources --stack-name my-eks-custom-networking-vpc \
    --query "StackResources[?LogicalResourceId=='PrivateSubnet02'].PhysicalResourceId" --output text)
----
.. Define variables with the Availability Zones of the subnets retrieved in the previous step.
+
[source,bash,subs="verbatim,attributes"]
----
az_1=$(aws ec2 describe-subnets --subnet-ids $subnet_id_1 --query 'Subnets[*].AvailabilityZone' --output text)
az_2=$(aws ec2 describe-subnets --subnet-ids $subnet_id_2 --query 'Subnets[*].AvailabilityZone' --output text)
----
. Create a cluster IAM role.
+
.. Run the following command to create an IAM trust policy JSON file. 
+
[source,json,subs="verbatim,attributes"]
----
cat >eks-cluster-role-trust-policy.json <<EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "eks.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF
----
.. Create the Amazon EKS cluster IAM role. If necessary, preface `eks-cluster-role-trust-policy.json` with the path on your computer that you wrote the file to in the previous step. The command associates the trust policy that you created in the previous step to the role. To create an IAM role, the  link:IAM/latest/UserGuide/id_roles.html#iam-term-principal[IAM principal,type="documentation"] that is creating the role must be assigned the `iam:CreateRole` action (permission).
+
[source,bash,subs="verbatim,attributes"]
----
aws iam create-role --role-name myCustomNetworkingAmazonEKSClusterRole --assume-role-policy-document file://"eks-cluster-role-trust-policy.json"
----
.. Attach the Amazon EKS managed policy named link:{arn-aws}iam::aws:policy/AmazonEKSClusterPolicy[AmazonEKSClusterPolicy,type="console"] to the role. To attach an IAM policy to an  link:IAM/latest/UserGuide/id_roles.html#iam-term-principal[IAM principal,type="documentation"], the principal that is attaching the policy must be assigned one of the following IAM actions (permissions): `iam:AttachUserPolicy` or `iam:AttachRolePolicy`.
+
[source,bash,subs="verbatim,attributes"]
----
aws iam attach-role-policy --policy-arn {arn-aws}iam::aws:policy/AmazonEKSClusterPolicy --role-name myCustomNetworkingAmazonEKSClusterRole
----
. Create an Amazon EKS cluster and configure your device to communicate with it.
+
.. Create a cluster.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks create-cluster --name my-custom-networking-cluster \
   --role-arn {arn-aws}iam::$account_id:role/myCustomNetworkingAmazonEKSClusterRole \
   --resources-vpc-config subnetIds=$subnet_id_1","$subnet_id_2
----
+
NOTE: You might receive an error that one of the Availability Zones in your request doesn't have sufficient capacity to create an Amazon EKS cluster. If this happens, the error output contains the Availability Zones that can support a new cluster. Retry creating your cluster with at least two subnets that are located in the supported Availability Zones for your account. For more information, see <<ice>>.
.. The cluster takes several minutes to create. To check on the cluster's deployment status, run the following command.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks describe-cluster --name my-custom-networking-cluster --query cluster.status
----
+
Don't continue to the next step until the output of the command is `"ACTIVE"`.
.. Configure `kubectl` to communicate with your cluster.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks update-kubeconfig --name my-custom-networking-cluster
----


[[custom-networking-configure-vpc,custom-networking-configure-vpc.title]]
====== Step 2: Configure your VPC

This tutorial requires the VPC created in <<custom-networking-create-cluster,Step 1: Create a test VPC and cluster>>. For a production cluster, adjust the steps accordingly for your VPC by replacing all of the [.replaceable]`example values` with your own.

. Confirm that your currently-installed [.noloc]`Amazon VPC CNI plugin for Kubernetes` is the latest version. To determine the latest version for the Amazon EKS add-on type and update your version to it, see <<updating-an-add-on>>. To determine the latest version for the self-managed add-on type and update your version to it, see <<managing-vpc-cni>>.
. Retrieve the ID of your cluster VPC and store it in a variable for use in later steps. For a production cluster, replace [.replaceable]`my-custom-networking-cluster` with the name of your cluster.
+
[source,bash,subs="verbatim,attributes"]
----
vpc_id=$(aws eks describe-cluster --name my-custom-networking-cluster --query "cluster.resourcesVpcConfig.vpcId" --output text)
----
. Associate an additional Classless Inter-Domain Routing (CIDR) block with your cluster's VPC. The CIDR block can't overlap with any existing associated CIDR blocks.
+
.. View the current CIDR blocks associated to your VPC.
+
[source,bash,subs="verbatim,attributes"]
----
aws ec2 describe-vpcs --vpc-ids $vpc_id \
    --query 'Vpcs[*].CidrBlockAssociationSet[*].{CIDRBlock: CidrBlock, State: CidrBlockState.State}' --out table
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
----------------------------------
|          DescribeVpcs          |
+-----------------+--------------+
|    CIDRBlock    |    State     |
+-----------------+--------------+
|  192.168.0.0/24 |  associated  |
+-----------------+--------------+
----
.. Associate an additional CIDR block to your VPC. For more information, see  link:vpc/latest/userguide/modify-vpcs.html#add-ipv4-cidr[Associate additional IPv4 CIDR blocks with your VPC,type="documentation"] in the Amazon VPC User Guide.
+
[source,bash,subs="verbatim,attributes"]
----
aws ec2 associate-vpc-cidr-block --vpc-id $vpc_id --cidr-block 192.168.1.0/24
----
.. Confirm that the new block is associated.
+
[source,bash,subs="verbatim,attributes"]
----
aws ec2 describe-vpcs --vpc-ids $vpc_id --query 'Vpcs[*].CidrBlockAssociationSet[*].{CIDRBlock: CidrBlock, State: CidrBlockState.State}' --out table
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
----------------------------------
|          DescribeVpcs          |
+-----------------+--------------+
|    CIDRBlock    |    State     |
+-----------------+--------------+
|  192.168.0.0/24 |  associated  |
|  192.168.1.0/24 |  associated  |
+-----------------+--------------+
----

+
Don't proceed to the next step until your new CIDR block's `State` is `associated`.
. Create as many subnets as you want to use in each Availability Zone that your existing subnets are in. Specify a CIDR block that's within the CIDR block that you associated with your VPC in a previous step. 
+
.. Create new subnets. The subnets must be created in a different VPC CIDR block than your existing subnets are in, but in the same Availability Zones as your existing subnets. In this example, one subnet is created in the new CIDR block in each Availability Zone that the current private subnets exist in. The IDs of the subnets created are stored in variables for use in later steps. The `Name` values match the values assigned to the subnets created using the Amazon EKS VPC template in a previous step. Names aren't required. You can use different names. 
+
[source,bash,subs="verbatim,attributes"]
----
new_subnet_id_1=$(aws ec2 create-subnet --vpc-id $vpc_id --availability-zone $az_1 --cidr-block 192.168.1.0/27 \
    --tag-specifications 'ResourceType=subnet,Tags=[{Key=Name,Value=my-eks-custom-networking-vpc-PrivateSubnet01},{Key=kubernetes.io/role/internal-elb,Value=1}]' \
    --query Subnet.SubnetId --output text)
new_subnet_id_2=$(aws ec2 create-subnet --vpc-id $vpc_id --availability-zone $az_2 --cidr-block 192.168.1.32/27 \
    --tag-specifications 'ResourceType=subnet,Tags=[{Key=Name,Value=my-eks-custom-networking-vpc-PrivateSubnet02},{Key=kubernetes.io/role/internal-elb,Value=1}]' \
    --query Subnet.SubnetId --output text)
----
+
IMPORTANT: By default, your new subnets are implicitly associated with your VPC's  link:vpc/latest/userguide/VPC_Route_Tables.html#RouteTables[main route table,type="documentation"]. This route table allows communication between all the resources that are deployed in the VPC. However, it doesn't allow communication with resources that have IP addresses that are outside the CIDR blocks that are associated with your VPC. You can associate your own route table to your subnets to change this behavior. For more information, see  link:vpc/latest/userguide/VPC_Route_Tables.html#subnet-route-tables[Subnet route tables,type="documentation"] in the Amazon VPC User Guide.
.. View the current subnets in your VPC.
+
[source,bash,subs="verbatim,attributes"]
----
aws ec2 describe-subnets --filters "Name=vpc-id,Values=$vpc_id" \
    --query 'Subnets[*].{SubnetId: SubnetId,AvailabilityZone: AvailabilityZone,CidrBlock: CidrBlock}' \
    --output table
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
----------------------------------------------------------------------
|                           DescribeSubnets                          |
+------------------+--------------------+----------------------------+
| AvailabilityZone |     CidrBlock      |         SubnetId           |
+------------------+--------------------+----------------------------+
|  us-west-2d      |  192.168.0.0/27    |     subnet-example1        |
|  us-west-2a      |  192.168.0.32/27   |     subnet-example2        |
|  us-west-2a      |  192.168.0.64/27   |     subnet-example3        |
|  us-west-2d      |  192.168.0.96/27   |     subnet-example4        |
|  us-west-2a      |  192.168.1.0/27    |     subnet-example5        |
|  us-west-2d      |  192.168.1.32/27   |     subnet-example6        |
+------------------+--------------------+----------------------------+
----
+
You can see the subnets in the `192.168.1.0` CIDR block that you created are in the same Availability Zones as the subnets in the `192.168.0.0` CIDR block.


[[custom-networking-configure-kubernetes,custom-networking-configure-kubernetes.title]]
====== Step 3: Configure [.noloc]`Kubernetes` resources
. Set the `AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG` environment variable to `true` in the `aws-node` [.noloc]`DaemonSet`.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl set env daemonset aws-node -n kube-system AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG=true
----
. Retrieve the ID of your <<sec-group-reqs,cluster security group>> and store it in a variable for use in the next step. Amazon EKS automatically creates this security group when you create your cluster.
+
[source,bash,subs="verbatim,attributes"]
----
cluster_security_group_id=$(aws eks describe-cluster --name $cluster_name --query cluster.resourcesVpcConfig.clusterSecurityGroupId --output text)
----
. [[custom-networking-create-eniconfig]]Create an `ENIConfig` custom resource for each subnet that you want to deploy [.noloc]`Pods` in.
+
.. Create a unique file for each network interface configuration.
+
+
The following commands create separate `ENIConfig` files for the two subnets that were created in a previous step. The value for `name` must be unique. The name is the same as the Availability Zone that the subnet is in. The cluster security group is assigned to the `ENIConfig`.
+
[source,yaml,subs="verbatim,attributes"]
----
cat >$az_1.yaml <<EOF
apiVersion: crd.k8s.amazonaws.com/v1alpha1
kind: ENIConfig
metadata: 
  name: $az_1
spec: 
  securityGroups: 
    - $cluster_security_group_id
  subnet: $new_subnet_id_1
EOF
----
[source,yaml,subs="verbatim,attributes"]
----
cat >$az_2.yaml <<EOF
apiVersion: crd.k8s.amazonaws.com/v1alpha1
kind: ENIConfig
metadata: 
  name: $az_2
spec: 
  securityGroups: 
    - $cluster_security_group_id
  subnet: $new_subnet_id_2
EOF
----
+
For a production cluster, you can make the following changes to the previous commands:
+
*** Replace [.replaceable]`$cluster_security_group_id` with the ID of an existing link:AWSEC2/latest/UserGuide/ec2-security-groups.html[security group,type="documentation"] that you want to use for each `ENIConfig`.
*** We recommend naming your `ENIConfigs` the same as the Availability Zone that you'll use the `ENIConfig` for, whenever possible. You might need to use different names for your `ENIConfigs` than the names of the Availability Zones for a variety of reasons. For example, if you have more than two subnets in the same Availability Zone and want to use them both with custom networking, then you need multiple `ENIConfigs` for the same Availability Zone. Since each `ENIConfig` requires a unique name, you can't name more than one of your `ENIConfigs` using the Availability Zone name.
+
If your `ENIConfig` names aren't all the same as Availability Zone names, then replace [.replaceable]`$az_1` and [.replaceable]`$az_2` with your own names in the previous commands and <<custom-networking-annotate-eniconfig,annotate your nodes with the ENIConfig>> later in this tutorial.
+
NOTE: If you don't specify a valid security group for use with a production cluster and you're using:

*** version `1.8.0` or later of the [.noloc]`Amazon VPC CNI plugin for Kubernetes`, then the security groups associated with the node's primary elastic network interface are used.
*** a version of the [.noloc]`Amazon VPC CNI plugin for Kubernetes` that's earlier than `1.8.0`, then the default security group for the VPC is assigned to secondary network interfaces.

+
IMPORTANT: 
*** `AWS_VPC_K8S_CNI_EXTERNALSNAT=false` is a default setting in the configuration for the Amazon VPC CNI plugin for [.noloc]`Kubernetes`. If you're using the default setting, then traffic that is destined for IP addresses that aren't within one of the CIDR blocks associated with your VPC use the security groups and subnets of your node's primary network interface. The subnets and security groups defined in your `ENIConfigs` that are used to create secondary network interfaces aren't used for this traffic. For more information about this setting, see <<external-snat>>.
*** If you also use security groups for [.noloc]`Pods`, the security group that's specified in a `SecurityGroupPolicy` is used instead of the security group that's specified in the `ENIConfigs`. For more information, see <<security-groups-for-pods>>.

.. Apply each custom resource file that you created to your cluster with the following commands.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl apply -f $az_1.yaml
kubectl apply -f $az_2.yaml
----
. Confirm that your `ENIConfigs` were created.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl get ENIConfigs
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
NAME         AGE
us-west-2a   117s
us-west-2d   105s
----
. If you're enabling custom networking on a production cluster and named your `ENIConfigs` something other than the Availability Zone that you're using them for, then skip to the <<custom-networking-deploy-nodes,next step>> to deploy Amazon EC2 nodes.
+
Enable [.noloc]`Kubernetes` to automatically apply the `ENIConfig` for an Availability Zone to any new Amazon EC2 nodes created in your cluster.
+
.. For the test cluster in this tutorial, skip to the <<custom-networking-automatically-apply-eniconfig,next step>>.
+
For a production cluster, check to see if an [.noloc]`annotation` with the key `k8s.amazonaws.com/eniConfig` for the `https://github.com/aws/amazon-vpc-cni-k8s#eni_config_annotation_def[ENI_CONFIG_ANNOTATION_DEF]` environment variable exists in the container spec for the `aws-node` [.noloc]`DaemonSet`.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl describe daemonset aws-node -n kube-system | grep ENI_CONFIG_ANNOTATION_DEF
----
+
If output is returned, the annotation exists. If no output is returned, then the variable is not set. For a production cluster, you can use either this setting or the setting in the following step. If you use this setting, it overrides the setting in the following step. In this tutorial, the setting in the next step is used.
.. [[custom-networking-automatically-apply-eniconfig]]Update your `aws-node` [.noloc]`DaemonSet` to automatically apply the `ENIConfig` for an Availability Zone to any new Amazon EC2 nodes created in your cluster.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl set env daemonset aws-node -n kube-system ENI_CONFIG_LABEL_DEF=topology.kubernetes.io/zone
----


[[custom-networking-deploy-nodes,custom-networking-deploy-nodes.title]]
====== Step 4: Deploy Amazon EC2 nodes
. Create a node IAM role.
+
.. Run the following command to create an IAM trust policy JSON file.
+
[source,json,subs="verbatim,attributes"]
----
cat >node-role-trust-relationship.json <<EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "ec2.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF
----
.. Run the following command to set a variable for your role name. You can replace [.replaceable]`myCustomNetworkingNodeRole` with any name you choose.
+
[source,bash,subs="verbatim,attributes"]
----
export node_role_name=myCustomNetworkingNodeRole
----
.. Create the IAM role and store its returned Amazon Resource Name (ARN) in a variable for use in a later step.
+
[source,bash,subs="verbatim,attributes"]
----
node_role_arn=$(aws iam create-role --role-name $node_role_name --assume-role-policy-document file://"node-role-trust-relationship.json" \
    --query Role.Arn --output text)
----
.. Attach three required IAM managed policies to the IAM role.
+
[source,bash,subs="verbatim,attributes"]
----
aws iam attach-role-policy \
  --policy-arn {arn-aws}iam::aws:policy/AmazonEKSWorkerNodePolicy \
  --role-name $node_role_name
aws iam attach-role-policy \
  --policy-arn {arn-aws}iam::aws:policy/AmazonEC2ContainerRegistryReadOnly \
  --role-name $node_role_name
aws iam attach-role-policy \
    --policy-arn {arn-aws}iam::aws:policy/AmazonEKS_CNI_Policy \
    --role-name $node_role_name
----
+
IMPORTANT: For simplicity in this tutorial, the  link:aws-managed-policy/latest/reference/AmazonEKS_CNI_Policy.html[AmazonEKS_CNI_Policy,type="documentation"] policy is attached to the node IAM role. In a production cluster however, we recommend attaching the policy to a separate IAM role that is used only with the [.noloc]`Amazon VPC CNI plugin for Kubernetes`. For more information, see <<cni-iam-role>>.
. Create one of the following types of node groups. To determine the instance type that you want to deploy, see <<choosing-instance-type>>. For this tutorial, complete the *Managed*, *Without a launch template or with a launch template without an AMI ID specified* option. If you're going to use the node group for production workloads, then we recommend that you familiarize yourself with all of the managed node group <<create-managed-node-group>> and self-managed node group <<worker>> options before deploying the node group.
+
** *Managed*  Deploy your node group using one of the following options:
+
*** *Without a launch template or with a launch template without an AMI ID specified*  Run the following command. For this tutorial, use the [.replaceable]`example values`. For a production node group, replace all [.replaceable]`example values` with your own. The node group name can't be longer than 63 characters. It must start with letter or digit, but can also include hyphens and underscores for the remaining characters.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks create-nodegroup --cluster-name $cluster_name --nodegroup-name my-nodegroup \
    --subnets $subnet_id_1 $subnet_id_2 --instance-types t3.medium --node-role $node_role_arn
----
*** *With a launch template with a specified AMI ID*::

+
.... Determine the Amazon EKS recommended number of maximum [.noloc]`Pods `for your nodes. Follow the instructions in  <<determine-max-pods,Amazon EKS recommended maximum Pods for each Amazon EC2 instance type>>, adding `--cni-custom-networking-enabled` to step 3 in that topic. Note the output for use in the next step.
.... In your launch template, specify an Amazon EKS optimized AMI ID, or a custom AMI built off the Amazon EKS optimized AMI, then <<launch-templates,deploy the node group using a launch template>> and provide the following user data in the launch template. This user data passes arguments into the `bootstrap.sh` file. For more information about the bootstrap file, see https://github.com/awslabs/amazon-eks-ami/blob/main/templates/al2/runtime/bootstrap.sh[bootstrap.sh] on [.noloc]`GitHub`. You can replace [.replaceable]`20` with either the value from the previous step (recommended) or your own value.
+
[source,bash,subs="verbatim,attributes"]
----
/etc/eks/bootstrap.sh my-cluster --use-max-pods false --kubelet-extra-args '--max-pods=20'
----
+
If you've created a custom AMI that is not built off the Amazon EKS optimized AMI, then you need to custom create the configuration yourself.  
** *Self-managed*::

+
... Determine the Amazon EKS recommended number of maximum [.noloc]`Pods` for your nodes. Follow the instructions in  <<determine-max-pods,Amazon EKS recommended maximum Pods for each Amazon EC2 instance type>>, adding `--cni-custom-networking-enabled` to step 3 in that topic. Note the output for use in the next step.
... Deploy the node group using the instructions in <<launch-workers,Create self-managed Amazon Linux nodes>>. Specify the following text for the *BootstrapArguments* parameter. You can replace [.replaceable]`20` with either the value from the previous step (recommended) or your own value.
+
[source,bash,subs="verbatim,attributes"]
----
--use-max-pods false --kubelet-extra-args '--max-pods=20'
----
+
NOTE: If you want nodes in a production cluster to support a significantly higher number of [.noloc]`Pods`, run the script in  <<determine-max-pods,Amazon EKS recommended maximum Pods for each Amazon EC2 instance type>> again. Also, add the `--cni-prefix-delegation-enabled` option to the command. For example, [.replaceable]`110` is returned for an `m5.large` instance type. For instructions on how to enable this capability, see <<cni-increase-ip-addresses>>. You can use this capability with custom networking.
+
Node group creation takes several minutes. You can check the status of the creation of a managed node group with the following command.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks describe-nodegroup --cluster-name $cluster_name --nodegroup-name my-nodegroup --query nodegroup.status --output text
----
+
Don't continue to the next step until the output returned is `ACTIVE`.
. [[custom-networking-annotate-eniconfig]]For the tutorial, you can skip this step.
+
For a production cluster, if you didn't name your `ENIConfigs` the same as the Availability Zone that you're using them for, then you must annotate your nodes with the `ENIConfig` name that should be used with the node. This step isn't necessary if you only have one subnet in each Availability Zone and you named your `ENIConfigs` with the same names as your Availability Zones. This is because the [.noloc]`Amazon VPC CNI plugin for Kubernetes` automatically associates the correct `ENIConfig` with the node for you when you enabled it to do so in a <<custom-networking-automatically-apply-eniconfig,previous step>>. 
+
.. Get the list of nodes in your cluster.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl get nodes
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
NAME                                          STATUS   ROLES    AGE     VERSION
ip-192-168-0-126.us-west-2.compute.internal   Ready    <none>   8m49s   v1.22.9-eks-810597c
ip-192-168-0-92.us-west-2.compute.internal    Ready    <none>   8m34s   v1.22.9-eks-810597c
----
.. Determine which Availability Zone each node is in. Run the following command for each node that was returned in the previous step.
+
[source,bash,subs="verbatim,attributes"]
----
aws ec2 describe-instances --filters Name=network-interface.private-dns-name,Values=ip-192-168-0-126.us-west-2.compute.internal \
--query 'Reservations[].Instances[].{AvailabilityZone: Placement.AvailabilityZone, SubnetId: SubnetId}'
----
+
An example output is as follows.
+
[source,json,subs="verbatim,attributes"]
----
[
    {
        "AvailabilityZone": "us-west-2d",
        "SubnetId": "subnet-Example5"
    }
]
----
.. Annotate each node with the `ENIConfig` that you created for the subnet ID and Availability Zone. You can only annotate a node with one `ENIConfig`, though multiple nodes can be annotated with the same `ENIConfig`. Replace the [.replaceable]`example values` with your own.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl annotate node ip-192-168-0-126.us-west-2.compute.internal k8s.amazonaws.com/eniConfig=EniConfigName1
kubectl annotate node ip-192-168-0-92.us-west-2.compute.internal k8s.amazonaws.com/eniConfig=EniConfigName2
----
. [[custom-networking-terminate-existing-nodes]]If you had nodes in a production cluster with running [.noloc]`Pods` before you switched to using the custom networking feature, complete the following tasks:
+
.. Make sure that you have available nodes that are using the custom networking feature.
.. Cordon and drain the nodes to gracefully shut down the [.noloc]`Pods`. For more information, see https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/[Safely Drain a Node] in the [.noloc]`Kubernetes` documentation.
.. Terminate the nodes. If the nodes are in an existing managed node group, you can delete the node group. Copy the command that follows to your device. Make the following modifications to the command as needed and then run the modified command:
+
*** Replace [.replaceable]`my-cluster` with the name for your cluster.
*** Replace [.replaceable]`my-nodegroup` with the name for your node group.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks delete-nodegroup --cluster-name my-cluster --nodegroup-name my-nodegroup
----

+
Only new nodes that are registered with the `k8s.amazonaws.com/eniConfig` label use the custom networking feature.
. Confirm that [.noloc]`Pods` are assigned an IP address from a CIDR block that's associated to one of the subnets that you created in a previous step.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl get pods -A -o wide
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
NAMESPACE     NAME                       READY   STATUS    RESTARTS   AGE     IP              NODE                                          NOMINATED NODE   READINESS GATES
kube-system   aws-node-2rkn4             1/1     Running   0          7m19s   192.168.0.92    ip-192-168-0-92.us-west-2.compute.internal    <none>           <none>
kube-system   aws-node-k96wp             1/1     Running   0          7m15s   192.168.0.126   ip-192-168-0-126.us-west-2.compute.internal   <none>           <none>
kube-system   coredns-657694c6f4-smcgr   1/1     Running   0          56m     192.168.1.23    ip-192-168-0-92.us-west-2.compute.internal    <none>           <none>
kube-system   coredns-657694c6f4-stwv9   1/1     Running   0          56m     192.168.1.28    ip-192-168-0-92.us-west-2.compute.internal    <none>           <none>
kube-system   kube-proxy-jgshq           1/1     Running   0          7m19s   192.168.0.92    ip-192-168-0-92.us-west-2.compute.internal    <none>           <none>
kube-system   kube-proxy-wx9vk           1/1     Running   0          7m15s   192.168.0.126   ip-192-168-0-126.us-west-2.compute.internal   <none>           <none>
----
+
You can see that the coredns [.noloc]`Pods` are assigned IP addresses from the `192.168.1.0` CIDR block that you added to your VPC. Without custom networking, they would have been assigned addresses from the `192.168.0.0` CIDR block, because it was the only CIDR block originally associated with the VPC.
+
If a [.noloc]`Pod's` `spec` contains `hostNetwork=true`, it's assigned the primary IP address of the node. It isn't assigned an address from the subnets that you added. By default, this value is set to `false`. This value is set to `true` for the `kube-proxy` and [.noloc]`Amazon VPC CNI plugin for Kubernetes` (`aws-node`) [.noloc]`Pods` that run on your cluster. This is why the `kube-proxy` and the plugin's `aws-node` [.noloc]`Pods` aren't assigned `192.168.1.[.replaceable]``x``` addresses in the previous output. For more information about a [.noloc]`Pod's` `hostNetwork` setting, see https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.30/#podspec-v1-core[PodSpec v1 core] in the [.noloc]`Kubernetes` API reference.


[[custom-network-delete-resources,custom-network-delete-resources.title]]
====== Step 5: Delete tutorial resources

After you complete the tutorial, we recommend that you delete the resources that you created. You can then adjust the steps to enable custom networking for a production cluster.

. If the node group that you created was just for testing, then delete it.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks delete-nodegroup --cluster-name $cluster_name --nodegroup-name my-nodegroup
----
+
Even after the {aws} CLI output says that the cluster is deleted, the delete process might not actually be complete. The delete process takes a few minutes. Confirm that it's complete by running the following command.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks describe-nodegroup --cluster-name $cluster_name --nodegroup-name my-nodegroup --query nodegroup.status --output text
----
+
Don't continue until the returned output is similar to the following output.
+
[source,bash,subs="verbatim,attributes"]
----
An error occurred (ResourceNotFoundException) when calling the DescribeNodegroup operation: No node group found for name: my-nodegroup.
----
. If the node group that you created was just for testing, then delete the node IAM role.
+
.. Detach the policies from the role.
+
[source,bash,subs="verbatim,attributes"]
----
aws iam detach-role-policy --role-name myCustomNetworkingNodeRole --policy-arn {arn-aws}iam::aws:policy/AmazonEKSWorkerNodePolicy
aws iam detach-role-policy --role-name myCustomNetworkingNodeRole --policy-arn {arn-aws}iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
aws iam detach-role-policy --role-name myCustomNetworkingNodeRole --policy-arn {arn-aws}iam::aws:policy/AmazonEKS_CNI_Policy
----
.. Delete the role.
+
[source,bash,subs="verbatim,attributes"]
----
aws iam delete-role --role-name myCustomNetworkingNodeRole
----
. Delete the cluster.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks delete-cluster --name $cluster_name
----
+
Confirm the cluster is deleted with the following command.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks describe-cluster --name $cluster_name --query cluster.status --output text
----
+
When output similar to the following is returned, the cluster is successfully deleted.
+
[source,bash,subs="verbatim,attributes"]
----
An error occurred (ResourceNotFoundException) when calling the DescribeCluster operation: No cluster found for name: my-cluster.
----
. Delete the cluster IAM role.
+
.. Detach the policies from the role.
+
[source,bash,subs="verbatim,attributes"]
----
aws iam detach-role-policy --role-name myCustomNetworkingAmazonEKSClusterRole --policy-arn {arn-aws}iam::aws:policy/AmazonEKSClusterPolicy
----
.. Delete the role.
+
[source,bash,subs="verbatim,attributes"]
----
aws iam delete-role --role-name myCustomNetworkingAmazonEKSClusterRole
----
. Delete the subnets that you created in a previous step.
+
[source,bash,subs="verbatim,attributes"]
----
aws ec2 delete-subnet --subnet-id $new_subnet_id_1
aws ec2 delete-subnet --subnet-id $new_subnet_id_2
----
. Delete the VPC that you created.
+
[source,bash,subs="verbatim,attributes"]
----
aws cloudformation delete-stack --stack-name my-eks-custom-networking-vpc
----


[.topic]
[[cni-increase-ip-addresses,cni-increase-ip-addresses.title]]
==== Assign more IP addresses to Amazon EKS nodes with prefixes

[abstract]
--
Learn how to significantly increase the number of IP addresses that you can assign to [.noloc]`Pods` by assigning IP prefixes with Amazon EKS, improving scalability and reducing launch delays for large and spiky workloads.
--

*Applies to*: Linux and Windows nodes with Amazon EC2 instances

*Applies to*: Public and private subnets 

Each Amazon EC2 instance supports a maximum number of elastic network interfaces and a maximum number of IP addresses that can be assigned to each network interface. Each node requires one IP address for each network interface. All other available IP addresses can be assigned to `Pods`. Each `Pod` requires its own IP address. As a result, you might have nodes that have available compute and memory resources, but can't accommodate additional `Pods` because the node has run out of IP addresses to assign to `Pods`.

You can increase the number of IP addresses that nodes can assign to `Pods` by assigning IP prefixes, rather than assigning individual secondary IP addresses to your nodes. Each prefix includes several IP addresses. If you don't configure your cluster for IP prefix assignment, your cluster must make more Amazon EC2 application programming interface (API) calls to configure network interfaces and IP addresses necessary for [.noloc]`Pod` connectivity. As clusters grow to larger sizes, the frequency of these API calls can lead to longer [.noloc]`Pod` and instance launch times. This results in scaling delays to meet the demand of large and spiky workloads, and adds cost and management overhead because you need to provision additional clusters and VPCs to meet scaling requirements. For more information, see https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/thresholds.md[Kubernetes Scalability thresholds] on GitHub.

[[cni-increase-ip-addresses-compatability,cni-increase-ip-addresses-compatability.title]]
===== Compatibility with [.noloc]`Amazon VPC CNI plugin for Kubernetes` features

You can use IP prefixes with the following features:



* IPv4 Source Network Address Translation - For more information, see <<external-snat>>.
* IPv6 addresses to clusters, Pods, and services - For more information, see <<cni-ipv6>>.
* Restricting traffic using [.noloc]`Kubernetes` network policies - For more information, see <<cni-network-policy>>.

The following list provides information about the Amazon VPC CNI plugin settings that apply. For more information about each setting, see https://github.com/aws/amazon-vpc-cni-k8s/blob/master/README.md[amazon-vpc-cni-k8s] on [.noloc]`GitHub`.



* `WARM_IP_TARGET`
* `MINIMUM_IP_TARGET`
* `WARM_PREFIX_TARGET`


[[cni-increase-ip-addresses-considerations,cni-increase-ip-addresses-considerations.title]]
===== Considerations

Consider the following when you use this feature:



* Each Amazon EC2 instance type supports a maximum number of [.noloc]`Pods`. If your managed node group consists of multiple instance types, the smallest number of maximum [.noloc]`Pods` for an instance in the cluster is applied to all nodes in the cluster.
* By default, the maximum number of `Pods` that you can run on a node is 110, but you can change that number. If you change the number and have an existing managed node group, the next AMI or launch template update of your node group results in new nodes coming up with the changed value.
* When transitioning from assigning IP addresses to assigning IP prefixes, we recommend that you create new node groups to increase the number of available IP addresses, rather than doing a rolling replacement of existing nodes. Running [.noloc]`Pods` on a node that has both IP addresses and prefixes assigned can lead to inconsistency in the advertised IP address capacity, impacting the future workloads on the node. For the recommended way of performing the transition, see https://github.com/aws/aws-eks-best-practices/blob/master/content/networking/prefix-mode/index_windows.md#replace-all-nodes-during-migration-from-secondary-ip-mode-to-prefix-delegation-mode-or-vice-versa[Replace all nodes during migration from Secondary IP mode to Prefix Delegation mode or vice versa] in the Amazon EKS best practices guide.
* The security group scope is at the node-level - For more information, see  link:vpc/latest/userguide/VPC_SecurityGroups.html[Security group,type="documentation"].
* IP prefixes assigned to a network interface support high [.noloc]`Pod` density per node and have the best launch time.
* IP prefixes and IP addresses are associated with standard Amazon EC2 elastic network interfaces. Pods requiring specific security groups are assigned the primary IP address of a branch network interface. You can mix [.noloc]`Pods` getting IP addresses, or IP addresses from IP prefixes with [.noloc]`Pods` getting branch network interfaces on the same node.
* For clusters with Linux nodes only.
+
** After you configure the add-on to assign prefixes to network interfaces, you can't downgrade your [.noloc]`Amazon VPC CNI plugin for Kubernetes` add-on to a version lower than `1.9.0` (or `1.10.1`) without removing all nodes in all node groups in your cluster.
** If you're also using security groups for [.noloc]`Pods`, with `POD_SECURITY_GROUP_ENFORCING_MODE`=``standard`` and `AWS_VPC_K8S_CNI_EXTERNALSNAT`=``false``, when your [.noloc]`Pods` communicate with endpoints outside of your VPC, the node's security groups are used, rather than any security groups you've assigned to your [.noloc]`Pods`.  
+
If you're also using <<security-groups-for-pods,security groups for Pods>>, with `POD_SECURITY_GROUP_ENFORCING_MODE`=``strict``, when your `Pods` communicate with endpoints outside of your VPC, the `Pod's` security groups are used.


[.topic]
[[cni-increase-ip-addresses-procedure,cni-increase-ip-addresses-procedure.title]]
===== Increase the available IP addresses for your Amazon EKS node

You can increase the number of IP addresses that nodes can assign to [.noloc]`Pods` by assigning IP prefixes, rather than assigning individual secondary IP addresses to your nodes.

Complete the following before you start the procedure:



* Review the considerations.
* You need an existing cluster. To deploy one, see <<create-cluster>>. 
* The subnets that your Amazon EKS nodes are in must have sufficient contiguous `/28` (for `IPv4` clusters) or `/80` (for `IPv6` clusters) Classless Inter-Domain Routing (CIDR) blocks. You can only have Linux nodes in an `IPv6` cluster. Using IP prefixes can fail if IP addresses are scattered throughout the subnet CIDR. We recommend that following:
+
** Using a subnet CIDR reservation so that even if any IP addresses within the reserved range are still in use, upon their release, the IP addresses aren't reassigned. This ensures that prefixes are available for allocation without segmentation.
** Use new subnets that are specifically used for running the workloads that IP prefixes are assigned to. Both [.noloc]`Windows` and [.noloc]`Linux` workloads can run in the same subnet when assigning IP prefixes.
* To assign IP prefixes to your nodes, your nodes must be {aws} Nitro-based. Instances that aren't Nitro-based continue to allocate individual secondary IP addresses, but have a significantly lower number of IP addresses to assign to [.noloc]`Pods` than [.noloc]`Nitro-based` instances do.
* *For clusters with [.noloc]`Linux` nodes only*  If your cluster is configured for the `IPv4` family, you must have version `1.9.0` or later of the [.noloc]`Amazon VPC CNI plugin for Kubernetes` add-on installed. You can check your current version with the following command.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl describe daemonset aws-node --namespace kube-system | grep Image | cut -d "/" -f 2
----
+
If your cluster is configured for the `IPv6` family, you must have version `1.10.1` of the add-on installed. If your plugin version is earlier than the required versions, you must update it. For more information, see the updating sections of <<managing-vpc-cni,Assign IPs to Pods with the Amazon VPC CNI>>.
* *For clusters with [.noloc]`Windows` nodes only*::

+
** Your cluster and its platform version must be at, or later than the versions in the following table. To upgrade your cluster version, see <<update-cluster>>. If your cluster isn't at the minimum platform version, then you can't assign IP prefixes to your nodes until Amazon EKS has updated your platform version.
+
[cols="1,1", options="header"]
|===
|Kubernetes version
|Platform version


|`1.27`
|`eks.3`

|`1.26`
|`eks.4`

|`1.25`
|`eks.5`
|===
+
You can check your current [.noloc]`Kubernetes` and platform version by replacing [.replaceable]`my-cluster` in the following command with the name of your cluster and then running the modified command: ``aws eks describe-cluster --name [.replaceable]`my-cluster` --query 'cluster.{"Kubernetes Version": version, "Platform Version": platformVersion}'``.
** [.noloc]`Windows` support enabled for your cluster. For more information, see <<windows-support>>.
. Configure your cluster to assign IP address prefixes to nodes. Complete the procedure on the tab that matches your node's operating system.
+
[.noloc]`Linux`:::
... Enable the parameter to assign prefixes to network interfaces for the Amazon VPC CNI [.noloc]`DaemonSet`. When you deploy a `1.21` or later cluster, version `1.10.1` or later of the [.noloc]`Amazon VPC CNI plugin for Kubernetes` add-on is deployed with it. If you created the cluster with the `IPv6` family, this setting was set to `true` by default. If you created the cluster with the `IPv4` family, this setting was set to `false` by default.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl set env daemonset aws-node -n kube-system ENABLE_PREFIX_DELEGATION=true
----
+
IMPORTANT: Even if your subnet has available IP addresses, if the subnet does not have any contiguous `/28` blocks available, you will see the following error in the [.noloc]`Amazon VPC CNI plugin for Kubernetes` logs.

[source,bash,subs="verbatim,attributes"]
----
InsufficientCidrBlocks: The specified subnet does not have enough free cidr blocks to satisfy the request
----

This can happen due to fragmentation of existing secondary IP addresses spread out across a subnet. To resolve this error, either create a new subnet and launch [.noloc]`Pods` there, or use an Amazon EC2 subnet CIDR reservation to reserve space within a subnet for use with prefix assignment. For more information, see link:vpc/latest/userguide/subnet-cidr-reservation.html[Subnet CIDR reservations,type="documentation"] in the Amazon VPC User Guide.
... If you plan to deploy a managed node group without a launch template, or with a launch template that you haven't specified an AMI ID in, and you're using a version of the [.noloc]`Amazon VPC CNI plugin for Kubernetes` at or later than the versions listed in the prerequisites, then skip to the next step. Managed node groups automatically calculates the maximum number of [.noloc]`Pods` for you.
+
If you're deploying a self-managed node group or a managed node group with a launch template that you have specified an AMI ID in, then you must determine the Amazon EKS recommend number of maximum [.noloc]`Pods` for your nodes. Follow the instructions in  <<determine-max-pods,Amazon EKS recommended maximum Pods for each Amazon EC2 instance type>>, adding `--cni-prefix-delegation-enabled` to step 3. Note the output for use in a later step.
+
IMPORTANT: Managed node groups enforces a maximum number on the value of `maxPods`. For instances with less than 30 vCPUs the maximum number is 110 and for all other instances the maximum number is 250. This maximum number is applied whether prefix delegation is enabled or not. 
... If you're using a `1.21` or later cluster configured for `IPv6`, skip to the next step.
+
Specify the parameters in one of the following options. To determine which option is right for you and what value to provide for it, see https://github.com/aws/amazon-vpc-cni-k8s/blob/master/docs/prefix-and-ip-target.md[WARM_PREFIX_TARGET, WARM_IP_TARGET, and MINIMUM_IP_TARGET] on [.noloc]`GitHub`.
+
You can replace the [.replaceable]`example values` with a value greater than zero.
+
**** `WARM_PREFIX_TARGET`
+
[source,bash,subs="verbatim,attributes"]
----
kubectl set env ds aws-node -n kube-system WARM_PREFIX_TARGET=1
----
**** `WARM_IP_TARGET` or `MINIMUM_IP_TARGET`  If either value is set, it overrides any value set for `WARM_PREFIX_TARGET`.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl set env ds aws-node -n kube-system WARM_IP_TARGET=5
----
[source,bash,subs="verbatim,attributes"]
----
kubectl set env ds aws-node -n kube-system MINIMUM_IP_TARGET=2
----
... Create one of the following types of node groups with at least one Amazon EC2 Nitro Amazon Linux 2 instance type. For a list of Nitro instance types, see  link:AWSEC2/latest/UserGuide/instance-types.html#ec2-nitro-instances[Instances built on the Nitro System,type="documentation"] in the Amazon EC2 User Guide. This capability is not supported on [.noloc]`Windows`. For the options that include [.replaceable]`110`, replace it with either the value from step 3 (recommended), or your own value. 
+
**** *Self-managed*  Deploy the node group using the instructions in  <<launch-workers,Create self-managed Amazon Linux nodes>>. Specify the following text for the *BootstrapArguments* parameter.
+
[source,bash,subs="verbatim,attributes"]
----
--use-max-pods false --kubelet-extra-args '--max-pods=110'
----
+
If you're using `eksctl` to create the node group, you can use the following command.
+
[source,bash,subs="verbatim,attributes"]
----
eksctl create nodegroup --cluster my-cluster --managed=false --max-pods-per-node 110
----
**** *Managed*  Deploy your node group using one of the following options:
+
***** *Without a launch template or with a launch template without an AMI ID specified*  Complete the procedure in  <<create-managed-node-group,Create a managed node group for your cluster>>. Managed node groups automatically calculates the Amazon EKS recommended `max-pods` value for you.
***** *With a launch template with a specified AMI ID*  In your launch template, specify an Amazon EKS optimized AMI ID, or a custom AMI built off the Amazon EKS optimized AMI, then  <<launch-templates,deploy the node group using a launch template>> and provide the following user data in the launch template. This user data passes arguments into the `bootstrap.sh` file. For more information about the bootstrap file, see https://github.com/awslabs/amazon-eks-ami/blob/main/templates/al2/runtime/bootstrap.sh[bootstrap.sh] on [.noloc]`GitHub`.
+
[source,bash,subs="verbatim,attributes"]
----
/etc/eks/bootstrap.sh my-cluster \
  --use-max-pods false \
  --kubelet-extra-args '--max-pods=110'
----
+
If you're using `eksctl` to create the node group, you can use the following command.
+
[source,bash,subs="verbatim,attributes"]
----
eksctl create nodegroup --cluster my-cluster --max-pods-per-node 110
----
+
If you've created a custom AMI that is not built off the Amazon EKS optimized AMI, then you need to custom create the configuration yourself.
+
NOTE: If you also want to assign IP addresses to [.noloc]`Pods` from a different subnet than the instance's, then you need to enable the capability in this step. For more information, see <<cni-custom-network>>.


[.noloc]`Windows`:::
... Enable assignment of IP prefixes.
+
.... Open the `amazon-vpc-cni` `ConfigMap` for editing.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl edit configmap -n kube-system amazon-vpc-cni -o yaml
----
.... Add the following line to the `data` section.
+
[source,yaml,subs="verbatim,attributes"]
----
  enable-windows-prefix-delegation: "true"
----
.... Save the file and close the editor.
.... Confirm that the line was added to the `ConfigMap`.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl get configmap -n kube-system amazon-vpc-cni -o "jsonpath={.data.enable-windows-prefix-delegation}"
----
+
If the returned output isn't `true`, then there might have been an error. Try completing the step again.
+
IMPORTANT: Even if your subnet has available IP addresses, if the subnet does not have any contiguous `/28` blocks available, you will see the following error in the node events. 

[source,bash,subs="verbatim,attributes"]
----
"failed to allocate a private IP/Prefix address: InsufficientCidrBlocks: The specified subnet does not have enough free cidr blocks to satisfy the request"
----

This can happen due to fragmentation of existing secondary IP addresses spread out across a subnet. To resolve this error, either create a new subnet and launch [.noloc]`Pods` there, or use an Amazon EC2 subnet CIDR reservation to reserve space within a subnet for use with prefix assignment. For more information, see link:vpc/latest/userguide/subnet-cidr-reservation.html[Subnet CIDR reservations,type="documentation"] in the Amazon VPC User Guide.
... (Optional) Specify additional configuration for controlling the pre-scaling and dynamic scaling behavior for your cluster. For more information, see https://github.com/aws/amazon-vpc-resource-controller-k8s/blob/master/docs/windows/prefix_delegation_config_options.md[Configuration options with Prefix Delegation mode on Windows] on GitHub.
+
.... Open the `amazon-vpc-cni` `ConfigMap` for editing. 
+
[source,bash,subs="verbatim,attributes"]
----
kubectl edit configmap -n kube-system amazon-vpc-cni -o yaml
----
.... Replace the [.replaceable]`example values` with a value greater than zero and add the entries that you require to the `data` section of the `ConfigMap`. If you set a value for either `warm-ip-target` or `minimum-ip-target`, the value overrides any value set for `warm-prefix-target`.
+
[source,yaml,subs="verbatim,attributes"]
----
  warm-prefix-target: "1" 
  warm-ip-target: "5"
  minimum-ip-target: "2"
----
.... Save the file and close the editor.
... Create [.noloc]`Windows` node groups with at least one Amazon EC2 [.noloc]`Nitro` instance type. For a list of [.noloc]`Nitro` instance types, see   link:AWSEC2/latest/WindowsGuide/instance-types.html#ec2-nitro-instances[Instances built on the Nitro System,type="documentation"] in the Amazon EC2 User Guide. By default, the maximum number of [.noloc]`Pods` that you can deploy to a node is 110. If you want to increase or decrease that number, specify the following in the user data for the bootstrap configuration. Replace [.replaceable]`max-pods-quantity` with your max pods value.
+
[source,bash,subs="verbatim,attributes"]
----
-KubeletExtraArgs '--max-pods=max-pods-quantity'
----
+
If you're deploying managed node groups, this configuration needs to be added in the launch template. For more information, see <<launch-templates>>. For more information about the configuration parameters for [.noloc]`Windows` bootstrap script, see <<bootstrap-script-configuration-parameters>>.
. Once your nodes are deployed, view the nodes in your cluster.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl get nodes
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
NAME                                             STATUS     ROLES    AGE   VERSION
ip-192-168-22-103.region-code.compute.internal   Ready      <none>   19m   v1.XX.X-eks-6b7464
ip-192-168-97-94.region-code.compute.internal    Ready      <none>   19m   v1.XX.X-eks-6b7464
----
. Describe one of the nodes to determine the value of `max-pods` for the node and the number of available IP addresses. Replace [.replaceable]`192.168.30.193` with the `IPv4` address in the name of one of your nodes returned in the previous output. 
+
[source,bash,subs="verbatim,attributes"]
----
kubectl describe node ip-192-168-30-193.region-code.compute.internal | grep 'pods\|PrivateIPv4Address'
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
pods:                                  110
vpc.amazonaws.com/PrivateIPv4Address:  144
----
+
In the previous output, `110` is the maximum number of [.noloc]`Pods` that [.noloc]`Kubernetes` will deploy to the node, even though [.replaceable]`144` IP addresses are available.


[.topic]
[[security-groups-for-pods,security-groups-for-pods.title]]
==== Assign security groups to individual [.noloc]`pods`

[abstract]
--
Learn how to configure security groups for [.noloc]`Pods` on Amazon EKS, integrating Amazon EC2 security groups with [.noloc]`Kubernetes` [.noloc]`Pods` to define network traffic rules. Discover the considerations, setup process, and deploy a sample application with assigned security groups.
--

*Applies to*: [.noloc]`Linux` nodes with Amazon EC2 instances  

*Applies to*: Private subnets

Security groups for [.noloc]`Pods` integrate Amazon EC2 security groups with [.noloc]`Kubernetes` [.noloc]`Pods`. You can use Amazon EC2 security groups to define rules that allow inbound and outbound network traffic to and from [.noloc]`Pods` that you deploy to nodes running on many Amazon EC2 instance types and Fargate. For a detailed explanation of this capability, see the link:containers/introducing-security-groups-for-pods[Introducing security groups for Pods,type="blog"] blog post.

[[security-groups-for-pods-compatability,security-groups-for-pods-compatability.title]]
===== Compatibility with [.noloc]`Amazon VPC CNI plugin for Kubernetes` features

You can use security groups for [.noloc]`Pods` with the following features:



* IPv4 Source Network Address Translation - For more information, see <<external-snat>>.
* IPv6 addresses to clusters, Pods, and services - For more information, see <<cni-ipv6>>.
* Restricting traffic using [.noloc]`Kubernetes` network policies - For more information, see <<cni-network-policy>>.


[[sg-pods-considerations,sg-pods-considerations.title]]
===== Considerations

Before deploying security groups for [.noloc]`Pods`, consider the following limitations and conditions:



* Security groups for [.noloc]`Pods` can't be used with [.noloc]`Windows` nodes.
* Security groups for [.noloc]`Pods` can be used with clusters configured for the `IPv6` family that contain Amazon EC2 nodes by using version 1.16.0 or later of the Amazon VPC CNI plugin. You can use security groups for [.noloc]`Pods` with clusters configure `IPv6` family that contain only Fargate nodes by using version 1.7.7 or later of the Amazon VPC CNI plugin. For more information, see <<cni-ipv6>>
* Security groups for [.noloc]`Pods` are supported by most   link:AWSEC2/latest/UserGuide/instance-types.html#ec2-nitro-instances[Nitro-based,type="documentation"] Amazon EC2 instance families, though not by all generations of a family. For example, the `m5`, `c5`, `r5`, `m6g`, `c6g`, and `r6g` instance family and generations are supported. No instance types in the `t` family are supported. For a complete list of supported instance types, see the https://github.com/aws/amazon-vpc-resource-controller-k8s/blob/v1.5.0/pkg/aws/vpc/limits.go[limits.go] file on [.noloc]`GitHub`. Your nodes must be one of the listed instance types that have `IsTrunkingCompatible: true` in that file.
* If you're also using [.noloc]`Pod` security policies to restrict access to [.noloc]`Pod` mutation, then the `eks:vpc-resource-controller` [.noloc]`Kubernetes` user must be specified in the [.noloc]`Kubernetes` `ClusterRoleBinding` for the `role` that your `psp` is assigned to. If you're using the default Amazon EKS `psp`, `role`, and `ClusterRoleBinding`, this is the `eks:podsecuritypolicy:authenticated` `ClusterRoleBinding`. For example, you add the user to the `subjects:` section, as shown in the following example:
+
[source,yaml,subs="verbatim,attributes"]
----
[...]
subjects:
  - kind: Group
    apiGroup: rbac.authorization.k8s.io
    name: system:authenticated
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: eks:vpc-resource-controller
  - kind: ServiceAccount
    name: eks-vpc-resource-controller
----
* If you're using custom networking and security groups for [.noloc]`Pods` together, the security group specified by security groups for [.noloc]`Pods` is used instead of the security group specified in the `ENIConfig`.
* If you're using version `1.10.2` or earlier of the Amazon VPC CNI plugin and you include the `terminationGracePeriodSeconds` setting in your [.noloc]`Pod` spec, the value for the setting can't be zero.  
* If you're using version `1.10` or earlier of the Amazon VPC CNI plugin, or version `1.11` with `POD_SECURITY_GROUP_ENFORCING_MODE`=``strict``, which is the default setting, then [.noloc]`Kubernetes` services of type `NodePort` and `LoadBalancer` using instance targets with an `externalTrafficPolicy` set to `Local` aren't supported with [.noloc]`Pods` that you assign security groups to. For more information about using a load balancer with instance targets, see <<network-load-balancing>>.
* If you're using version `1.10` or earlier of the Amazon VPC CNI plugin or version `1.11` with `POD_SECURITY_GROUP_ENFORCING_MODE`=``strict``, which is the default setting, source NAT is disabled for outbound traffic from [.noloc]`Pods` with assigned security groups so that outbound security group rules are applied. To access the internet, [.noloc]`Pods` with assigned security groups must be launched on nodes that are deployed in a private subnet configured with a NAT gateway or instance. [.noloc]`Pods` with assigned security groups deployed to public subnets are not able to access the internet.
+
If you're using version `1.11` or later of the plugin with `POD_SECURITY_GROUP_ENFORCING_MODE`=``standard``, then [.noloc]`Pod` traffic destined for outside of the VPC is translated to the IP address of the instance's primary network interface. For this traffic, the rules in the security groups for the primary network interface are used, rather than the rules in the [.noloc]`Pod's` security groups.  
* To use [.noloc]`Calico` network policy with [.noloc]`Pods` that have associated security groups, you must use version `1.11.0` or later of the Amazon VPC CNI plugin and set `POD_SECURITY_GROUP_ENFORCING_MODE`=``standard``. Otherwise, traffic flow to and from [.noloc]`Pods` with associated security groups are not subjected to [.noloc]`Calico` network policy enforcement and are limited to Amazon EC2 security group enforcement only. To update your Amazon VPC CNI version, see <<managing-vpc-cni>>
* [.noloc]`Pods` running on Amazon EC2 nodes that use security groups in clusters that use https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/[NodeLocal DNSCache] are only supported with version `1.11.0` or later of the Amazon VPC CNI plugin and with `POD_SECURITY_GROUP_ENFORCING_MODE`=``standard``. To update your Amazon VPC CNI plugin version, see <<managing-vpc-cni>>
* Security groups for [.noloc]`Pods` might lead to higher [.noloc]`Pod` startup latency for [.noloc]`Pods` with high churn. This is due to rate limiting in the resource controller.
* The EC2 security group scope is at the [.noloc]`Pod`-level - For more information, see   link:vpc/latest/userguide/VPC_SecurityGroups.html[Security group,type="documentation"].
+
If you set `POD_SECURITY_GROUP_ENFORCING_MODE=standard` and `AWS_VPC_K8S_CNI_EXTERNALSNAT=false`, traffic destined for endpoints outside the VPC use the node's security groups, not the [.noloc]`Pod's` security groups.


[.topic]
[[security-groups-pods-deployment,security-groups-pods-deployment.title]]
===== Configure the [.noloc]`Amazon VPC CNI plugin for Kubernetes` for security groups for Amazon EKS [.noloc]`Pods`

If you use [.noloc]`Pods` with Amazon EC2 instances, you need to configure the [.noloc]`Amazon VPC CNI plugin for Kubernetes` for security groups

If you use Fargate [.noloc]`Pods` only, and don't have any Amazon EC2 nodes in your cluster, see <<sg-pods-example-deployment>>.

. Check your current [.noloc]`Amazon VPC CNI plugin for Kubernetes` version with the following command:
+
[source,bash,subs="verbatim,attributes"]
----
kubectl describe daemonset aws-node --namespace kube-system | grep amazon-k8s-cni: | cut -d : -f 3
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
v1.7.6
----
+
If your [.noloc]`Amazon VPC CNI plugin for Kubernetes` version is earlier than `1.7.7`, then update the plugin to version `1.7.7` or later. For more information, see <<managing-vpc-cni>>
. Add the link:iam/home#/policies/arn:aws:iam::aws:policy/AmazonEKSVPCResourceController[AmazonEKSVPCResourceController,type="console"] managed IAM policy to the <<create-service-role,cluster role>> that is associated with your Amazon EKS cluster. The policy allows the role to manage network interfaces, their private IP addresses, and their attachment and detachment to and from network instances.
+
.. Retrieve the name of your cluster IAM role and store it in a variable. Replace [.replaceable]`my-cluster` with the name of your cluster.
+
[source,bash,subs="verbatim,attributes"]
----
cluster_role=$(aws eks describe-cluster --name my-cluster --query cluster.roleArn --output text | cut -d / -f 2)
----
.. Attach the policy to the role.
+
[source,bash,subs="verbatim,attributes"]
----
aws iam attach-role-policy --policy-arn {arn-aws}iam::aws:policy/AmazonEKSVPCResourceController --role-name $cluster_role
----
. Enable the Amazon VPC CNI add-on to manage network interfaces for [.noloc]`Pods` by setting the `ENABLE_POD_ENI` variable to `true` in the `aws-node` [.noloc]`DaemonSet`. Once this setting is set to `true`, for each node in the cluster the add-on creates a `cninode` custom resource. The VPC resource controller creates and attaches one special network interface called a _trunk network interface_ with the description `aws-k8s-trunk-eni`.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl set env daemonset aws-node -n kube-system ENABLE_POD_ENI=true
----
+
NOTE: The trunk network interface is included in the maximum number of network interfaces supported by the instance type. For a list of the maximum number of network interfaces supported by each instance type, see  link:AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI[IP addresses per network interface per instance type,type="documentation"] in the _Amazon EC2 User Guide_. If your node already has the maximum number of standard network interfaces attached to it then the VPC resource controller will reserve a space. You will have to scale down your running [.noloc]`Pods` enough for the controller to detach and delete a standard network interface, create the trunk network interface, and attach it to the instance.
. You can see which of your nodes have a `CNINode` custom resource with the following command. If `No resources found` is returned, then wait several seconds and try again. The previous step requires restarting the [.noloc]`Amazon VPC CNI plugin for` Kubernetes Pods`, which takes several seconds.
+
[source,shell,subs="verbatim,attributes"]
----
$ kubectl get cninode -A
     NAME FEATURES
     ip-192-168-64-141.us-west-2.compute.internal [{"name":"SecurityGroupsForPods"}]
     ip-192-168-7-203.us-west-2.compute.internal [{"name":"SecurityGroupsForPods"}]
----
+
If you are using VPC CNI versions older than `1.15`, node labels were used instead of the `CNINode` custom resource. You can see which of your nodes have the node label `aws-k8s-trunk-eni` set to `true` with the following command. If `No resources found` is returned, then wait several seconds and try again. The previous step requires restarting the [.noloc]`Amazon VPC CNI plugin for Kubernetes Pods`, which takes several seconds.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl get nodes -o wide -l vpc.amazonaws.com/has-trunk-attached=true
-
----
+
Once the trunk network interface is created, [.noloc]`Pods` are assigned secondary IP addresses from the trunk or standard network interfaces. The trunk interface is automatically deleted if the node is deleted.
+
When you deploy a security group for a [.noloc]`Pod` in a later step, the VPC resource controller creates a special network interface called a  _branch network interface_ with a description of `aws-k8s-branch-eni` and associates the security groups to it. Branch network interfaces are created in addition to the standard and trunk network interfaces attached to the node.
+
If you are using liveness or readiness probes, then you also need to disable _TCP early demux_, so that the `kubelet` can connect to [.noloc]`Pods` on branch network interfaces using TCP. To disable _TCP early demux_, run the following command:
+
[source,bash,subs="verbatim,attributes"]
----
kubectl patch daemonset aws-node -n kube-system \
  -p '{"spec": {"template": {"spec": {"initContainers": [{"env":[{"name":"DISABLE_TCP_EARLY_DEMUX","value":"true"}],"name":"aws-vpc-cni-init"}]}}}}'
----
+
NOTE: If you're using `1.11.0` or later of the [.noloc]`Amazon VPC CNI plugin for Kubernetes` add-on and set `POD_SECURITY_GROUP_ENFORCING_MODE`=``standard``, as described in the next step, then you don't need to run the previous command.
. If your cluster uses `NodeLocal DNSCache`, or you want to use [.noloc]`Calico` network policy with your [.noloc]`Pods` that have their own security groups, or you have [.noloc]`Kubernetes` services of type `NodePort` and `LoadBalancer` using instance targets with an `externalTrafficPolicy` set to `Local` for [.noloc]`Pods` that you want to assign security groups to, then you must be using version `1.11.0` or later of the [.noloc]`Amazon VPC CNI plugin for Kubernetes` add-on, and you must enable the following setting:
+
[source,bash,subs="verbatim,attributes"]
----
kubectl set env daemonset aws-node -n kube-system POD_SECURITY_GROUP_ENFORCING_MODE=standard
----
+
IMPORTANT: 
** [.noloc]`Pod` security group rules aren't applied to traffic between [.noloc]`Pods` or between [.noloc]`Pods` and [.noloc]`services`, such as `kubelet` or `nodeLocalDNS`, that are on the same node. Pods using different security groups on the same node can't communicate because they are configured in different subnets, and routing is disabled between these subnets.
** Outbound traffic from [.noloc]`Pods` to addresses outside of the VPC is network address translated to the IP address of the instance's primary network interface (unless you've also set `AWS_VPC_K8S_CNI_EXTERNALSNAT=true`). For this traffic, the rules in the security groups for the primary network interface are used, rather than the rules in the [.noloc]`Pod's` security groups.
** For this setting to apply to existing [.noloc]`Pods`, you must restart the [.noloc]`Pods` or the nodes that the [.noloc]`Pods` are running on.

. To see how to use a security group policy for your [.noloc]`Pod`, see <<sg-pods-example-deployment>>.


[.topic]
[[sg-pods-example-deployment,sg-pods-example-deployment.title]]
===== Use a security group policy for an Amazon EKS [.noloc]`Pod`

To use security groups for [.noloc]`Pods`, you must have an existing security group. The following steps show you how to use the security group policy for a [.noloc]`Pod`. Unless otherwise noted, complete all steps from the same terminal because variables are used in the following steps that don't persist across terminals.

If you have a [.noloc]`Pod` with Amazon EC2 instances, you must configure the plugin before you use this procedure. For more information, see <<security-groups-pods-deployment>>.

. Create a [.noloc]`Kubernetes` namespace to deploy resources to. You can replace [.replaceable]`my-namespace` with the name of a namespace that you want to use.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl create namespace my-namespace
----
. [[deploy-securitygrouppolicy]]Deploy an Amazon EKS `SecurityGroupPolicy` to your cluster.
+
.. Copy the following contents to your device. You can replace [.replaceable]`podSelector` with `serviceAccountSelector` if you'd rather select [.noloc]`Pods` based on service account labels. You must specify one selector or the other. An empty `podSelector` (example: `podSelector: {}`) selects all [.noloc]`Pods` in the namespace. You can change [.replaceable]`my-role` to the name of your role. An empty `serviceAccountSelector` selects all service accounts in the namespace. You can replace [.replaceable]`my-security-group-policy` with a name for your `SecurityGroupPolicy` and [.replaceable]`my-namespace` with the namespace that you want to create the `SecurityGroupPolicy` in. 
+
You must replace [.replaceable]`my_pod_security_group_id` with the ID of an existing security group. If you don't have an existing security group, then you must create one. For more information, see link:AWSEC2/latest/UserGuide/ec2-security-groups.html[Amazon EC2 security groups for Linux instances,type="documentation"] in the link:AWSEC2/latest/UserGuide/[Amazon EC2 User Guide,type="documentation"]. You can specify 1-5 security group IDs. If you specify more than one ID, then the combination of all the rules in all the security groups are effective for the selected [.noloc]`Pods`.
+
[source,yaml,subs="verbatim,attributes"]
----
cat >my-security-group-policy.yaml <<EOF
apiVersion: vpcresources.k8s.aws/v1beta1
kind: SecurityGroupPolicy
metadata:
  name: my-security-group-policy
  namespace: my-namespace
spec:
  podSelector: 
    matchLabels:
      role: my-role
  securityGroups:
    groupIds:
      - my_pod_security_group_id
EOF
----
+
[IMPORTANT]
====
The security group or groups that you specify for your [.noloc]`Pods` must meet the following criteria:

* They must exist. If they don't exist, then, when you deploy a [.noloc]`Pod` that matches the selector, your [.noloc]`Pod` remains stuck in the creation process. If you describe the [.noloc]`Pod`, you'll see an error message similar to the following one: `An error occurred (InvalidSecurityGroupID.NotFound) when calling the CreateNetworkInterface operation: The securityGroup ID '[.replaceable]``sg-05b1d815d1EXAMPLE``' does not exist`.
* They must allow inbound communication from the security group applied to your nodes (for `kubelet`) over any ports that you've configured probes for.
* They must allow outbound communication over `TCP` and `UDP` ports 53 to a security group assigned to the [.noloc]`Pods` (or nodes that the [.noloc]`Pods` run on) running [.noloc]`CoreDNS`. The security group for your [.noloc]`CoreDNS` [.noloc]`Pods` must allow inbound `TCP` and `UDP` port 53 traffic from the security group that you specify.
* They must have necessary inbound and outbound rules to communicate with other [.noloc]`Pods` that they need to communicate with.
* They must have rules that allow the [.noloc]`Pods` to communicate with the [.noloc]`Kubernetes` control plane if you're using the security group with Fargate. The easiest way to do this is to specify the cluster security group as one of the security groups.

Security group policies only apply to newly scheduled [.noloc]`Pods`. They do not affect running [.noloc]`Pods`.
====

.. Deploy the policy.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl apply -f my-security-group-policy.yaml
----
. Deploy a sample application with a label that matches the [.replaceable]`my-role` value for [.replaceable]`podSelector` that you specified in a previous step.
+
.. Copy the following contents to your device. Replace the [.replaceable]`example values` with your own and then run the modified command. If you replace [.replaceable]`my-role`, make sure that it's the same as the value you specified for the selector in a previous step.
+
[source,yaml,subs="verbatim,attributes"]
----
cat >sample-application.yaml <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
  namespace: my-namespace
  labels:
    app: my-app
spec:
  replicas: 4
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
        role: my-role
    spec:
      terminationGracePeriodSeconds: 120
      containers:
      - name: nginx
        image: public.ecr.aws/nginx/nginx:1.23
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: my-app
  namespace: my-namespace
  labels:
    app: my-app
spec:
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
EOF
----
.. Deploy the application with the following command. When you deploy the application, the [.noloc]`Amazon VPC CNI plugin for Kubernetes` matches the `role` label and the security groups that you specified in the previous step are applied to the [.noloc]`Pod`.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl apply -f sample-application.yaml
----
. View the [.noloc]`Pods` deployed with the sample application. For the remainder of this topic, this terminal is referred to as `TerminalA`.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl get pods -n my-namespace -o wide
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
NAME                             READY   STATUS    RESTARTS   AGE     IP               NODE                                            NOMINATED NODE   READINESS GATES
my-deployment-5df6f7687b-4fbjm   1/1     Running   0          7m51s   192.168.53.48    ip-192-168-33-28.region-code.compute.internal   <none>           <none>
my-deployment-5df6f7687b-j9fl4   1/1     Running   0          7m51s   192.168.70.145   ip-192-168-92-33.region-code.compute.internal   <none>           <none>
my-deployment-5df6f7687b-rjxcz   1/1     Running   0          7m51s   192.168.73.207   ip-192-168-92-33.region-code.compute.internal   <none>           <none>
my-deployment-5df6f7687b-zmb42   1/1     Running   0          7m51s   192.168.63.27    ip-192-168-33-28.region-code.compute.internal   <none>           <none>
----
+
[NOTE]
====
Try these tips if any [.noloc]`Pods` are stuck.

* If any [.noloc]`Pods` are stuck in the `Waiting` state, then run `kubectl describe pod [.replaceable]``my-deployment-xxxxxxxxxx-xxxxx`` -n [.replaceable]`my-namespace```. If you see `Insufficient permissions: Unable to create Elastic Network Interface.`, confirm that you added the IAM policy to the IAM cluster role in a previous step.
* If any [.noloc]`Pods` are stuck in the `Pending` state, confirm that your node instance type is listed in https://github.com/aws/amazon-vpc-resource-controller-k8s/blob/master/pkg/aws/vpc/limits.go[limits.go] and that the product of the maximum number of branch network interfaces supported by the instance type multiplied times the number of nodes in your node group hasn't already been met. For example, an `m5.large` instance supports nine branch network interfaces. If your node group has five nodes, then a maximum of 45 branch network interfaces can be created for the node group. The 46th [.noloc]`Pod` that you attempt to deploy will sit in `Pending` state until another [.noloc]`Pod` that has associated security groups is deleted.

====
+
If you run `kubectl describe pod [.replaceable]``my-deployment-xxxxxxxxxx-xxxxx`` -n [.replaceable]``my-namespace``` and see a message similar to the following message, then it can be safely ignored. This message might appear when the [.noloc]`Amazon VPC CNI plugin for Kubernetes` tries to set up host networking and fails while the network interface is being created. The plugin logs this event until the network interface is created.
+
[source,bash,subs="verbatim,attributes"]
----
Failed to create Pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container "e24268322e55c8185721f52df6493684f6c2c3bf4fd59c9c121fd4cdc894579f" network for Pod "my-deployment-5df6f7687b-4fbjm": networkPlugin
cni failed to set up Pod "my-deployment-5df6f7687b-4fbjm-c89wx_my-namespace" network: add cmd: failed to assign an IP address to container
----
+
You can't exceed the maximum number of [.noloc]`Pods` that can be run on the instance type. For a list of the maximum number of [.noloc]`Pods` that you can run on each instance type, see https://github.com/awslabs/amazon-eks-ami/blob/main/templates/shared/runtime/eni-max-pods.txt[eni-max-pods.txt] on [.noloc]`GitHub`. When you delete a [.noloc]`Pod` that has associated security groups, or delete the node that the [.noloc]`Pod` is running on, the VPC resource controller deletes the branch network interface. If you delete a cluster with [.noloc]`Pods` using [.noloc]`Pods` for security groups, then the controller doesn't delete the branch network interfaces, so you'll need to delete them yourself. For information about how to delete network interfaces, see   link:AWSEC2/latest/UserGuide/using-eni.html#delete_eni[Delete a network interface,type="documentation"] in the Amazon EC2 User Guide.
. In a separate terminal, shell into one of the [.noloc]`Pods`. For the remainder of this topic, this terminal is referred to as `TerminalB`. Replace ``[.replaceable]`5df6f7687b`-[.replaceable]`4fbjm` `` with the ID of one of the [.noloc]`Pods` returned in your output from the previous step.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl exec -it -n my-namespace my-deployment-5df6f7687b-4fbjm -- /bin/bash
----
. From the shell in `TerminalB`, confirm that the sample application works.
+
[source,bash,subs="verbatim,attributes"]
----
curl my-app
----
+
An example output is as follows.
+
[source,html,subs="verbatim"]
----
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
[...]
----
+
You received the output because all [.noloc]`Pods` running the application are associated with the security group that you created. That group contains a rule that allows all traffic between all [.noloc]`Pods` that the security group is associated to. DNS traffic is allowed outbound from that security group to the cluster security group, which is associated with your nodes. The nodes are running the [.noloc]`CoreDNS` [.noloc]`Pods`, which your [.noloc]`Pods` did a name lookup to.
. From `TerminalA`, remove the security group rules that allow DNS communication to the cluster security group from your security group. If you didn't add the DNS rules to the cluster security group in a previous step, then replace [.replaceable]`$my_cluster_security_group_id` with the ID of the security group that you created the rules in.
+
[source,bash,subs="verbatim,attributes"]
----
aws ec2 revoke-security-group-ingress --group-id $my_cluster_security_group_id --security-group-rule-ids $my_tcp_rule_id
aws ec2 revoke-security-group-ingress --group-id $my_cluster_security_group_id --security-group-rule-ids $my_udp_rule_id
----
. From `TerminalB`, attempt to access the application again.
+
[source,bash,subs="verbatim,attributes"]
----
curl my-app
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
curl: (6) Could not resolve host: my-app
----
+
The attempt fails because the [.noloc]`Pod` is no longer able to access the [.noloc]`CoreDNS` [.noloc]`Pods`, which have the cluster security group associated to them. The cluster security group no longer has the security group rules that allow DNS communication from the security group associated to your [.noloc]`Pod`.
+
If you attempt to access the application using the IP addresses returned for one of the [.noloc]`Pods` in a previous step, you still receive a response because all ports are allowed between [.noloc]`Pods` that have the security group associated to them and a name lookup isn't required.
. Once you've finished experimenting, you can remove the sample security group policy, application, and security group that you created. Run the following commands from `TerminalA`.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl delete namespace my-namespace
aws ec2 revoke-security-group-ingress --group-id $my_pod_security_group_id --security-group-rule-ids $my_inbound_self_rule_id
wait
sleep 45s 
aws ec2 delete-security-group --group-id $my_pod_security_group_id
----


[.topic]
[[pod-multiple-network-interfaces,pod-multiple-network-interfaces.title]]
==== Attach multiple network interfaces to [.noloc]`Pods` with [.noloc]`Multus`

[abstract]
--
Learn how to use Multus CNI to attach multiple network interfaces to a [.noloc]`Pod` in Amazon EKS for advanced networking scenarios, while leveraging the [.noloc]`Amazon VPC CNI` plugin for primary networking.
--

Multus CNI is a container network interface (CNI) plugin for Amazon EKS that enables attaching multiple network interfaces to a [.noloc]`Pod`. For more information, see the https://github.com/k8snetworkplumbingwg/multus-cni[Multus-CNI] documentation on [.noloc]`GitHub`.  

In Amazon EKS, each [.noloc]`Pod` has one network interface assigned by the Amazon VPC CNI plugin. With Multus, you can create a multi-homed [.noloc]`Pod` that has multiple interfaces. This is accomplished by Multus acting as a "meta-plugin"; a CNI plugin that can call multiple other CNI plugins. {aws} support for Multus comes configured with the Amazon VPC CNI plugin as the default delegate plugin.

* Amazon EKS won't be building and publishing single root I/O virtualization (SR-IOV) and Data Plane Development Kit (DPDK) CNI plugins. However, you can achieve packet acceleration by connecting directly to Amazon EC2 Elastic Network Adapters (ENA) through Multus managed host-device and `ipvlan` plugins.
* Amazon EKS is supporting Multus, which provides a generic process that enables simple chaining of additional CNI plugins. Multus and the process of chaining is supported, but {aws} won't provide support for all compatible CNI plugins that can be chained, or issues that may arise in those CNI plugins that are unrelated to the chaining configuration.
* Amazon EKS is providing support and life cycle management for the Multus plugin, but isn't responsible for any IP address or additional management associated with the additional network interfaces. The IP address and management of the default network interface utilizing the Amazon VPC CNI plugin remains unchanged.
* Only the Amazon VPC CNI plugin is officially supported as the default delegate plugin. You need to modify the published Multus installation manifest to reconfigure the default delegate plugin to an alternate CNI if you choose not to use the Amazon VPC CNI plugin for primary networking.
* Multus is only supported when using the Amazon VPC CNI as the primary CNI. We do not support the Amazon VPC CNI when used for higher order interfaces, secondary or otherwise.
* To prevent the Amazon VPC CNI plugin from trying to manage additional network interfaces assigned to [.noloc]`Pods`, add the following tag to the network interface:
+
*key*::
: `node.k8s.amazonaws.com/no_manage`
+
*value*::
: `true`
* Multus is compatible with network policies, but the policy has to be enriched to include ports and IP addresses that may be part of additional network interfaces attached to [.noloc]`Pods`.

For an implementation walk through, see the https://github.com/aws-samples/eks-install-guide-for-multus/blob/main/README.md[Multus Setup Guide] on [.noloc]`GitHub`.

[.topic]
[[alternate-cni-plugins,alternate-cni-plugins.title]]
== Alternate CNI plugins for Amazon EKS clusters

[abstract]
--
Learn how to use alternate network and security plugins on Amazon EKS to customize networking for your [.noloc]`Kubernetes` clusters on Amazon EC2 nodes.
--

The https://github.com/aws/amazon-vpc-cni-plugins[Amazon VPC CNI plugin for Kubernetes] is the only CNI plugin supported by Amazon EKS. Amazon EKS runs upstream [.noloc]`Kubernetes`, so you can install alternate compatible CNI plugins to Amazon EC2 nodes in your cluster. If you have Fargate nodes in your cluster, the [.noloc]`Amazon VPC CNI plugin for Kubernetes` is already on your Fargate nodes. It's the only CNI plugin you can use with Fargate nodes. An attempt to install an alternate CNI plugin on Fargate nodes fails.

If you plan to use an alternate CNI plugin on Amazon EC2 nodes, we recommend that you obtain commercial support for the plugin or have the in-house expertise to troubleshoot and contribute fixes to the CNI plugin project. 

Amazon EKS maintains relationships with a network of partners that offer support for alternate compatible CNI plugins. For details about the versions, qualifications, and testing performed, see the following partner documentation.

[cols="1,1,1", options="header"]
|===
|Partner
|Product
|Documentation


|Tigera
|https://www.tigera.io/partners/aws/[Calico]
|https://docs.projectcalico.org/getting-started/kubernetes/managed-public-cloud/eks[Installation instructions]

|Isovalent
|https://cilium.io[Cilium]
|https://docs.cilium.io/en/stable/gettingstarted/k8s-install-default/[Installation instructions]

|Juniper
|https://www.juniper.net/us/en/products/sdn-and-orchestration/contrail/cloud-native-contrail-networking.html[Cloud-Native Contrail Networking (CN2)]
|https://www.juniper.net/documentation/us/en/software/cn-cloud-native23.2/cn-cloud-native-eks-install-and-lcm/index.html[Installation instructions]

|VMware
|https://antrea.io/[Antrea]
|https://antrea.io/docs/main/docs/eks-installation[Installation instructions]
|===

Amazon EKS aims to give you a wide selection of options to cover all use cases.

[[alternate-network-policy-plugins,alternate-network-policy-plugins.title]]
=== Alternate compatible network policy plugins

https://www.tigera.io/project-calico[Calico] is a widely adopted solution for container networking and security. Using [.noloc]`Calico` on EKS provides a fully compliant network policy enforcement for your EKS clusters. Additionally, you can opt to use [.noloc]`Calico's` networking, which conserve IP addresses from your underlying VPC. https://www.tigera.io/tigera-products/calico-cloud/[Calico Cloud] enhances the features of [.noloc]`Calico Open Source`, providing advanced security and observability capabilities.

Traffic flow to and from [.noloc]`Pods` with associated security groups are not subjected to [.noloc]`Calico` network policy enforcement and are limited to Amazon VPC security group enforcement only.  

If you use [.noloc]`Calico` network policy enforcement, we recommend that you set the environment variable `ANNOTATE_POD_IP` to `true` to avoid a known issue with [.noloc]`Kubernetes`. To use this feature, you must add `patch` permission for pods to the `aws-node` [.noloc]`ClusterRole`. Note that adding patch permissions to the `aws-node` [.noloc]`DaemonSet` increases the security scope for the plugin. For more information, see https://github.com/aws/amazon-vpc-cni-k8s/?tab=readme-ov-file#annotate_pod_ip-v193[ANNOTATE_POD_IP] in the VPC CNI repo on GitHub.

[.topic]
[[aws-load-balancer-controller,aws-load-balancer-controller.title]]
== Route internet traffic with {aws} Load Balancer Controller

[abstract]
--
Learn how to configure and use the [.noloc]`{aws} Load Balancer Controller` to expose [.noloc]`Kubernetes` cluster apps to the internet with {aws} Elastic Load Balancing for [.noloc]`Kubernetes` [.noloc]`services` and [.noloc]`ingresses`.
--

The [.noloc]`{aws} Load Balancer Controller` manages {aws} Elastic Load Balancers for a [.noloc]`Kubernetes` cluster. You can use the controller to expose your cluster apps to the internet. The controller provisions {aws} load balancers that point to cluster Service or Ingress resources. In other words, the controller creates a single IP address or DNS name that points to multiple pods in your cluster.  



image::images/lbc-overview.png["Architecture diagram. Illustration of traffic coming from internet users, to Amazon Load Balancer. Amazon Load Balancer distributes traffic to pods in the cluster.",scaledwidth=50%]

The controller watches for [.noloc]`Kubernetes` [.noloc]`Ingress` or [.noloc]`Service` resources. In response, it creates the appropriate {aws} Elastic Load Balancing resources. You can configure the specific behavior of the load balancers by applying annotations to the [.noloc]`Kubernetes` resources. For example, you can attach {aws} security groups to load balancers using annotations.  

The controller provisions the following resources: 



*[.noloc]`Kubernetes` `Ingress`*::
The LBC creates an link:elasticloadbalancing/latest/application/introduction.html[{aws} Application Load Balancer (ALB),type="documentation"] when you create a [.noloc]`Kubernetes` `Ingress`. https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.7/guide/ingress/annotations/[Review the annotations you can apply to an Ingress resource.]


*[.noloc]`Kubernetes` service of the `LoadBalancer` type*::
The LBC creates an link:elasticloadbalancing/latest/network/introduction.html[{aws} Network Load Balancer (NLB),type="documentation"]when you create a [.noloc]`Kubernetes` service of type `LoadBalancer`. https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.7/guide/service/annotations/[Review the annotations you can apply to a Service resource.]
+
In the past, the [.noloc]`Kubernetes` network load balancer was used for  _instance_ targets, but the LBC was used for _IP_ targets. With the [.noloc]`{aws} Load Balancer Controller` version `2.3.0` or later, you can create NLBs using either target type. For more information about NLB target types, see  link:elasticloadbalancing/latest/network/load-balancer-target-groups.html#target-type[Target type,type="documentation"] in the User Guide for Network Load Balancers.

The controller is an https://github.com/kubernetes-sigs/aws-load-balancer-controller[open-source project] managed on [.noloc]`GitHub`.

Before deploying the controller, we recommend that you review the prerequisites and considerations in <<alb-ingress,Route application and HTTP traffic with Application Load Balancers>> and <<network-load-balancing>>. In those topics, you will deploy a sample app that includes an {aws} load balancer. 

[[lbc-overview,lbc-overview.title]]
=== Install the controller

You can use one of the following procedures to install the [.noloc]`{aws} Load Balancer Controller`:



* If you are new to Amazon EKS we recommend that you use Helm for the installation because it simplifies the [.noloc]`{aws} Load Balancer Controller` installation. For more information, see <<lbc-helm>>. 
* For advanced configurations, such as clusters with restricted network access to public container registries,  use [.noloc]`Kubernetes` Manifests. For more information, see <<lbc-manifest>>.


[[lbc-deprecated,lbc-deprecated.title]]
=== Migrate from deprecated controller versions

* If you have deprecated versions of the [.noloc]`{aws} Load Balancer Controller` installed, see <<lbc-remove>>.
* Deprecated versions cannot be upgraded. They must be removed and a current version of the [.noloc]`{aws} Load Balancer Controller` installed.  
+
[[lbc-deprecated-list]]
* Deprecated versions include:
+
** {aws} ALB Ingress Controller for [.noloc]`Kubernetes` ("Ingress Controller"), a predecessor to the [.noloc]`{aws} Load Balancer Controller`.
** Any `0.1.[.replaceable]``x``` version of the [.noloc]`{aws} Load Balancer Controller`


[[lbc-legacy,lbc-legacy.title]]
=== Legacy cloud provider

[.noloc]`Kubernetes` includes a legacy cloud provider for {aws}. The legacy cloud provider is capable of provisioning {aws} load balancers, similar to the [.noloc]`{aws} Load Balancer Controller`. The legacy cloud provider creates Classic Load Balancers. If you do not install the [.noloc]`{aws} Load Balancer Controller`, [.noloc]`Kubernetes` will default to using the legacy cloud provider. You should install the [.noloc]`{aws} Load Balancer Controller` and avoid using the legacy cloud provider.  

[IMPORTANT]
====

In versions 2.5 and newer, the [.noloc]`{aws} Load Balancer Controller` becomes the default controller for [.noloc]`Kubernetes` _service_ resources with the `type: LoadBalancer` and makes an {aws} Network Load Balancer (NLB) for each service. It does this by making a mutating webhook for services, which sets the `spec.loadBalancerClass` field to `service.k8s.aws/nlb` for new services of `type: LoadBalancer`. You can turn off this feature and revert to using the https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.7/guide/service/annotations/#legacy-cloud-provider[legacy Cloud Provider] as the default controller, by setting the helm chart value `enableServiceMutatorWebhook` to `false`. The cluster won't provision new Classic Load Balancers for your services unless you turn off this feature. Existing Classic Load Balancers will continue to work.

====

[.topic]
[[lbc-helm,lbc-helm.title]]
=== Install [.noloc]`{aws} Load Balancer Controller` with [.noloc]`Helm`

[abstract]
--
Learn how to install the [.noloc]`{aws} Load Balancer Controller` on Amazon EKS using Helm to manage K8s load balancing with {aws} Cloud. Discover the prerequisites and steps for creating an IAM role, installing with Helm, and verifying the controller deployment.
--

This topic describes how to install the [.noloc]`{aws} Load Balancer Controller` using Helm, a package manager for [.noloc]`Kubernetes`, and `eksctl`. The controller is installed with default options. For more information about the controller, including details on configuring it with annotations, see the https://kubernetes-sigs.github.io/aws-load-balancer-controller/[{aws} Load Balancer Controller Documentation] on [.noloc]`GitHub`.  

In the following steps, replace the [.replaceable]`example values` with your own values.

[[lbc-prereqs,lbc-prereqs.title]]
==== Prerequisites

Before starting this tutorial, you must install and configure the following tools and resources that you need to create and manage an Amazon EKS cluster.  



* An existing Amazon EKS cluster. To deploy one, see <<getting-started>>.
* An existing {aws} Identity and Access Management (IAM) [.noloc]`OpenID Connect` ([.noloc]`OIDC`) provider for your cluster. To determine whether you already have one, or to create one, see <<enable-iam-roles-for-service-accounts>>.
* Make sure that your [.noloc]`Amazon VPC CNI plugin for Kubernetes`, `kube-proxy`, and [.noloc]`CoreDNS` add-ons are at the minimum versions listed in  <<boundserviceaccounttoken-validated-add-on-versions,Service account tokens>>.
* Familiarity with {aws} Elastic Load Balancing. For more information, see the link:elasticloadbalancing/latest/userguide/[Elastic Load Balancing User Guide,type="documentation"].
* Familiarity with Kubernetes https://kubernetes.io/docs/concepts/services-networking/service/[service] and https://kubernetes.io/docs/concepts/services-networking/ingress/[ingress] resources.


* https://helm.sh/docs/helm/helm_install/[Helm] installed locally. 


[[lbc-helm-iam,lbc-helm-iam.title]]
==== Step 1: Create IAM Role using `eksctl`

[NOTE]
====

You only need to create an IAM Role for the [.noloc]`{aws} Load Balancer Controller` one per {aws} account. Check if `AmazonEKSLoadBalancerControllerRole` exists in the link:iam[IAM Console,type="console"]. If this role exists, skip to <<lbc-helm-install,Step 2: Install {aws} Load Balancer Controller>>.

====
. Download an IAM policy for the [.noloc]`{aws} Load Balancer Controller` that allows it to make calls to {aws} APIs on your behalf.  
+
{aws}:::
** 
[source,shell,subs="verbatim,attributes"]
----
$ curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.7.2/docs/install/iam_policy.json
----


{aws} GovCloud (US):::
** 
[source,shell,subs="verbatim,attributes"]
----
$ curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.7.2/docs/install/iam_policy_us-gov.json
----
[source,shell,subs="verbatim,attributes"]
----
$ mv iam_policy_us-gov.json iam_policy.json
----
. Create an IAM policy using the policy downloaded in the previous step. 
+
[source,shell,subs="verbatim,attributes"]
----
$ aws iam create-policy \
    --policy-name AWSLoadBalancerControllerIAMPolicy \
    --policy-document file://iam_policy.json
----
+
NOTE: If you view the policy in the {aws-management-console}, the console shows warnings for the *ELB* service, but not for the  *ELB v2* service. This happens because some of the actions in the policy exist for  *ELB v2*, but not for  *ELB*. You can ignore the warnings for  *ELB*.
. Replace [.replaceable]`my-cluster` with the name of your cluster, [.replaceable]`111122223333` with your account ID, and then run the command. If your cluster is in the {aws} GovCloud (US-East) or {aws} GovCloud (US-West) {aws} Regions, then replace `{arn-aws}` with `arn:aws-us-gov:`.
+
[source,shell,subs="verbatim,attributes"]
----
$ eksctl create iamserviceaccount \
  --cluster=my-cluster \
  --namespace=kube-system \
  --name=aws-load-balancer-controller \
  --role-name AmazonEKSLoadBalancerControllerRole \
  --attach-policy-arn={arn-aws}iam::111122223333:policy/AWSLoadBalancerControllerIAMPolicy \
  --approve
----


[[lbc-helm-install,lbc-helm-install.title]]
==== Step 2: Install [.noloc]`{aws} Load Balancer Controller`
. Add the `eks-charts` Helm chart repository. {aws} maintains https://github.com/aws/eks-charts[this repository] on GitHub.
+
[source,shell,subs="verbatim,attributes"]
----
$ helm repo add eks https://aws.github.io/eks-charts
----
. Update your local repo to make sure that you have the most recent charts.
+
[source,shell,subs="verbatim,attributes"]
----
$ helm repo update eks
----
. Install the [.noloc]`{aws} Load Balancer Controller`.  
+
Replace [.replaceable]`my-cluster` with the name of your cluster. In the following command, `aws-load-balancer-controller` is the [.noloc]`Kubernetes` service account that you created in a previous step.
+
For more information about configuring the helm chart, see https://github.com/aws/eks-charts/blob/master/stable/aws-load-balancer-controller/values.yaml[values.yaml] on GitHub.
+
[source,shell,subs="verbatim,attributes"]
----
$ helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=my-cluster \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller
----
+
.. If you're deploying the controller to Amazon EC2 nodes that have https://aws.github.io/aws-eks-best-practices/security/docs/iam/#restrict-access-to-the-instance-profile-assigned-to-the-worker-node[restricted access to the Amazon EC2 instance metadata service (IMDS)], or if you're deploying to Fargate, then add the following flags to the `helm` command that follows:
+
*** ``--set region=[.replaceable]`region-code` ``
*** ``--set vpcId=[.replaceable]`vpc-xxxxxxxx` ``
.. To view the available versions of the Helm Chart and Load Balancer Controller, use the following command:
+
[source,bash,subs="verbatim,attributes"]
----
helm search repo eks/aws-load-balancer-controller --versions
----
+
IMPORTANT: The deployed chart doesn't receive security updates automatically. You need to manually upgrade to a newer chart when it becomes available. When upgrading, change [.replaceable]`install` to `upgrade` in the previous command.

The `helm install` command automatically installs the custom resource definitions ([.noloc]`CRDs`) for the controller. The `helm upgrade` command does not. If you use `helm upgrade,` you must manually install the [.noloc]`CRDs`. Run the following command to install the [.noloc]`CRDs`:

[source,shell,subs="verbatim,attributes"]
----
wget https://raw.githubusercontent.com/aws/eks-charts/master/stable/aws-load-balancer-controller/crds/crds.yaml 
kubectl apply -f crds.yaml
----


[[lbc-helm-verify,lbc-helm-verify.title]]
==== Step 3: Verify that the controller is installed
. Verify that the controller is installed.
+
[source,shell,subs="verbatim,attributes"]
----
$ kubectl get deployment -n kube-system aws-load-balancer-controller
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
aws-load-balancer-controller   2/2     2            2           84s
----
+
You receive the previous output if you deployed using Helm. If you deployed using the [.noloc]`Kubernetes` manifest, you only have one replica.
. Before using the controller to provision {aws} resources, your cluster must meet specific requirements. For more information, see <<alb-ingress>> and <<network-load-balancing>>.


[.topic]
[[lbc-manifest,lbc-manifest.title]]
=== Install [.noloc]`{aws} Load Balancer Controller` with manifests

[abstract]
--
Install the [.noloc]`{aws} Load Balancer Controller` add-on for Amazon EKS using [.noloc]`Kubernetes` manifests to provision Elastic Load Balancing resources. Configure IAM role and install `cert-manager` before applying controller manifest.
--

This topic describes how to install the controller by downloading and applying [.noloc]`Kubernetes` manifests. You can view the full https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/[documentation] for the controller on [.noloc]`GitHub`.  

In the following steps, replace the [.replaceable]`example values` with your own values.

[[lbc-manifest-prereqs,lbc-manifest-prereqs.title]]
==== Prerequisites

Before starting this tutorial, you must install and configure the following tools and resources that you need to create and manage an Amazon EKS cluster.



* An existing Amazon EKS cluster. To deploy one, see <<getting-started>>.
* An existing {aws} Identity and Access Management (IAM) [.noloc]`OpenID Connect` ([.noloc]`OIDC`) provider for your cluster. To determine whether you already have one, or to create one, see <<enable-iam-roles-for-service-accounts>>.
* Make sure that your [.noloc]`Amazon VPC CNI plugin for Kubernetes`, `kube-proxy`, and [.noloc]`CoreDNS` add-ons are at the minimum versions listed in  <<boundserviceaccounttoken-validated-add-on-versions,Service account tokens>>.
* Familiarity with {aws} Elastic Load Balancing. For more information, see the link:elasticloadbalancing/latest/userguide/[Elastic Load Balancing User Guide,type="documentation"].
* Familiarity with Kubernetes https://kubernetes.io/docs/concepts/services-networking/service/[service] and https://kubernetes.io/docs/concepts/services-networking/ingress/[ingress] resources.


[[lbc-iam,lbc-iam.title]]
==== Step 1: Configure IAM

[NOTE]
====

You only need to create a role for the [.noloc]`{aws} Load Balancer Controller` one per {aws} account. Check if `AmazonEKSLoadBalancerControllerRole` exists in the link:iam[IAM Console,type="console"]. If this role exists, skip to <<lbc-cert,Step 2: Install cert-manager>>.

====
. Download an IAM policy for the [.noloc]`{aws} Load Balancer Controller` that allows it to make calls to {aws} APIs on your behalf.  
+
{aws}:::
** 
[source,shell,subs="verbatim,attributes"]
----
$ curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.7.2/docs/install/iam_policy.json
----


{aws} GovCloud (US):::
** 
[source,shell,subs="verbatim,attributes"]
----
$ curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.7.2/docs/install/iam_policy_us-gov.json
----
[source,shell,subs="verbatim,attributes"]
----
$ mv iam_policy_us-gov.json iam_policy.json
----
. Create an IAM policy using the policy downloaded in the previous step. 
+
[source,shell,subs="verbatim,attributes"]
----
$ aws iam create-policy \
    --policy-name AWSLoadBalancerControllerIAMPolicy \
    --policy-document file://iam_policy.json
----
+
NOTE: If you view the policy in the {aws-management-console}, the console shows warnings for the *ELB* service, but not for the  *ELB v2* service. This happens because some of the actions in the policy exist for  *ELB v2*, but not for  *ELB*. You can ignore the warnings for  *ELB*.


eksctl::
.. Replace [.replaceable]`my-cluster` with the name of your cluster, [.replaceable]`111122223333` with your account ID, and then run the command. If your cluster is in the {aws} GovCloud (US-East) or {aws} GovCloud (US-West) {aws} Regions, then replace `{arn-aws}` with `arn:aws-us-gov:`.
+
[source,shell,subs="verbatim,attributes"]
----
$ eksctl create iamserviceaccount \
  --cluster=my-cluster \
  --namespace=kube-system \
  --name=aws-load-balancer-controller \
  --role-name AmazonEKSLoadBalancerControllerRole \
  --attach-policy-arn={arn-aws}iam::111122223333:policy/AWSLoadBalancerControllerIAMPolicy \
  --approve
----


{aws} CLI and kubectl::
.. Retrieve your cluster's [.noloc]`OIDC` provider ID and store it in a variable.
+
[source,bash,subs="verbatim,attributes"]
----
oidc_id=$(aws eks describe-cluster --name my-cluster --query "cluster.identity.oidc.issuer" --output text | cut -d '/' -f 5)
----
.. Determine whether an IAM [.noloc]`OIDC` provider with your cluster's ID is already in your account. You need [.noloc]`OIDC` configured for both the cluster and IAM.
+
[source,bash,subs="verbatim,attributes"]
----
aws iam list-open-id-connect-providers | grep $oidc_id | cut -d "/" -f4
----
+
If output is returned, then you already have an IAM [.noloc]`OIDC` provider for your cluster. If no output is returned, then you must create an IAM [.noloc]`OIDC` provider for your cluster. For more information, see <<enable-iam-roles-for-service-accounts>>.
.. Copy the following contents to your device. Replace [.replaceable]`111122223333` with your account ID. Replace [.replaceable]`region-code` with the {aws} Region that your cluster is in. Replace [.replaceable]`EXAMPLED539D4633E53DE1B71EXAMPLE` with the output returned in the previous step. If your cluster is in the {aws} GovCloud (US-East) or {aws} GovCloud (US-West) {aws} Regions, then replace `{arn-aws}` with `arn:aws-us-gov:`. After replacing the text, run the modified command to create the `load-balancer-role-trust-policy.json` file.
+
[source,json,subs="verbatim,attributes"]
----
cat >load-balancer-role-trust-policy.json <<EOF
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Federated": "{arn-aws}iam::111122223333:oidc-provider/oidc.eks.region-code.amazonaws.com/id/EXAMPLED539D4633E53DE1B71EXAMPLE"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.region-code.amazonaws.com/id/EXAMPLED539D4633E53DE1B71EXAMPLE:aud": "sts.amazonaws.com",
                    "oidc.eks.region-code.amazonaws.com/id/EXAMPLED539D4633E53DE1B71EXAMPLE:sub": "system:serviceaccount:kube-system:aws-load-balancer-controller"
                }
            }
        }
    ]
}
EOF
----
.. Create the IAM role.
+
[source,bash,subs="verbatim,attributes"]
----
aws iam create-role \
  --role-name AmazonEKSLoadBalancerControllerRole \
  --assume-role-policy-document file://"load-balancer-role-trust-policy.json"
----
.. Attach the required Amazon EKS managed IAM policy to the IAM role. Replace [.replaceable]`111122223333` with your account ID.
+
[source,bash,subs="verbatim,attributes"]
----
aws iam attach-role-policy \
  --policy-arn {arn-aws}iam::111122223333:policy/AWSLoadBalancerControllerIAMPolicy \
  --role-name AmazonEKSLoadBalancerControllerRole
----
.. Copy the following contents to your device. Replace [.replaceable]`111122223333` with your account ID. If your cluster is in the {aws} GovCloud (US-East) or {aws} GovCloud (US-West) {aws} Regions, then replace `{arn-aws}` with `arn:aws-us-gov:`. After replacing the text, run the modified command to create the `aws-load-balancer-controller-service-account.yaml` file.
+
[source,yaml,subs="verbatim,attributes"]
----
cat >aws-load-balancer-controller-service-account.yaml <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/name: aws-load-balancer-controller
  name: aws-load-balancer-controller
  namespace: kube-system
  annotations:
    eks.amazonaws.com/role-arn: {arn-aws}iam::111122223333:role/AmazonEKSLoadBalancerControllerRole
EOF
----
.. Create the [.noloc]`Kubernetes` service account on your cluster. The [.noloc]`Kubernetes` service account named `aws-load-balancer-controller` is annotated with the IAM role that you created named [.replaceable]`AmazonEKSLoadBalancerControllerRole`.
+
[source,shell,subs="verbatim,attributes"]
----
$ kubectl apply -f aws-load-balancer-controller-service-account.yaml
----


[[lbc-cert,lbc-cert.title]]
==== Step 2: Install `cert-manager`

// Not using `cert-manager` inline code in the cross-reference title because the underscore disrupts formatting.
Install `cert-manager` using one of the following methods to inject certificate configuration into the webhooks. For more information, see https://cert-manager.io/docs/installation/#getting-started[Getting Started] in the _cert-manager Documentation_.

We recommend using the `quay.io` container registry to install `cert-manager`. If your nodes do not have access to the `quay.io` container registry, Install `cert-manager` using Amazon ECR (see below).



[.noloc]`Quay.io`::
.. If your nodes have access to the `quay.io` container registry, install `cert-manager` to inject certificate configuration into the webhooks. 
+
[source,shell,subs="verbatim,attributes"]
----
$ kubectl apply \
    --validate=false \
    -f https://github.com/jetstack/cert-manager/releases/download/v1.13.5/cert-manager.yaml
----

// Not using `cert-manager` inline code in the cross-reference title because the underscore disrupts formatting.
Amazon ECR::
.. Install `cert-manager` using one of the following methods to inject certificate configuration into the webhooks. For more information, see https://cert-manager.io/docs/installation/#getting-started[Getting Started] in the _cert-manager Documentation_.
.. Download the manifest.
+
[source,bash,subs="verbatim,attributes"]
----
curl -Lo cert-manager.yaml https://github.com/jetstack/cert-manager/releases/download/v1.13.5/cert-manager.yaml
----
.. Pull the following images and push them to a repository that your nodes have access to. For more information on how to pull, tag, and push the images to your own repository, see <<copy-image-to-repository>>.
+
[source,bash,subs="verbatim,attributes"]
----
quay.io/jetstack/cert-manager-cainjector:v1.13.5
quay.io/jetstack/cert-manager-controller:v1.13.5
quay.io/jetstack/cert-manager-webhook:v1.13.5
----
.. Replace `quay.io` in the manifest for the three images with your own registry name. The following command assumes that your private repository's name is the same as the source repository. Replace [.replaceable]`111122223333.dkr.ecr.region-code.amazonaws.com` with your private registry.
+
[source,shell,subs="verbatim,attributes"]
----
$ sed -i.bak -e 's|quay.io|111122223333.dkr.ecr.region-code.amazonaws.com|' ./cert-manager.yaml
----
.. Apply the manifest.
+
[source,shell,subs="verbatim,attributes"]
----
$ kubectl apply \
    --validate=false \
    -f ./cert-manager.yaml
----


[[lbc-install,lbc-install.title]]
==== Step 3: Install [.noloc]`{aws} Load Balancer Controller`
. Download the controller specification. For more information about the controller, see the https://kubernetes-sigs.github.io/aws-load-balancer-controller/[documentation] on [.noloc]`GitHub`.
+
[source,bash,subs="verbatim,attributes"]
----
curl -Lo v2_7_2_full.yaml https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/v2.7.2/v2_7_2_full.yaml
----
. Make the following edits to the file.
+
.. If you downloaded the `v2_7_2_full.yaml` file, run the following command to remove the `ServiceAccount` section in the manifest. If you don't remove this section, the required annotation that you made to the service account in a previous step is overwritten. Removing this section also preserves the service account that you created in a previous step if you delete the controller.
+
[source,shell,subs="verbatim,attributes"]
----
$ sed -i.bak -e '612,620d' ./v2_7_2_full.yaml
----
+
If you downloaded a different file version, then open the file in an editor and remove the following lines.  
+
[source,yaml,subs="verbatim,attributes"]
----
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/name: aws-load-balancer-controller
  name: aws-load-balancer-controller
  namespace: kube-system
---
----
.. Replace `your-cluster-name` in the `Deployment` `spec` section of the file with the name of your cluster by replacing [.replaceable]`my-cluster` with the name of your cluster.
+
[source,shell,subs="verbatim,attributes"]
----
$ sed -i.bak -e 's|your-cluster-name|my-cluster|' ./v2_7_2_full.yaml
----
.. If your nodes don't have access to the Amazon EKS Amazon ECR image repositories, then you need to pull the following image and push it to a repository that your nodes have access to. For more information on how to pull, tag, and push an image to your own repository, see <<copy-image-to-repository>>.
+
[source,bash,subs="verbatim,attributes"]
----
public.ecr.aws/eks/aws-load-balancer-controller:v2.7.2
----
+
Add your registry's name to the manifest. The following command assumes that your private repository's name is the same as the source repository and adds your private registry's name to the file. Replace [.replaceable]`111122223333.dkr.ecr.region-code.amazonaws.com` with your registry. This line assumes that you named your private repository the same as the source repository. If not, change the `eks/aws-load-balancer-controller` text after your private registry name to your repository name.
+
[source,shell,subs="verbatim,attributes"]
----
$ sed -i.bak -e 's|public.ecr.aws/eks/aws-load-balancer-controller|111122223333.dkr.ecr.region-code.amazonaws.com/eks/aws-load-balancer-controller|' ./v2_7_2_full.yaml
----
.. (Required only for Fargate or Restricted IMDS) 
+
If you're deploying the controller to Amazon EC2 nodes that have https://aws.github.io/aws-eks-best-practices/security/docs/iam/#restrict-access-to-the-instance-profile-assigned-to-the-worker-node[restricted access to the Amazon EC2 instance metadata service (IMDS)], or if you're deploying to Fargate, then add the `following parameters` under `- args:`.
+
[source,yaml,subs="verbatim,attributes"]
----
[...]
spec:
      containers:
        - args:
            - --cluster-name=your-cluster-name
            - --ingress-class=alb
            - --aws-vpc-id=vpc-xxxxxxxx
            - --aws-region=region-code
            
            
[...]
----
. Apply the file.
+
[source,shell,subs="verbatim,attributes"]
----
$ kubectl apply -f v2_7_2_full.yaml
----
. Download the `IngressClass` and `IngressClassParams` manifest to your cluster.
+
[source,shell,subs="verbatim,attributes"]
----
$ curl -Lo v2_7_2_ingclass.yaml https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/v2.7.2/v2_7_2_ingclass.yaml
----
. Apply the manifest to your cluster.
+
[source,shell,subs="verbatim,attributes"]
----
$ kubectl apply -f v2_7_2_ingclass.yaml
----


[[lbc-verify,lbc-verify.title]]
==== Step 4: Verify that the controller is installed
. Verify that the controller is installed.
+
[source,shell,subs="verbatim,attributes"]
----
$ kubectl get deployment -n kube-system aws-load-balancer-controller
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
aws-load-balancer-controller   2/2     2            2           84s
----
+
You receive the previous output if you deployed using Helm. If you deployed using the [.noloc]`Kubernetes` manifest, you only have one replica.
. Before using the controller to provision {aws} resources, your cluster must meet specific requirements. For more information, see <<alb-ingress>> and <<network-load-balancing>>.


[.topic]
[[lbc-remove,lbc-remove.title]]
=== Migrate apps from deprecated ALB [.noloc]`Ingress Controller`

[abstract]
--
Learn how to migrate from the deprecated ALB Ingress Controller to the latest [.noloc]`{aws} Load Balancer Controller` release, ensuring smooth transition and uninterrupted load balancing capabilities.
--

This topic describes how to migrate from deprecated controller versions. More specifically, it describes how to remove deprecated versions of the [.noloc]`{aws} Load Balancer Controller`.  



* Deprecated versions cannot be upgraded. You must remove them first, and then install a current version.
+
[[lbc-deprecated-list]]
* Deprecated versions include:
+
** {aws} ALB Ingress Controller for [.noloc]`Kubernetes` ("Ingress Controller"), a predecessor to the [.noloc]`{aws} Load Balancer Controller`.
** Any `0.1.[.replaceable]``x``` version of the [.noloc]`{aws} Load Balancer Controller`


[[lbc-remove-desc,lbc-remove-desc.title]]
==== Remove the deprecated controller version

[NOTE]
====

You may have installed the deprecated version using Helm or manually with [.noloc]`Kubernetes` manifests. Complete the procedure using the tool that you originally installed it with.

====
. If you installed the `incubator/aws-alb-ingress-controller` Helm chart, uninstall it.
+
[source,shell,subs="verbatim,attributes"]
----
$ helm delete aws-alb-ingress-controller -n kube-system
----
. If you have version `0.1.[.replaceable]``x``` of the `eks-charts/aws-load-balancer-controller` chart installed, uninstall it. The upgrade from `0.1.[.replaceable]``x``` to version `1.0.0` doesn't work due to incompatibility with the webhook API version. 
+
[source,shell,subs="verbatim,attributes"]
----
$ helm delete aws-load-balancer-controller -n kube-system
----
. Check to see if the controller is currently installed.
+
[source,shell,subs="verbatim,attributes"]
----
$ kubectl get deployment -n kube-system alb-ingress-controller
----
+
This is the output if the controller isn't installed.  
+
+
This is the output if the controller is installed.
+
[source,bash,subs="verbatim,attributes"]
----
NAME                   READY UP-TO-DATE AVAILABLE AGE
alb-ingress-controller 1/1   1          1         122d
----
. Enter the following commands to remove the controller.
+
[source,shell,subs="verbatim,attributes"]
----
$ kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/alb-ingress-controller.yaml
kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/rbac-role.yaml
----


[[lbc-migrate,lbc-migrate.title]]
==== Migrate to [.noloc]`{aws} Load Balancer Controller`

To migrate from the ALB Ingress Controller for [.noloc]`Kubernetes` to the [.noloc]`{aws} Load Balancer Controller`, you need to:

. Remove the ALB Ingress Controller (see above).
. <<lbc-overview,Install the {aws} Load Balancer Controller.>>
. Add an additional policy to the IAM Role used by the [.noloc]`{aws} Load Balancer Controller`. This policy permits the LBC to manage resources created by the ALB Ingress Controller for [.noloc]`Kubernetes`.
. Download the IAM policy. This policy permits the [.noloc]`{aws} Load Balancer Controller` to manage resources created by the ALB Ingress Controller for [.noloc]`Kubernetes`. You can also https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy_v1_to_v2_additional.json[view the policy].
+
[source,shell,subs="verbatim,attributes"]
----
$ curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.7.2/docs/install/iam_policy_v1_to_v2_additional.json
----
. If your cluster is in the {aws} GovCloud (US-East) or {aws} GovCloud (US-West) {aws} Regions, then replace `{arn-aws}` with `arn:aws-us-gov:`..
+
[source,shell,subs="verbatim,attributes"]
----
$ sed -i.bak -e 's|{arn-aws}|arn:aws-us-gov:|' iam_policy_v1_to_v2_additional.json
----
. Create the IAM policy and note the ARN that is returned.
+
[source,shell,subs="verbatim,attributes"]
----
$ aws iam create-policy \
  --policy-name AWSLoadBalancerControllerAdditionalIAMPolicy \
  --policy-document file://iam_policy_v1_to_v2_additional.json
----
. Attach the IAM policy to the IAM role used by the [.noloc]`{aws} Load Balancer Controller`. Replace [.replaceable]`your-role-name` with the name of the role, such as `AmazonEKSLoadBalancerControllerRole`. 
+
If you created the role using `eksctl`, then to find the role name that was created, open the link:cloudformation[{aws} CloudFormation console,type="console"] and select the *eksctl-[.replaceable]`my-cluster`-addon-iamserviceaccount-kube-system-aws-load-balancer-controller* stack. Select the  *Resources* tab. The role name is in the  *Physical ID* column.  If your cluster is in the {aws} GovCloud (US-East) or {aws} GovCloud (US-West) {aws} Regions, then replace `{arn-aws}` with `arn:aws-us-gov:`.
+
[source,shell,subs="verbatim,attributes"]
----
$ aws iam attach-role-policy \
  --role-name your-role-name \
  --policy-arn {arn-aws}iam::111122223333:policy/AWSLoadBalancerControllerAdditionalIAMPolicy
----


[.topic]
[[managing-coredns,managing-coredns.title]]
== Manage CoreDNS for DNS in Amazon EKS clusters

[abstract]
--
Learn how to manage the [.noloc]`CoreDNS` Amazon EKS add-on for DNS service discovery in [.noloc]`Kubernetes` clusters with configuration updates and version upgrades.
--

[.noloc]`CoreDNS` is a flexible, extensible DNS server that can serve as the [.noloc]`Kubernetes` cluster DNS. When you launch an Amazon EKS cluster with at least one node, two replicas of the [.noloc]`CoreDNS` image are deployed by default, regardless of the number of nodes deployed in your cluster. The [.noloc]`CoreDNS` [.noloc]`Pods` provide name resolution for all [.noloc]`Pods` in the cluster. The [.noloc]`CoreDNS` [.noloc]`Pods` can be deployed to Fargate nodes if your cluster includes a Fargate Profile with a namespace that matches the namespace for the [.noloc]`CoreDNS` `deployment`. For more information on Fargate Profiles, see <<fargate-profile>>. For more information about [.noloc]`CoreDNS`, see https://kubernetes.io/docs/tasks/administer-cluster/coredns/[Using CoreDNS for Service Discovery] in the [.noloc]`Kubernetes` documentation.

[[coredns-versions,coredns-versions.title]]
=== [.noloc]`CoreDNS` versions

The following table lists the latest version of the Amazon EKS add-on type for each [.noloc]`Kubernetes` version.

[options="header"]
|===
| Kubernetes version | [.noloc]`CoreDNS` version
| 1.31 | v1.11.3-eksbuild.2
| 1.30 | v1.11.3-eksbuild.2
| 1.29 | v1.11.3-eksbuild.2
| 1.28 | v1.10.1-eksbuild.15
| 1.27 | v1.10.1-eksbuild.15
| 1.26 | v1.9.3-eksbuild.19
| 1.25 | v1.9.3-eksbuild.19
| 1.24 | v1.9.3-eksbuild.19
| 1.23 | v1.8.7-eksbuild.18
|===

[IMPORTANT]
====

If you're self-managing this add-on, the versions in the table might not be the same as the available self-managed versions. For more information about updating the self-managed type of this add-on, see <<coredns-add-on-self-managed-update>>.

====

[[coredns-upgrade,coredns-upgrade.title]]
=== Important [.noloc]`CoreDNS` upgrade considerations

* To improve the stability and availability of the [.noloc]`CoreDNS` [.noloc]`Deployment`, versions `v1.9.3-eksbuild.6` and later and `v1.10.1-eksbuild.3` are deployed with a `PodDisruptionBudget`. If you've deployed an existing `PodDisruptionBudget`, your upgrade to these versions might fail. If the upgrade fails, completing one of the following tasks should resolve the issue:
+
** When doing the upgrade of the Amazon EKS add-on, choose to override the existing settings as your conflict resolution option. If you've made other custom settings to the [.noloc]`Deployment`, make sure to back up your settings before upgrading so that you can reapply your other custom settings after the upgrade.
** Remove your existing `PodDisruptionBudget` and try the upgrade again.
* In EKS add-on versions `v1.9.3-eksbuild.3` and later and `v1.10.1-eksbuild.6` and later, the [.noloc]`CoreDNS` [.noloc]`Deployment` sets the `readinessProbe` to use the `/ready` endpoint. This endpoint is enabled in the `Corefile` configuration file for [.noloc]`CoreDNS`.
+
If you use a custom `Corefile`, you must add the `ready` plugin to the config, so that the `/ready` endpoint is active in [.noloc]`CoreDNS` for the probe to use.
* In EKS add-on versions `v1.9.3-eksbuild.7` and later and `v1.10.1-eksbuild.4` and later, you can change the `PodDisruptionBudget`. You can edit the add-on and change these settings in the *Optional configuration settings* using the fields in the following example. This example shows the default `PodDisruptionBudget`.
+
[source,json,subs="verbatim,attributes"]
----
{
    "podDisruptionBudget": {
        "enabled": true,
        "maxUnavailable": 1
        }
}
----
// Not using [.noloc]`Kubernetes` here because the _ causes issues with the rendering.
+
You can set `maxUnavailable` or `minAvailable`, but you can't set both in a single `PodDisruptionBudget`. For more information about `PodDisruptionBudgets`, see https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget[Specifying a PodDisruptionBudget] in the _Kubernetes documentation_.
+
Note that if you set `enabled` to `false`, the `PodDisruptionBudget` isn't removed. After you set this field to `false`, you must delete the `PodDisruptionBudget` object. Similarly, if you edit the add-on to use an older version of the add-on (downgrade the add-on) after upgrading to a version with a `PodDisruptionBudget`, the `PodDisruptionBudget` isn't removed. To delete the `PodDisruptionBudget`, you can run the following command:
+
[source,bash,subs="verbatim,attributes"]
----
kubectl delete poddisruptionbudget coredns -n kube-system
----
* In EKS add-on versions `v1.10.1-eksbuild.5` and later, change the default toleration from `node-role.kubernetes.io/master:NoSchedule` to `node-role.kubernetes.io/control-plane:NoSchedule` to comply with KEP 2067. For more information about KEP 2067, see https://github.com/kubernetes/enhancements/tree/master/keps/sig-cluster-lifecycle/kubeadm/2067-rename-master-label-taint#renaming-the-node-rolekubernetesiomaster-node-taint[KEP-2067: Rename the kubeadm "master" label and taint] in the _Kubernetes Enhancement Proposals (KEPs)_ on [.noloc]`GitHub`.
+
In EKS add-on versions `v1.8.7-eksbuild.8` and later and `v1.9.3-eksbuild.9` and later, both tolerations are set to be compatible with every [.noloc]`Kubernetes` version.
* In EKS add-on versions `v1.9.3-eksbuild.11` and `v1.10.1-eksbuild.7` and later, the [.noloc]`CoreDNS` [.noloc]`Deployment` sets a default value for `topologySpreadConstraints`. The default value ensures that the [.noloc]`CoreDNS` [.noloc]`Pods` are spread across the Availability Zones if there are nodes in multiple Availability Zones available. You can set a custom value that will be used instead of the default value. The default value follows:
+
[source,yaml,subs="verbatim,attributes"]
----
topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: ScheduleAnyway
    labelSelector:
      matchLabels:
        k8s-app: kube-dns
----


[[coredns-upgrade-1.11,coredns-upgrade-1.11.title]]
==== [.noloc]`CoreDNS` `v1.11` upgrade considerations

* In EKS add-on versions `v1.11.1-eksbuild.4` and later, the container image is based on a https://gallery.ecr.aws/eks-distro-build-tooling/eks-distro-minimal-base[minimal base image] maintained by Amazon EKS Distro, which contains minimal packages and doesn't have shells. For more information, see https://distro.eks.amazonaws.com/[Amazon EKS Distro]. The usage and troubleshooting of the [.noloc]`CoreDNS` image remains the same.


[.topic]
[[coredns-add-on-create,coredns-add-on-create.title]]
=== Create the [.noloc]`CoreDNS` Amazon EKS add-on

Create the [.noloc]`CoreDNS` Amazon EKS add-on. You must have a cluster before you create the add-on. For more information, see <<create-cluster>>.

. See which version of the add-on is installed on your cluster.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl describe deployment coredns --namespace kube-system | grep coredns: | cut -d : -f 3
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
v1.10.1-eksbuild.13
----
. See which type of the add-on is installed on your cluster. Depending on the tool that you created your cluster with, you might not currently have the Amazon EKS add-on type installed on your cluster. Replace [.replaceable]`my-cluster` with the name of your cluster.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks describe-addon --cluster-name my-cluster --addon-name coredns --query addon.addonVersion --output text
----
+
If a version number is returned, you have the Amazon EKS type of the add-on installed on your cluster and don't need to complete the remaining steps in this procedure. If an error is returned, you don't have the Amazon EKS type of the add-on installed on your cluster. Complete the remaining steps of this procedure to install it.
. Save the configuration of your currently installed add-on.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl get deployment coredns -n kube-system -o yaml > aws-k8s-coredns-old.yaml
----
. Create the add-on using the {aws} CLI. If you want to use the {aws-management-console} or `eksctl` to create the add-on, see <<creating-an-add-on>> and specify `coredns` for the add-on name. Copy the command that follows to your device. Make the following modifications to the command, as needed, and then run the modified command.
+
** Replace [.replaceable]`my-cluster` with the name of your cluster.
** Replace [.replaceable]`v1.11.3-eksbuild.1` with the latest version listed in the <<coredns-versions,latest version table>> for your cluster version.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks create-addon --cluster-name my-cluster --addon-name coredns --addon-version v1.11.3-eksbuild.1
----
+
If you've applied custom settings to your current add-on that conflict with the default settings of the Amazon EKS add-on, creation might fail. If creation fails, you receive an error that can help you resolve the issue. Alternatively, you can add `--resolve-conflicts OVERWRITE` to the previous command. This allows the add-on to overwrite any existing custom settings. Once you've created the add-on, you can update it with your custom settings.
. Confirm that the latest version of the add-on for your cluster's [.noloc]`Kubernetes` version was added to your cluster. Replace [.replaceable]`my-cluster` with the name of your cluster.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks describe-addon --cluster-name my-cluster --addon-name coredns --query addon.addonVersion --output text
----
+
It might take several seconds for add-on creation to complete.
+
An example output is as follows.
+
[source,json,subs="verbatim,attributes"]
----
v1.11.3-eksbuild.1
----
. If you made custom settings to your original add-on, before you created the Amazon EKS add-on, use the configuration that you saved in a previous step to update the Amazon EKS add-on with your custom settings. For instructions to update the add-on, see <<coredns-add-on-update>>.


[.topic]
[[coredns-add-on-update,coredns-add-on-update.title]]
=== Update the [.noloc]`CoreDNS` Amazon EKS add-on

Update the Amazon EKS type of the add-on. If you haven't added the Amazon EKS add-on to your cluster, either  <<coredns-add-on-create,add it>> or see <<coredns-add-on-self-managed-update>>.

Before you begin, review the upgrade considerations. For more information, see <<coredns-upgrade>>.

. See which version of the add-on is installed on your cluster. Replace [.replaceable]`my-cluster` with your cluster name.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks describe-addon --cluster-name my-cluster --addon-name coredns --query "addon.addonVersion" --output text
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
v1.10.1-eksbuild.13
----
+
If the version returned is the same as the version for your cluster's [.noloc]`Kubernetes` version in the  <<coredns-versions,latest version table>>, then you already have the latest version installed on your cluster and don't need to complete the rest of this procedure. If you receive an error, instead of a version number in your output, then you don't have the Amazon EKS type of the add-on installed on your cluster. You need to <<coredns-add-on-create,create the add-on>> before you can update it with this procedure.
. Save the configuration of your currently installed add-on.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl get deployment coredns -n kube-system -o yaml > aws-k8s-coredns-old.yaml
----
. Update your add-on using the {aws} CLI. If you want to use the {aws-management-console} or `eksctl` to update the add-on, see <<updating-an-add-on>>. Copy the command that follows to your device. Make the following modifications to the command, as needed, and then run the modified command.
+
** Replace [.replaceable]`my-cluster` with the name of your cluster.
** Replace [.replaceable]`v1.11.3-eksbuild.1` with the latest version listed in the <<coredns-versions,latest version table>> for your cluster version.
** The `--resolve-conflicts[.replaceable]``PRESERVE``` option preserves existing configuration values for the add-on. If you've set custom values for add-on settings, and you don't use this option, Amazon EKS overwrites your values with its default values. If you use this option, then we recommend testing any field and value changes on a non-production cluster before updating the add-on on your production cluster. If you change this value to `OVERWRITE`, all settings are changed to Amazon EKS default values. If you've set custom values for any settings, they might be overwritten with Amazon EKS default values. If you change this value to `none`, Amazon EKS doesn't change the value of any settings, but the update might fail. If the update fails, you receive an error message to help you resolve the conflict. 
** If you're not updating a configuration setting, remove ``--configuration-values '{[.replaceable]`"replicaCount":3`}'`` from the command. If you're updating a configuration setting, replace [.replaceable]`"replicaCount":3` with the setting that you want to set. In this example, the number of replicas of [.noloc]`CoreDNS` is set to `3`. The value that you specify must be valid for the configuration schema. If you don't know the configuration schema, run ``aws eks describe-addon-configuration --addon-name coredns --addon-version [.replaceable]`v1.11.3-eksbuild.1` ``, replacing [.replaceable]`v1.11.3-eksbuild.1` with the version number of the add-on that you want to see the configuration for. The schema is returned in the output. If you have any existing custom configuration, want to remove it all, and set the values for all settings back to Amazon EKS defaults, remove [.replaceable]`"replicaCount":3` from the command, so that you have empty `{}`. For more information about [.noloc]`CoreDNS` settings, see https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/[Customizing DNS Service] in the [.noloc]`Kubernetes` documentation.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks update-addon --cluster-name my-cluster --addon-name coredns --addon-version v1.11.3-eksbuild.1 \
    --resolve-conflicts PRESERVE --configuration-values '{"replicaCount":3}'
----
+
It might take several seconds for the update to complete.
. Confirm that the add-on version was updated. Replace [.replaceable]`my-cluster` with the name of your cluster.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks describe-addon --cluster-name my-cluster --addon-name coredns
----
+
It might take several seconds for the update to complete.
+
An example output is as follows.
+
[source,json,subs="verbatim,attributes"]
----
{
    "addon": {
        "addonName": "coredns",
        "clusterName": "my-cluster",
        "status": "ACTIVE",
        "addonVersion": "v1.11.3-eksbuild.1",
        "health": {
            "issues": []
        },
        "addonArn": "{arn-aws}eks:region:111122223333:addon/my-cluster/coredns/d2c34f06-1111-2222-1eb0-24f64ce37fa4",
        "createdAt": "2023-03-01T16:41:32.442000+00:00",
        "modifiedAt": "2023-03-01T18:16:54.332000+00:00",
        "tags": {},
        "configurationValues": "{\"replicaCount\":3}"
    }
}
----


[.topic]
[[coredns-add-on-self-managed-update,coredns-add-on-self-managed-update.title]]
=== Update the [.noloc]`CoreDNS` Amazon EKS self-managed add-on

[IMPORTANT]
====

We recommend adding the Amazon EKS type of the add-on to your cluster instead of using the self-managed type of the add-on. If you're not familiar with the difference between the types, see <<eks-add-ons>>. For more information about adding an Amazon EKS add-on to your cluster, see <<creating-an-add-on>>. If you're unable to use the Amazon EKS add-on, we encourage you to submit an issue about why you can't to the https://github.com/aws/containers-roadmap/issues[Containers roadmap GitHub repository].

====

Before you begin, review the upgrade considerations. For more information, see <<coredns-upgrade>>.

. Confirm that you have the self-managed type of the add-on installed on your cluster. Replace [.replaceable]`my-cluster` with the name of your cluster.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks describe-addon --cluster-name my-cluster --addon-name coredns --query addon.addonVersion --output text
----
+
If an error message is returned, you have the self-managed type of the add-on installed on your cluster. Complete the remaining steps in this procedure. If a version number is returned, you have the Amazon EKS type of the add-on installed on your cluster. To update the Amazon EKS type of the add-on, use the procedure in  <<coredns-add-on-update,Update the CoreDNS Amazon EKS add-on>>, rather than using this procedure. If you're not familiar with the differences between the add-on types, see <<eks-add-ons>>.
. See which version of the container image is currently installed on your cluster.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl describe deployment coredns -n kube-system | grep Image | cut -d ":" -f 3
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
v1.8.7-eksbuild.2
----
. If your current [.noloc]`CoreDNS` version is `v1.5.0` or later, but earlier than the version listed in the <<coredns-versions,CoreDNS versions>> table, then skip this step. If your current version is earlier than `1.5.0`, then you need to modify the `ConfigMap` for [.noloc]`CoreDNS` to use the forward add-on, rather than the proxy add-on.
+
.. Open the `ConfigMap` with the following command.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl edit configmap coredns -n kube-system
----
.. Replace `proxy` in the following line with `forward`. Save the file and exit the editor.
+
[source,bash,subs="verbatim,attributes"]
----
proxy . /etc/resolv.conf
----
. If you originally deployed your cluster on [.noloc]`Kubernetes` `1.17` or earlier, then you may need to remove a discontinued line from your [.noloc]`CoreDNS` manifest.
+
IMPORTANT: You must complete this step before updating to [.noloc]`CoreDNS` version `1.7.0`, but it's recommended that you complete this step even if you're updating to an earlier version. 
+
.. Check to see if your [.noloc]`CoreDNS` manifest has the line.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl get configmap coredns -n kube-system -o jsonpath='{$.data.Corefile}' | grep upstream
----
+
If no output is returned, your manifest doesn't have the line and you can skip to the next step to update [.noloc]`CoreDNS`. If output is returned, then you need to remove the line.
.. Edit the `ConfigMap` with the following command, removing the line in the file that has the word `upstream` in it. Do not change anything else in the file. Once the line is removed, save the changes.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl edit configmap coredns -n kube-system -o yaml
----
. Retrieve your current [.noloc]`CoreDNS` image version:
+
[source,bash,subs="verbatim,attributes"]
----
kubectl describe deployment coredns -n kube-system | grep Image
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
602401143452.dkr.ecr.region-code.amazonaws.com/eks/coredns:v1.8.7-eksbuild.2
----
. If you're updating to [.noloc]`CoreDNS` `1.8.3` or later, then you need to add the `endpointslices` permission to the `system:coredns` [.noloc]`Kubernetes` `clusterrole`.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl edit clusterrole system:coredns -n kube-system
----
+
Add the following lines under the existing permissions lines in the `rules` section of the file.
+
[source,yaml,subs="verbatim,attributes"]
----
[...]
- apiGroups:
  - discovery.k8s.io
  resources:
  - endpointslices
  verbs:
  - list
  - watch
[...]
----
. Update the [.noloc]`CoreDNS` add-on by replacing [.replaceable]`602401143452` and [.replaceable]`region-code` with the values from the output returned in a previous step. Replace [.replaceable]`v1.11.3-eksbuild.1` with the [.noloc]`CoreDNS` version listed in the  <<coredns-versions,latest versions table>> for your [.noloc]`Kubernetes` version.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl set image deployment.apps/coredns -n kube-system  coredns=602401143452.dkr.ecr.region-code.amazonaws.com/eks/coredns:v1.11.3-eksbuild.1
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
deployment.apps/coredns image updated
----
. Check the container image version again to confirm that it was updated to the version that you specified in the previous step.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl describe deployment coredns -n kube-system | grep Image | cut -d ":" -f 3
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
v1.11.3-eksbuild.1
----


[.topic]
[[coredns-autoscaling,coredns-autoscaling.title]]
=== Scale [.noloc]`CoreDNS` [.noloc]`Pods` for high DNS traffic

[abstract]
--
Learn how the Amazon EKS add-on for [.noloc]`CoreDNS` autoscales to handle increased load on DNS pods, improving application availability and cluster scalability.
--

When you launch an Amazon EKS cluster with at least one node, a [.noloc]`Deployment` of two replicas of the [.noloc]`CoreDNS` image are deployed by default, regardless of the number of nodes deployed in your cluster. The [.noloc]`CoreDNS` Pods provide name resolution for all Pods in the cluster. Applications use name resolution to connect to pods and services in the cluster as well as connecting to services outside the cluster. As the number of requests for name resolution (queries) from pods increase, the [.noloc]`CoreDNS` pods can get overwhelmed and slow down, and reject requests that the pods can`'t handle.

To handle the increased load on the [.noloc]`CoreDNS` pods, consider an autoscaling system for [.noloc]`CoreDNS`. Amazon EKS can manage the autoscaling of the [.noloc]`CoreDNS` Deployment in the EKS Add-on version of [.noloc]`CoreDNS`. This [.noloc]`CoreDNS` autoscaler continuously monitors the cluster state, including the number of nodes and CPU cores. Based on that information, the controller will dynamically adapt the number of replicas of the [.noloc]`CoreDNS` deployment in an EKS cluster. This feature works for [.noloc]`CoreDNS` `v1.9` and EKS release version `1.25` and later. For more information about which versions are compatible with [.noloc]`CoreDNS` Autoscaling, see the following section.

We recommend using this feature in conjunction with other https://aws.github.io/aws-eks-best-practices/cluster-autoscaling/[EKS Cluster Autoscaling best practices] to improve overall application availability and cluster scalability.

[[coredns-autoscaling-prereqs,coredns-autoscaling-prereqs.title]]
==== Prerequisites

For Amazon EKS to scale your [.noloc]`CoreDNS` deployment, there are three prerequisites:



* You must be using the _EKS Add-on_ version of [.noloc]`CoreDNS`.
* Your cluster must be running at least the minimum cluster versions and platform versions.
* Your cluster must be running at least the minimum version of the EKS Add-on of [.noloc]`CoreDNS`.


[[coredns-autoscaling-cluster-version,coredns-autoscaling-cluster-version.title]]
===== Minimum cluster version

Autoscaling of [.noloc]`CoreDNS` is done by a new component in the cluster control plane, managed by Amazon EKS. Because of this, you must upgrade your cluster to an EKS release that supports the minimum platform version that has the new component.

A new Amazon EKS cluster. To deploy one, see <<getting-started>>. The cluster must be [.noloc]`Kubernetes` version `1.25` or later. The cluster must be running one of the [.noloc]`Kubernetes` versions and platform versions listed in the following table or a later version. Note that any [.noloc]`Kubernetes` and platform versions later than those listed are also supported. You can check your current [.noloc]`Kubernetes` version by replacing [.replaceable]`my-cluster` in the following command with the name of your cluster and then running the modified command:

[source,bash,subs="verbatim,attributes"]
----
aws eks describe-cluster
              --name my-cluster --query cluster.version --output
              text
----

[cols="1,1", options="header"]
|===
|Kubernetes version
|Platform version


|`1.29.3`
|`eks.7`

|`1.28.8`
|`eks.13`

|`1.27.12`
|`eks.17`

|`1.26.15`
|`eks.18`

|`1.25.16`
|`eks.19`
|===

[NOTE]
====

Every platform version of later [.noloc]`Kubernetes` versions are also supported, for example [.noloc]`Kubernetes` version `1.30` from `eks.1` and on.

====

[[coredns-autoscaling-coredns-version,coredns-autoscaling-coredns-version.title]]
===== Minimum EKS Add-on version

[cols="1,1,1,1,1,1", options="header"]
|===
|Kubernetes version
|1.29
|1.28
|1.27
|1.26
|1.25


|
|`v1.11.1-eksbuild.9`
|`v1.10.1-eksbuild.11`
|`v1.10.1-eksbuild.11`
|`v1.9.3-eksbuild.15`
|`v1.9.3-eksbuild.15`
|===


[[coredns-autoscaling-console,coredns-autoscaling-console.title]]
.Configuring [.noloc]`CoreDNS` autoscaling in the {aws-management-console}
[%collapsible]
====
. Ensure that your cluster is at or above the minimum cluster version.
+
Amazon EKS upgrades clusters between platform versions of the same [.noloc]`Kubernetes` version automatically, and you can`'t start this process yourself. Instead, you can upgrade your cluster to the next [.noloc]`Kubernetes` version, and the cluster will be upgraded to that K8s version and the latest platform version. For example, if you upgrade from `1.25` to `1.26`, the cluster will upgrade to `1.26.15 eks.18`.
+
New [.noloc]`Kubernetes` versions sometimes introduce significant changes. Therefore, we recommend that you test the behavior of your applications by using a separate cluster of the new [.noloc]`Kubernetes` version before you update your production clusters.
+
To upgrade a cluster to a new [.noloc]`Kubernetes` version, follow the procedure in  <<update-cluster,Update existing cluster to new Kubernetes version>>.
. Ensure that you have the EKS Add-on for [.noloc]`CoreDNS`, not the self-managed [.noloc]`CoreDNS` Deployment.
+
Depending on the tool that you created your cluster with, you might not currently have the Amazon EKS add-on type installed on your cluster. To see which type of the add-on is installed on your cluster, you can run the following command. Replace `my-cluster` with the name of your cluster.
+
[source,shell,subs="verbatim,attributes"]
----
aws eks describe-addon --cluster-name my-cluster --addon-name coredns --query addon.addonVersion --output text
----
+
If a version number is returned, you have the Amazon EKS type of the add-on installed on your cluster and you can continue with the next step. If an error is returned, you don't have the Amazon EKS type of the add-on installed on your cluster. Complete the remaining steps of the procedure  <<coredns-add-on-create,Create the CoreDNS Amazon EKS add-on>> to replace the self-managed version with the Amazon EKS add-on.
. Ensure that your EKS Add-on for [.noloc]`CoreDNS` is at a version the same or higher than the minimum EKS Add-on version.
+
See which version of the add-on is installed on your cluster. You can check in the {aws-management-console} or run the following command:
+
[source,shell,subs="verbatim,attributes"]
----
kubectl describe deployment coredns --namespace kube-system | grep coredns: | cut -d : -f 3
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
v1.10.1-eksbuild.13
----
+
Compare this version with the minimum EKS Add-on version in the previous section. If needed, upgrade the EKS Add-on to a higher version by following the procedure  <<coredns-add-on-update,Update the CoreDNS Amazon EKS add-on>>.
. Add the autoscaling configuration to the *Optional configuration settings* of the EKS Add-on.
+
.. Open the link:eks/home#/clusters[Amazon EKS console,type="console"].
.. In the left navigation pane, select *Clusters*, and then select the name of the cluster that you want to configure the add-on for.
.. Choose the *Add-ons* tab.
.. Select the box in the top right of the [.noloc]`CoreDNS` add-on box and then choose  *Edit*.
.. On the *Configure [.noloc]`CoreDNS`* page:
+
... Select the *Version* that you'd like to use. We recommend that you keep the same version as the previous step, and update the version and configuration in separate actions.
... Expand the *Optional configuration settings*.
... Enter the JSON key `"autoscaling":` and value of a nested JSON object with a key `"enabled":` and value `true` in *Configuration values*. The resulting text must be a valid JSON object. If this key and value are the only data in the text box, surround the key and value with curly braces `{ }`. The following example shows autoscaling is enabled:
+
[source,json,subs="verbatim,attributes"]
----
{
  "autoScaling": {
    "enabled": true
  }
}
----
... (Optional) You can provide minimum and maximum values that autoscaling can scale the number of [.noloc]`CoreDNS` pods to.
+
The following example shows autoscaling is enabled and all of the optional keys have values. We recommend that the minimum number of [.noloc]`CoreDNS` pods is always greater than 2 to provide resilience for the DNS service in the cluster.
+
[source,json,subs="verbatim,attributes"]
----
{
  "autoScaling": {
    "enabled": true,
    "minReplicas": 2,
    "maxReplicas": 10
  }
}
----
.. To apply the new configuration by replacing the [.noloc]`CoreDNS` pods, choose  *Save changes*.
+
Amazon EKS applies changes to the EKS Add-ons by using a _rollout_ of the [.noloc]`Kubernetes` Deployment for CoreDNS. You can track the status of the rollout in the  *Update history* of the add-on in the {aws-management-console} and with `kubectl rollout status deployment/coredns --namespace kube-system`.
+
`kubectl rollout` has the following commands:
+
[source,shell,subs="verbatim,attributes"]
----
$ kubectl rollout
                            
history  -- View rollout history
pause    -- Mark the provided resource as paused
restart  -- Restart a resource
resume   -- Resume a paused resource
status   -- Show the status of the rollout
undo     -- Undo a previous rollout
----
+
If the rollout takes too long, Amazon EKS will undo the rollout, and a message with the type of  *Addon Update* and a status of  *Failed* will be added to the  *Update history* of the add-on. To investigate any issues, start from the history of the rollout, and run `kubectl logs` on a [.noloc]`CoreDNS` pod to see the logs of [.noloc]`CoreDNS`.
. If the new entry in the *Update history* has a status of  *Successful*, then the rollout has completed and the add-on is using the new configuration in all of the [.noloc]`CoreDNS` pods. As you change the number of nodes and CPU cores of nodes in the cluster, Amazon EKS scales the number of replicas of the [.noloc]`CoreDNS` deployment.

====

[[coredns-autoscaling-cli,coredns-autoscaling-cli.title]]
.Configuring [.noloc]`CoreDNS` autoscaling in the {aws} Command Line Interface
[%collapsible]
====
. Ensure that your cluster is at or above the minimum cluster version.
+
Amazon EKS upgrades clusters between platform versions of the same [.noloc]`Kubernetes` version automatically, and you can`'t start this process yourself. Instead, you can upgrade your cluster to the next [.noloc]`Kubernetes` version, and the cluster will be upgraded to that K8s version and the latest platform version. For example, if you upgrade from `1.25` to `1.26`, the cluster will upgrade to `1.26.15 eks.18`.
+
New [.noloc]`Kubernetes` versions sometimes introduce significant changes. Therefore, we recommend that you test the behavior of your applications by using a separate cluster of the new [.noloc]`Kubernetes` version before you update your production clusters.
+
To upgrade a cluster to a new [.noloc]`Kubernetes` version, follow the procedure in  <<update-cluster,Update existing cluster to new Kubernetes version>>.
. Ensure that you have the EKS Add-on for [.noloc]`CoreDNS`, not the self-managed [.noloc]`CoreDNS` Deployment.
+
Depending on the tool that you created your cluster with, you might not currently have the Amazon EKS add-on type installed on your cluster. To see which type of the add-on is installed on your cluster, you can run the following command. Replace `my-cluster` with the name of your cluster.
+
[source,shell,subs="verbatim,attributes"]
----
aws eks describe-addon --cluster-name my-cluster --addon-name coredns --query addon.addonVersion --output text
----
+
If a version number is returned, you have the Amazon EKS type of the add-on installed on your cluster. If an error is returned, you don't have the Amazon EKS type of the add-on installed on your cluster. Complete the remaining steps of the procedure <<coredns-add-on-create,Create the CoreDNS Amazon EKS add-on>> to replace the self-managed version with the Amazon EKS add-on.
. Ensure that your EKS Add-on for [.noloc]`CoreDNS` is at a version the same or higher than the minimum EKS Add-on version.
+
See which version of the add-on is installed on your cluster. You can check in the {aws-management-console} or run the following command:
+
[source,shell,subs="verbatim,attributes"]
----
kubectl describe deployment coredns --namespace kube-system | grep coredns: | cut -d : -f 3
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
v1.10.1-eksbuild.13
----
+
Compare this version with the minimum EKS Add-on version in the previous section. If needed, upgrade the EKS Add-on to a higher version by following the procedure  <<coredns-add-on-update,Update the CoreDNS Amazon EKS add-on>>.
. Add the autoscaling configuration to the *Optional configuration settings* of the EKS Add-on.
+
Run the following {aws} CLI command. Replace `my-cluster` with the name of your cluster and the IAM role ARN with the role that you are using.
+
[source,shell,subs="verbatim,attributes"]
----
aws eks update-addon --cluster-name my-cluster --addon-name coredns \
    --resolve-conflicts PRESERVE --configuration-values '{"autoScaling":{"enabled":true}}'
----
+
Amazon EKS applies changes to the EKS Add-ons by using a  _rollout_ of the [.noloc]`Kubernetes` Deployment for CoreDNS. You can track the status of the rollout in the  *Update history* of the add-on in the {aws-management-console} and with `kubectl rollout status deployment/coredns --namespace kube-system`.
+
`kubectl rollout` has the following commands:
+
[source,shell,subs="verbatim,attributes"]
----
kubectl rollout
                            
history  -- View rollout history
pause    -- Mark the provided resource as paused
restart  -- Restart a resource
resume   -- Resume a paused resource
status   -- Show the status of the rollout
undo     -- Undo a previous rollout
----
+
If the rollout takes too long, Amazon EKS will undo the rollout, and a message with the type of  *Addon Update* and a status of  *Failed* will be added to the  *Update history* of the add-on. To investigate any issues, start from the history of the rollout, and run `kubectl logs` on a [.noloc]`CoreDNS` pod to see the logs of [.noloc]`CoreDNS`.
. (Optional) You can provide minimum and maximum values that autoscaling can scale the number of [.noloc]`CoreDNS` pods to.
+
The following example shows autoscaling is enabled and all of the optional keys have values. We recommend that the minimum number of [.noloc]`CoreDNS` pods is always greater than 2 to provide resilience for the DNS service in the cluster.
+
[source,shell,subs="verbatim,attributes"]
----
aws eks update-addon --cluster-name my-cluster --addon-name coredns \
    --resolve-conflicts PRESERVE --configuration-values '{"autoScaling":{"enabled":true,"minReplicas":2,"maxReplicas":10}}'
----
. Check the status of the update to the add-on by running the following command:
+
[source,shell,subs="verbatim,attributes"]
----
aws eks describe-addon --cluster-name my-cluster --addon-name coredns \
----
+
If you see this line: `"status": "ACTIVE"`, then the rollout has completed and the add-on is using the new configuration in all of the [.noloc]`CoreDNS` pods. As you change the number of nodes and CPU cores of nodes in the cluster, Amazon EKS scales the number of replicas of the [.noloc]`CoreDNS` deployment.

====

[.topic]
[[coredns-metrics,coredns-metrics.title]]
=== Monitor [.noloc]`Kubernetes` DNS resolution with [.noloc]`CoreDNS` metrics

[abstract]
--
Learn how to collect [.noloc]`CoreDNS` metrics in Amazon EKS using Prometheus or CloudWatch Agent, enabling monitoring and observability for your [.noloc]`Kubernetes` DNS resolution.
--

[.noloc]`CoreDNS` as an EKS add-on exposes the metrics from [.noloc]`CoreDNS` on port `9153` in the Prometheus format in the `kube-dns` service. You can use Prometheus, the Amazon CloudWatch agent, or any other compatible system to scrape (collect) these metrics.

For an example _scrape configuration_ that is compatible with both Prometheus and the CloudWatch agent, see link:AmazonCloudWatch/latest/monitoring/ContainerInsights-Prometheus-Setup-configure.html[CloudWatch agent configuration for Prometheus,type="documentation"] in the _Amazon CloudWatch User Guide_.

[.topic]
[[managing-kube-proxy,managing-kube-proxy.title]]
== Manage `kube-proxy` in Amazon EKS clusters

[abstract]
--
Learn how to manage the `kube-proxy` add-on on your Amazon EKS cluster to manage network rules and enable network communication to your Pods.
--

[IMPORTANT]
====

We recommend adding the Amazon EKS type of the add-on to your cluster instead of using the self-managed type of the add-on. If you're not familiar with the difference between the types, see <<eks-add-ons>>. For more information about adding an Amazon EKS add-on to your cluster, see <<creating-an-add-on>>. If you're unable to use the Amazon EKS add-on, we encourage you to submit an issue about why you can't to the https://github.com/aws/containers-roadmap/issues[Containers roadmap GitHub repository].

====

The `kube-proxy` add-on is deployed on each Amazon EC2 node in your Amazon EKS cluster. It maintains network rules on your nodes and enables network communication to your [.noloc]`Pods`. The add-on isn't deployed to Fargate nodes in your cluster. For more information, see https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/[kube-proxy] in the [.noloc]`Kubernetes` documentation.

[[kube-proxy-versions,kube-proxy-versions.title]]
=== `kube-proxy` versions

The following table lists the latest version of the Amazon EKS add-on type for each [.noloc]`Kubernetes` version.

[options="header"]
|===
| Kubernetes version | `kube-proxy` version
| 1.31 | v1.31.2-eksbuild.3
| 1.30 | v1.30.6-eksbuild.3
| 1.29 | v1.29.10-eksbuild.3
| 1.28 | v1.28.15-eksbuild.4
| 1.27 | v1.27.16-eksbuild.14
| 1.26 | v1.26.15-eksbuild.19
| 1.25 | v1.25.16-eksbuild.22
| 1.24 | v1.24.17-eksbuild.19
| 1.23 | v1.23.17-eksbuild.20
|===

[IMPORTANT]
====

An earlier version of the documentation was incorrect. `kube-proxy` versions `v1.28.5`, `v1.27.9`, and `v1.26.12` aren't available.

If you're self-managing this add-on, the versions in the table might not be the same as the available self-managed versions.

====

[[managing-kube-proxy-images,managing-kube-proxy-images.title]]
=== `kube-proxy` container image migration

There are two types of the `kube-proxy` container image available for each Amazon EKS cluster version:



* *Default*  This image type is based on a Debian-based Docker image that is maintained by the [.noloc]`Kubernetes` upstream community.
* *Minimal*  This image type is based on a https://gallery.ecr.aws/eks-distro-build-tooling/eks-distro-minimal-base-iptables[minimal base image] maintained by Amazon EKS Distro, which contains minimal packages and doesn't have shells. For more information, see https://distro.eks.amazonaws.com/[Amazon EKS Distro].

The following table lists the latest available self-managed `kube-proxy` container image version for each Amazon EKS cluster version.

// GDC Update

[options="header"]
|===
| Version | kube-proxy (default type) | kube-proxy (minimal type)
| 1.31 | Only minimal type is available | v1.31.2-minimal-eksbuild.3
| 1.30 | Only minimal type is available | v1.30.6-minimal-eksbuild.3
| 1.29 | Only minimal type is available | v1.29.10-minimal-eksbuild.3
| 1.28 | Only minimal type is available | v1.28.15-minimal-eksbuild.4
| 1.27 | Only minimal type is available | v1.27.16-minimal-eksbuild.14
| 1.26 | Only minimal type is available | v1.26.15-minimal-eksbuild.19
| 1.25 | Only minimal type is available | v1.25.16-minimal-eksbuild.22
| 1.24 | v1.24.10-eksbuild.2 | v1.24.17-minimal-eksbuild.19
| 1.23 | v1.23.16-eksbuild.2 | v1.23.17-minimal-eksbuild.20
|===


[IMPORTANT]
====


* The default image type isn't available for [.noloc]`Kubernetes` version `1.25` and later. You must use the minimal image type.
* When you  <<updating-an-add-on,update an Amazon EKS add-on type>>, you specify a valid Amazon EKS add-on version, which might not be a version listed in this table. This is because <<add-ons-kube-proxy,Amazon EKS add-on>> versions don't always match container image versions specified when updating the self-managed type of this add-on. When you update the self-managed type of this add-on, you specify a valid container image version listed in this table. 

====

[.topic]
[[kube-proxy-add-on-self-managed-update,kube-proxy-add-on-self-managed-update.title]]
=== Update the Kubernetes `kube-proxy` self-managed add-on

[IMPORTANT]
====

We recommend adding the Amazon EKS type of the add-on to your cluster instead of using the self-managed type of the add-on. If you're not familiar with the difference between the types, see <<eks-add-ons>>. For more information about adding an Amazon EKS add-on to your cluster, see <<creating-an-add-on>>. If you're unable to use the Amazon EKS add-on, we encourage you to submit an issue about why you can't to the https://github.com/aws/containers-roadmap/issues[Containers roadmap GitHub repository].

====

[[managing-kube-proxy-prereqs,managing-kube-proxy-prereqs.title]]
==== Prerequisites

* An existing Amazon EKS cluster. To deploy one, see <<getting-started>>.


[[managing-kube-proxy-considerations,managing-kube-proxy-considerations.title]]
==== Considerations

* `Kube-proxy` on an Amazon EKS cluster has the same https://kubernetes.io/releases/version-skew-policy/#kube-proxy[compatibility and skew policy as Kubernetes]. Learn how to <<addon-compat,Verifying Amazon EKS add-on version compatibility with a cluster>>.
. Confirm that you have the self-managed type of the add-on installed on your cluster. Replace [.replaceable]`my-cluster` with the name of your cluster.
+
[source,bash,subs="verbatim,attributes"]
----
aws eks describe-addon --cluster-name my-cluster --addon-name kube-proxy --query addon.addonVersion --output text
----
+
If an error message is returned, you have the self-managed type of the add-on installed on your cluster. The remaining steps in this topic are for updating the self-managed type of the add-on. If a version number is returned, you have the Amazon EKS type of the add-on installed on your cluster. To update it, use the procedure in  <<updating-an-add-on,Updating an Amazon EKS add-on>>, rather than using the procedure in this topic. If you're not familiar with the differences between the add-on types, see <<eks-add-ons>>.
. See which version of the container image is currently installed on your cluster.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl describe daemonset kube-proxy -n kube-system | grep Image
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
Image:    602401143452.dkr.ecr.region-code.amazonaws.com/eks/kube-proxy:v1.29.1-eksbuild.2
----
+
In the example output, [.replaceable]`v1.29.1-eksbuild.2` is the version installed on the cluster.
. Update the `kube-proxy` add-on by replacing [.replaceable]`602401143452` and [.replaceable]`region-code` with the values from your output in the previous step. Replace [.replaceable]`v1.30.6-eksbuild.3` with the `kube-proxy` version listed in the <<managing-kube-proxy-images,Latest available self-managed kube-proxy container image version for each Amazon EKS cluster version>> table.
+
IMPORTANT: The manifests for each image type are different and not compatible between the _default_ or _minimal_ image types. You must use the same image type as the previous image, so that the entrypoint and arguments match.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl set image daemonset.apps/kube-proxy -n kube-system kube-proxy=602401143452.dkr.ecr.region-code.amazonaws.com/eks/kube-proxy:v1.30.6-eksbuild.3
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
daemonset.apps/kube-proxy image updated
----
. Confirm that the new version is now installed on your cluster.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl describe daemonset kube-proxy -n kube-system | grep Image | cut -d ":" -f 3
----
+
An example output is as follows.
+
[source,bash,subs="verbatim,attributes"]
----
v1.30.0-eksbuild.3
----
. If you're using `x86` and `Arm` nodes in the same cluster and your cluster was deployed before August 17, 2020. Then, edit your `kube-proxy` manifest to include a node selector for multiple hardware architectures with the following command. This is a one-time operation. After you've added the selector to your manifest, you don't need to add it each time you update the add-on. If your cluster was deployed on or after August 17, 2020, then `kube-proxy` is already multi-architecture capable.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl edit -n kube-system daemonset/kube-proxy
----
+
Add the following node selector to the file in the editor and then save the file. For an example of where to include this text in the editor, see the https://github.com/aws/amazon-vpc-cni-k8s/blob/release-1.11/config/master/aws-k8s-cni.yaml#L265-#L269[CNI manifest] file on [.noloc]`GitHub`. This enables [.noloc]`Kubernetes` to pull the correct hardware image based on the node's hardware architecture.
+
[source,yaml,subs="verbatim,attributes"]
----
- key: "kubernetes.io/arch"
  operator: In
  values:
  - amd64
  - arm64
----
. If your cluster was originally created with [.noloc]`Kubernetes` version `1.14` or later, then you can skip this step because `kube-proxy` already includes this `Affinity Rule`. If you originally created an Amazon EKS cluster with [.noloc]`Kubernetes` version `1.13` or earlier and intend to use Fargate nodes in your cluster, then edit your `kube-proxy` manifest to include a `NodeAffinity` rule to prevent `kube-proxy` [.noloc]`Pods` from scheduling on Fargate nodes. This is a one-time edit. Once you've added the `Affinity Rule` to your manifest, you don't need to add it each time that you update the add-on. Edit your `kube-proxy` [.noloc]`DaemonSet`.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl edit -n kube-system daemonset/kube-proxy
----
+
Add the following `Affinity Rule` to the [.noloc]`DaemonSet`spec`` section of the file in the editor and then save the file. For an example of where to include this text in the editor, see the https://github.com/aws/amazon-vpc-cni-k8s/blob/release-1.11/config/master/aws-k8s-cni.yaml#L270-#L273[CNI manifest] file on [.noloc]`GitHub`.
+
[source,yaml,subs="verbatim,attributes"]
----
- key: eks.amazonaws.com/compute-type
  operator: NotIn
  values:
  - fargate
----
