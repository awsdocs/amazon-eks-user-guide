//!!NODE_ROOT <chapter>
include::attributes.txt[]
[.topic]
[[quickstart,quickstart.title]]
= Quickstart: Deploy a web app and store data
:doctype: book
:sectnums:
:toc: left
:icons: font
:experimental:
:idprefix:
:idseparator: -
:sourcedir: .
:info_doctype: chapter
:info_title: Quickstart: Deploy a web app and store data
:info_titleabbrev: Quickstart
:keywords: quickstart, web, cluster
:info_abstract: Deploy a game application and persist its data on Amazon EKS

[abstract]
--
Deploy a game application and persist its data on Amazon EKS
--

This quickstart tutorial shows the steps to deploy the 2048 game sample application and persist its data on an Amazon EKS cluster using https://eksctl.io/[eksctl]. eksctl is an infrastructure-as-code utility leveraging link:cloudformation/[{aws} CloudFormation,type="marketing"], allowing you to set up a fully-functional cluster, complete with all the essential components. These components include an Amazon VPC and an IAM role tailored to provide permissions to the {aws} services we've defined. As we progress, we'll walk you through the cluster setup process, incorporating <<eks-add-ons,Amazon EKS add-ons>> to power your cluster with operational capabilities. Finally, you'll deploy a sample workload with the custom annotations required to fully integrate with {aws} services.

[[quickstart-in-tutorial,quickstart-in-tutorial.title]]
== In this tutorial

Using the `eksctl` cluster template that follows, you'll build an Amazon EKS cluster with managed node groups. It configures the following components:



*VPC Configuration*::
When using the eksctl cluster template that follows, eksctl automatically creates an IPv4 Virtual Private Cloud (VPC) for the cluster. By default, eksctl configures a VPC that addresses all <<network-reqs,networking requirements>>, in addition to creating both public and private endpoints.


*Instance type*::
Utilize the <<choosing-instance-type,t3.medium instance type>>. This instance type offers a well-balanced combination of compute, memory, and network resources--ideal for applications with moderate CPU usage that may experience occasional spikes in demand.


*Authentication*::
Establish the IRSA mappings to facilitate communication between Kubernetes pods and {aws} services. The template is configured to set up an <<enable-iam-roles-for-service-accounts,OpenID Connect (OIDC) endpoint>> for authentication and authorization. It also establishes a service account for the https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.8/[{aws} Load Balancer Controller (LBC)], a controller responsible for exposing applications and managing traffic.


*Data Persistence*::
Integrate the <<managing-ebs-csi,{aws} EBS CSI Driver>> managed add-on to ensure the persistence of application data, even in scenarios involving pod restarts or failures. The template is configured to install the add-on and establish a service account


*External App Access*::
Set up and integrate with the {aws} Load Balancer Controller (LBC) add-on to expose the https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.6.0/docs/examples/2048/2048_full.yaml[2048 game application], using the LBC to dynamically provision an Application Load Balancer (ALB).


[[quickstart-prereqs,quickstart-prereqs.title]]
== Prerequisites

Before you begin, ensure you have the following prerequisites set up to use Amazon EKS.

* <<setting-up,Set up to use Amazon EKS>>
* https://helm.sh/docs/intro/install/[Install Helm]


[[quickstart-config-cluster,quickstart-config-cluster.title]]
== Step 1: Configure the cluster

In this section, you'll create a managed node group-based cluster using link:ec2/instance-types/[t3.medium,type="marketing"] instances containing two nodes. The configuration includes a service account for https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.8/deploy/installation/[{aws} Load Balancer Controller (LBC)] add-on, and installation of the latest version of the <<managing-ebs-csi.title,{aws} Amazon EBS CSI driver>>. For all available `eksctl` add-ons, see https://eksctl.io/usage/addons/#discovering-addons[Discovering addons] in `eksctl` documentation.

. Create a `cluster-config.yaml` file and paste the following contents into it. Replace [.replaceable]`region-code` with a valid Region, such as `us-east-1`
+
Example output is as follows:
+
[source,yaml,subs="verbatim,attributes,quotes"]
----
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: web-quickstart
  region: [.replaceable]`region-code`

managedNodeGroups:
  - name: eks-mng
    instanceType: t3.medium
    desiredCapacity: 2

iam:
  withOIDC: true
  serviceAccounts:
  - metadata:
      name: aws-load-balancer-controller
      namespace: kube-system
    wellKnownPolicies:
      awsLoadBalancerController: true

addons:
  - name: aws-ebs-csi-driver
    wellKnownPolicies: # Adds an IAM service account
      ebsCSIController: true
      
cloudWatch:
 clusterLogging:
   enableTypes: ["*"]
   logRetentionInDays: 30
----


[[quickstart-create-cluster,quickstart-create-cluster.title]]
== Step 2: Create the cluster

Now, we're ready to create our Amazon EKS cluster. This process takes several minutes to complete. If you'd like to monitor the status, see the link:cloudformation[{aws} CloudFormation,type="console"] console.

. Create the Amazon EKS cluster and specify the cluster-config.yaml.
+
[source,bash,subs="verbatim,attributes,quotes"]
----
eksctl create cluster -f cluster-config.yaml
----
+
NOTE: If you receive an `Error: checking STS access` in the response, make sure that you're using the correct user identity for the current shell session. You may also need to specify a named profile (for example, `--profile clusteradmin`) or get a new security token for the {aws} CLI.
+
Upon completion, you should see the following response output:
+
[source,bash,subs="verbatim,attributes,quotes"]
----
2024-07-04 21:47:53 [✔]  EKS cluster "web-quickstart" in "[.replaceable]`region-code`" region is ready
----


[[quickstart-lbc,quickstart-lbc.title]]
== Step 3: Set up external access to applications using the [.noloc]`{aws} Load Balancer Controller` (LBC)

With the cluster operational, our next step is making its containerized applications accessible externally. This is accomplished by deploying an link:elasticloadbalancing/latest/application/introduction.html[Application Load Balancer (ALB),type="documentation"] to direct traffic outside the cluster to our services, in other words, our applications. When we created our cluster, we established an link:emr/latest/EMR-on-EKS-DevelopmentGuide/setting-up-enable-IAM.html[IAM Roles for Service Accounts (IRSA),type="documentation"] for the https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.8/[Load Balancer Controller (LBC)] with the permissions needed to dynamically create ALBs, facilitating external traffic routing to our Kubernetes services. In this section, we'll set up the {aws} LBC on our cluster.

*To configure environment variables*

. Set the `CLUSTER_REGION` environment variable for your Amazon EKS cluster. Replace the sample value for [.replaceable]`region-code`.
+
[source,bash,subs="verbatim,attributes,quotes"]
----
export CLUSTER_REGION=[.replaceable]`region-code`
----
. Set the `CLUSTER_VPC` environment variable for your Amazon EKS cluster.
+
[source,bash,subs="verbatim,attributes,quotes"]
----
export CLUSTER_VPC=$(aws eks describe-cluster --name web-quickstart --region $CLUSTER_REGION --query "cluster.resourcesVpcConfig.vpcId" --output text)
----

*To install the {aws} Load Balancer Controller (LBC)*

The {aws} Load Balancer Controller (LBC) leverages Custom Resource Definitions (CRDs) in Kubernetes to manage {aws} Elastic Load Balancers (ELBs). These CRDs define custom resources such as load balancers and TargetGroupBindings, enabling the [.noloc]`Kubernetes` cluster to recognize and manage them.

. Use https://helm.sh/docs/intro/install/[Helm] to add the Amazon EKS chart repository to Helm.
+
[source,bash,subs="verbatim,attributes,quotes"]
----
helm repo add eks https://aws.github.io/eks-charts
----
. Update the repositories to ensure Helm is aware of the latest versions of the charts:
+
[source,bash,subs="verbatim,attributes,quotes"]
----
helm repo update eks
----
. Run the following https://helm.sh/docs/intro/install/[Helm] command to simultaneously install the Custom Resource Definitions (CRDs) and the main controller for the {aws} Load Balancer Controller ({aws} LBC). To skip the CRD installation, pass the `--skip-crds` flag, which might be useful if the CRDs are already installed, if specific version compatibility is required, or in environments with strict access control and customization needs.
+
[source,bash,subs="verbatim,attributes,quotes"]
----
helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
    --namespace kube-system \
    --set clusterName=web-quickstart \
    --set serviceAccount.create=false \
    --set region=${CLUSTER_REGION} \
    --set vpcId=${CLUSTER_VPC} \
    --set serviceAccount.name=aws-load-balancer-controller
----
+
You should see the following response output:
+
[source,bash,subs="verbatim,attributes,quotes"]
----
NAME: aws-load-balancer-controller
LAST DEPLOYED: Wed July 3 19:43:12 2024
NAMESPACE: kube-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
{aws} Load Balancer controller installed!
----


[[quickstart-deploy-game,quickstart-deploy-game.title]]
== Step 4: Deploy the 2048 game sample application

Now that the load balancer is set up, it's time to enable external access for containerized applications in the cluster. In this section, we walk you through the steps to deploy the popular "`2048 game`" as a sample application within the cluster. The provided manifest includes custom annotations for the Application Load Balancer (ALB), specifically the https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.8/guide/ingress/ingress_class/#specscheme['scheme' annotation] and https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.8/guide/ingress/annotations/#target-type['target-type' annotation]. These annotations integrate with and instruct the {aws} Load Balancer Controller (LBC) to handle incoming HTTP traffic as "internet-facing" and route it to the appropriate service in the 'game-2048' namespace using the target type "ip". For more annotations, see https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.8/guide/ingress/annotations/[Annotations] in the {aws} LBC documentation.

. Create a Kubernetes namespace called `game-2048` with the `--save-config` flag.
+
[source,bash,subs="verbatim,attributes,quotes"]
----
kubectl create namespace game-2048 --save-config
----
+
You should see the following response output:
+
[source,bash,subs="verbatim,attributes,quotes"]
----
namespace/game-2048 created
----
. Deploy the https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.8.0/docs/examples/2048/2048_full.yaml[2048 Game Sample application].
+
[source,bash,subs="verbatim,attributes,quotes"]
----
kubectl apply -n game-2048 -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.8.0/docs/examples/2048/2048_full.yaml
----
+
This manifest sets up a Kubernetes Deployment, Service, and Ingress for the `game-2048` namespace, creating the necessary resources to deploy and expose the `game-2048` application within the cluster. It includes the creation of a service named `service-2048` that exposes the deployment on port `80`, and an Ingress resource named `ingress-2048` that defines routing rules for incoming HTTP traffic and annotations for an internet-facing Application Load Balancer (ALB). You should see the following response output: 
+
[source,bash,subs="verbatim,attributes,quotes"]
----
namespace/game-2048 configured
deployment.apps/deployment-2048 created
service/service-2048 created
ingress.networking.k8s.io/ingress-2048 created
----
. Run the following command to get the Ingress resource for the `game-2048` namespace.
+
[source,bash,subs="verbatim,attributes,quotes"]
----
kubectl get ingress -n game-2048
----
+
You should see the following response output:
+
[source,bash,subs="verbatim,attributes,quotes"]
----
NAME           CLASS   HOSTS   ADDRESS                                                                    PORTS   AGE
ingress-2048   alb     *       k8s-game2048-ingress2-eb379a0f83-378466616.[.replaceable]`region-code`.elb.amazonaws.com   80      31s
----
+
You'll need to wait several minutes for the Application Load Balancer (ALB) to provision before you begin the following steps.
. Open a web browser and enter the `ADDRESS` from the previous step to access the web application. For example:
+
[source,bash,subs="verbatim,attributes,quotes"]
----
k8s-game2048-ingress2-eb379a0f83-378466616.[.replaceable]`region-code`.elb.amazonaws.com
----
+
You should see the 2048 game in your browser. Play!
+
image::images/quick2048.png[Play the 2048 game,scaledwidth=25%]


[[quickstart-persist-data,quickstart-persist-data.title]]
== Step 5: Persist Data using the Amazon EBS CSI Driver nodes

Now that the 2048 game is up and running on your Amazon EKS cluster, it's time to ensure that your game data is safely persisted using the  <<ebs-csi,Amazon EBS CSI Driver>> managed add-on. This add-on was installed on our cluster during the creation process. This integration is essential for preserving game progress and data even as Kubernetes pods or nodes are restarted or replaced.

. Create a https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/examples/kubernetes/dynamic-provisioning/manifests/storageclass.yaml[Storage Class] for the EBS CSI Driver:
+
[source,bash,subs="verbatim,attributes,quotes"]
----
kubectl apply -n game-2048 -f https://raw.githubusercontent.com/kubernetes-sigs/aws-ebs-csi-driver/master/examples/kubernetes/dynamic-provisioning/manifests/storageclass.yaml
----
. Create a Persistent Volume Claim (PVC) to request storage for your game data. Create a file named `ebs-pvc.yaml` and add the following content to it:
+
[source,bash,subs="verbatim,attributes,quotes"]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: game-data-pvc
  namespace: game-2048
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: ebs-sc
----
. Apply the PVC to your cluster:
+
[source,bash,subs="verbatim,attributes,quotes"]
----
kubectl apply -f ebs-pvc.yaml
----
+
You should see the following response output:
+
[source,bash,subs="verbatim,attributes,quotes"]
----
persistentvolumeclaim/game-data-pvc created
----
. Now, you need to update your 2048 game deployment to use this PVC for storing data. The following deployment is configured to use the PVC for storing game data. Create a file named `ebs-deployment.yaml` and add the following contents to it:
+
[source,bash,subs="verbatim,attributes,quotes"]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: game-2048
  name: deployment-2048
spec:
  replicas: 3  # Adjust the number of replicas as needed
  selector:
    matchLabels:
      app.kubernetes.io/name: app-2048
  template:
    metadata:
      labels:
        app.kubernetes.io/name: app-2048
    spec:
      containers:
        - name: app-2048
          image: public.ecr.aws/l6m2t8p7/docker-2048:latest
          imagePullPolicy: Always
          ports:
            - containerPort: 80
          volumeMounts:
            - name: game-data
              mountPath: /var/lib/2048
      volumes:
        - name: game-data
          persistentVolumeClaim:
            claimName: game-data-pvc
----
. Apply the updated deployment:
+
[source,bash,subs="verbatim,attributes,quotes"]
----
kubectl apply -f ebs-deployment.yaml
----
+
You should see the following response output:
+
[source,bash,subs="verbatim,attributes,quotes"]
----
deployment.apps/deployment-2048 configured
----

With these steps, your 2048 game on Amazon EKS is now set up to persist data using the Amazon EBS CSI Driver. This ensures that your game progress and data are safe even in the event of pod or node failures. 

If you liked this tutorial, let us know by providing feedback so we're able to provide you with more use case-specific quickstart tutorials like this one.

[[quickstart-delete-cluster,quickstart-delete-cluster.title]]
== Step 6: Delete your cluster and nodes

After you've finished with the cluster and nodes that you created for this tutorial, you should clean up by deleting the cluster and nodes with the following command. If you want to do more with this cluster before you clean up, see <<quickstart-next-steps,Next steps>>.

[source,bash,subs="verbatim,attributes,quotes"]
----
eksctl delete cluster -f ./cluster-config.yaml
----

Upon completion, you should see the following response output:

[source,bash,subs="verbatim,attributes,quotes"]
----
2024-07-05 17:26:44 [✔] all cluster resources were deleted
----


[[quickstart-next-steps,quickstart-next-steps.title]]
== Next steps

The following documetation topics help you to extend the functionality of your cluster:



* The  link:IAM/latest/UserGuide/id_roles.html#iam-term-principal[IAM principal,type="documentation"] that created the cluster is the only principal that can make calls to the [.noloc]`Kubernetes` API server with `kubectl` or the {aws-management-console}. If you want other IAM principals to have access to your cluster, then you need to add them. For more information, see <<grant-k8s-access>> and <<view-kubernetes-resources-permissions>>.
* Before deploying a cluster for production use, we recommend familiarizing yourself with all of the settings for <<create-cluster,clusters>> and <<eks-compute,nodes>>. Some settings (such as enabling SSH access to Amazon EC2 nodes) must be made when the cluster is created.
* To increase security for your cluster, <<cni-iam-role,configure the Amazon VPC Container Networking Interface plugin to use IAM roles for service accounts>>.

To explore ways to create different types of clusters:



* https://community.aws/tags/eks-cluster-setup[EKS Cluster Setup on {aws} Community]
