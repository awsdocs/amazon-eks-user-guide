//!!NODE_ROOT <section>
include::../attributes.txt[]

[.topic]
[[kubernetes-concepts,kubernetes-concepts.title]]
= [.noloc]`Kubernetes` concepts
:info_doctype: section
:info_title: Kubernetes concepts
:info_titleabbrev: Kubernetes concepts
:keywords: Amazon Elastic Kubernetes Service, Amazon EKS, architecture, control plane, nodes, data plane
:info_abstract: Learn core Kubernetes concepts and how they relate to deploying workloads, managing clusters, and working with control planes, nodes, Pods, containers, and networking on Amazon EKS.

[abstract]
--
Learn core [.noloc]`Kubernetes` concepts and how they relate to deploying workloads, managing clusters, and working with control planes, nodes, Pods, containers, and networking on Amazon EKS.
--

Amazon Elastic Kubernetes Service (Amazon EKS) is an {aws} managed service based on the open source https://kubernetes.io/[Kubernetes] project. While there are things you need to know about how the Amazon EKS service integrates with {aws} Cloud (particularly when you first create an Amazon EKS cluster), once it's up and running, you use your Amazon EKS cluster in much that same way as you would any other [.noloc]`Kubernetes` cluster. So to begin managing [.noloc]`Kubernetes` clusters and deploying workloads, you need at least a basic understanding of [.noloc]`Kubernetes` concepts.  

This page divides [.noloc]`Kubernetes` concepts into three sections: <<why-kubernetes>>, <<concepts-clusters>>, and <<workloads>>. The first section describes the value of running a [.noloc]`Kubernetes` service, in particular as a managed service like Amazon EKS. The Workloads section covers how [.noloc]`Kubernetes` applications are built, stored, run, and managed. The Clusters section lays out the different components that make up [.noloc]`Kubernetes` clusters and what your responsibilities are for creating and maintaining [.noloc]`Kubernetes` clusters.  

[.topiclist]
[[Topic List]]

As you go through this content, links will lead you to further descriptions of [.noloc]`Kubernetes` concepts in both Amazon EKS and [.noloc]`Kubernetes` documentation, in case you want to take deep dives into any of the topics we cover here. For details about how Amazon EKS implements [.noloc]`Kubernetes` control plane and compute features, see <<eks-architecture>>. 

[[why-kubernetes,why-kubernetes.title]]
== Why [.noloc]`Kubernetes`?

[.noloc]`Kubernetes` was designed to improve availability and scalability when running mission-critical, production-quality containerized applications. Rather than just running [.noloc]`Kubernetes` on a single machine (although that is possible), [.noloc]`Kubernetes` achieves those goals by allowing you to run applications across sets of computers that can expand or contract to meet demand. [.noloc]`Kubernetes` includes features that make it easier for you to:  



* Deploy applications on multiple machines (using containers deployed in Pods) 
* Monitor container health and restart failed containers 
* Scale containers up and down based on load 
* Update containers with new versions 
* Allocate resources between containers 
* Balance traffic across machines 

Having [.noloc]`Kubernetes` automate these types of complex tasks allows an application developer to focus on building and improving their application workloads, rather than worrying about infrastructure. The developer typically creates configuration files, formatted as YAML files, that describe the desired state of the application. This could include which containers to run, resource limits, number of Pod replicas, CPU/memory allocation, affinity rules, and more.  

[[attributes-of-kubernetes,attributes-of-kubernetes.title]]
=== Attributes of [.noloc]`Kubernetes`

To achieve its goals, [.noloc]`Kubernetes` has the following attributes:  



* *Containerized* -- [.noloc]`Kubernetes` is a container orchestration tool. To use [.noloc]`Kubernetes`, you must first have your applications containerized. Depending on the type of application, this could be as a set of  _microservices,_ as batch jobs or in other forms. Then, your applications can take advantage of a [.noloc]`Kubernetes` workflow that encompasses a huge ecosystem of tools, where containers can be stored as https://kubernetes.io/docs/concepts/containers/images/#multi-architecture-images-with-image-indexes[images in a container registry], deployed to a [.noloc]`Kubernetes` https://kubernetes.io/docs/concepts/architecture/[cluster], and run on an available https://kubernetes.io/docs/concepts/architecture/nodes/[node]. You can build and test individual containers on your local computer with [.noloc]`Docker` or another https://kubernetes.io/docs/setup/production-environment/container-runtimes/[container runtime], before deploying them to your [.noloc]`Kubernetes` cluster.  
* *Scalable* -- If the demand for your applications exceeds the capacity of the running instances of those applications, [.noloc]`Kubernetes` is able to scale up. As needed, [.noloc]`Kubernetes` can tell if applications require more CPU or memory and respond by either automatically expanding available capacity or using more of existing capacity. Scaling can be done at the Pod level, if there is enough compute available to just run more instances of the application (https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/[horizontal Pod autoscaling]), or at the node level, if more nodes need to be brought up to handle the increased capacity (https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler[Cluster Autoscaler] or https://karpenter.sh/[Karpenter]). As capacity is no longer needed, these services can delete unnecessary Pods and shut down unneeded nodes. 
* *Available* -- If an application or node becomes unhealthy or unavailable, [.noloc]`Kubernetes` can move running workloads to another available node. You can force the issue by simply deleting a running instance of a workload or node that's running your workloads. The bottom line here is that workloads can be brought up in other locations if they can no longer run where they are.  
* *Declarative* -- [.noloc]`Kubernetes` uses active reconciliation to constantly check that the state that you declare for your cluster matches the actual state. By applying https://kubernetes.io/docs/concepts/overview/working-with-objects/[Kubernetes objects] to a cluster, typically through YAML-formatted configuration files, you can, for example, ask to start up the workloads you want to run on your cluster. You can later change the configurations to do something like use a later version of a container or allocate more memory. [.noloc]`Kubernetes` will do what it needs to do to establish the desired state. This can include bringing nodes up or down, stopping and restarting workloads, or pulling updated containers.  
* *Composable* -- Because an application typically consists of multiple components, you want to be able to manage a set of these components (often represented by multiple containers) together. While [.noloc]`Docker` Compose offers a way to do this directly with [.noloc]`Docker`, the [.noloc]`Kubernetes` http://kompose.io/[Kompose] command can help you do that with [.noloc]`Kubernetes`. See https://kubernetes.io/docs/tasks/configure-pod-container/translate-compose-kubernetes/[Translate a Docker Compose File to Kubernetes Resources] for an example of how to do this. 
* *Extensible* -- Unlike proprietary software, the open source [.noloc]`Kubernetes` project is designed to be open to you extending [.noloc]`Kubernetes` any way that you like to meet your needs. APIs and configuration files are open to direct modifications. Third-parties are encouraged to write their own https://kubernetes.io/docs/concepts/architecture/controller/[Controllers], to extend both infrastructure and end-user [.noloc]`Kubernetes` featues. https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/[Webhooks] let you set up cluster rules to enforce policies and adapt to changing conditions. For more ideas on how to extend [.noloc]`Kubernetes` clusters, see https://kubernetes.io/docs/concepts/extend-kubernetes/[Extending Kubernetes]. 
* *Portable* -- Many organizations have standardized their operations on [.noloc]`Kubernetes` because it allows them to manage all of their application needs in the same way. Developers can use the same pipelines to build and store containerized applications. Those applications can then be deployed to [.noloc]`Kubernetes` clusters running on-premises, in clouds, on point-of-sales terminals in restaurants, or on IOT devices dispersed across company's remote sites. Its open source nature makes it possible for people to develop these special [.noloc]`Kubernetes` distributions, along will tools needed to manage them.  


[[managing-kubernetes,managing-kubernetes.title]]
=== Managing [.noloc]`Kubernetes`

[.noloc]`Kubernetes` source code is freely available, so with your own equipment you could install and manage [.noloc]`Kubernetes` yourself. However, self-managing [.noloc]`Kubernetes` requires deep operational expertise and takes time and effort to maintain. For those reasons, most people deploying production workloads choose a cloud provider (such as Amazon EKS) or on-premises provider (such as Amazon EKS Anywhere) with its own tested [.noloc]`Kubernetes` distribution and support of [.noloc]`Kubernetes` experts. This allows you to offload much of the undifferentiated heavy lifting needed to maintain your clusters, including:  



* *Hardware* -- If you don't have hardware available to run [.noloc]`Kubernetes` per your requirements, a cloud provider such as {aws} Amazon EKS can save you on upfront costs. With Amazon EKS, this means that you can consume the best cloud resources offered by {aws}, including computer instances (Amazon Elastic Compute Cloud), your own private environment (Amazon VPC), central identity and permissions management (IAM), and storage (Amazon EBS). {aws} manages the computers, networks, data centers, and all the other physical components needed to run [.noloc]`Kubernetes`. Likewise, you don't have to plan your datacenter to handle the maximum capacity on your highest-demand days. For Amazon EKS Anywhere, or other on premises [.noloc]`Kubernetes` clusters, you are responsible for managing the infrastructure used in your [.noloc]`Kubernetes` deployments, but you can still rely on {aws} to help you keep [.noloc]`Kubernetes` up to date.  
* *Control plane management* -- Amazon EKS manages the security and availability of the {aws}-hosted [.noloc]`Kubernetes` control plane, which is responsible for scheduling containers, managing the availability of applications, and other key tasks, so you can focus on your application workloads. If your cluster breaks, {aws} should have the means to restore your cluster to a running state. For Amazon EKS Anywhere, you would manage the control plane yourself.  
* *Tested upgrades* -- When you upgrade your clusters, you can rely on Amazon EKS or Amazon EKS Anywhere to provide tested versions of their [.noloc]`Kubernetes` distributions.  
* *Add-ons* -- There are hundreds of projects built to extend and work with [.noloc]`Kubernetes` that you can add to your cluster's infrastructure or use to aid the running of your workloads. Instead of building and managing those add-ons yourself, {aws} provides  <<eks-add-ons,Amazon EKS Add-ons>> that you can use with your clusters. Amazon EKS Anywhere provides https://anywhere.eks.amazonaws.com/docs/packages/[Curated Packages] that include builds of many popular open source projects. So you don't have to build the software yourself or manage critical security patches, bug fixes, or upgrades. Likewise, if the defaults meet your needs, it's typical for very little configuration of those add-ons to be needed. See <<extend-clusters>> for details on extending your cluster with add-ons. 


[[kubernetes-in-action,kubernetes-in-action.title]]
=== [.noloc]`Kubernetes` in action

The following diagram shows key activities you would do as a [.noloc]`Kubernetes` Admin or Application Developer to create and use a [.noloc]`Kubernetes` cluster. In the process, it illustrates how [.noloc]`Kubernetes` components interact with each other, using the {aws} cloud as the example of the underlying cloud provider.  



image::images/k8sinaction.png[A Kubernetes cluster in action.,scaledwidth=100%]

A [.noloc]`Kubernetes` Admin creates the [.noloc]`Kubernetes` cluster using a tool specific to the type of provider on which the cluster will be built. This example uses the {aws} cloud as the provider, which offers the managed [.noloc]`Kubernetes` service called Amazon EKS. The managed service automatically allocates the resources needed to create the cluster, including creating two new Virtual Private Clouds (Amazon VPCs) for the cluster, setting up networking, mapping [.noloc]`Kubernetes` permissions into those to manage assets in the cloud, seeing that the control plane services have places to run, and allocating zero or more Amazon EC2 instances as [.noloc]`Kubernetes` nodes for running workloads. {aws} manages one Amazon VPC itself for the control plane, while the other Amazon VPC contains the customer nodes that run workloads.  

Many of the [.noloc]`Kubernetes` Admin's tasks going forward are done using [.noloc]`Kubernetes` tools such as `kubectl`. That tool makes requests for services directly to the cluster's control plane. The ways that queries and changes are made to the cluster are then very similar to the ways you would do them on any [.noloc]`Kubernetes` cluster.  

An application developer wanting to deploy workloads to this cluster can perform several tasks. The developer needs to build the application into one or more container images, then push those images to a container registry that is accessible to the [.noloc]`Kubernetes` cluster. {aws} offers the Amazon Elastic Container Registry (Amazon ECR) for that purpose.  

To run the application, the developer can create YAML-formatted configuration files that tell the cluster how to run the application, including which containers to pull from the registry and how to wrap those containers in Pods. The control plane (scheduler) schedules the containers to one or more nodes and the container runtime on each node actually pulls and runs the needed containers. The developer can also set up an application load balancer to balance traffic to available containers running on each node and expose the application so it is available on a public network to the outside world. With that all done, someone wanting to use the application can connect to the application endpoint to access it. 

The following sections go through details of each of these features, from the perspective of [.noloc]`Kubernetes` Clusters and Workloads.  

[[concepts-clusters,concepts-clusters.title]]
== Clusters

If your job is to start and manage [.noloc]`Kubernetes` clusters, you should know how [.noloc]`Kubernetes` clusters are created, enhanced, managed, and deleted. You should also know what the components are that make up a cluster and what you need to do to maintain those components.  

Tools for managing clusters handle the overlap between the [.noloc]`Kubernetes` services and the underlying hardware provider. For that reason, automation of these tasks tend to be done by the [.noloc]`Kubernetes` provider (such as Amazon EKS or Amazon EKS Anywhere) using tools that are specific to the provider. For example, to start an Amazon EKS cluster you can use `eksctl create cluster`, while for Amazon EKS Anywhere you can use `eksctl anywhere create cluster`. Note that while these commands create a [.noloc]`Kubernetes` cluster, they are specific to the provider and are not part of the [.noloc]`Kubernetes` project itself.  

[[cluster-creation-and-management-tools,cluster-creation-and-management-tools.title]]
=== Cluster creation and management tools

The [.noloc]`Kubernetes` project offers tools for creating a [.noloc]`Kubernetes` cluster manually. So if you want to install [.noloc]`Kubernetes` on a single machine, or run the control plane on a machine and add nodes manually, you can use CLI tools like https://kind.sigs.k8s.io/[kind], https://kubernetes.io/docs/tutorials/hello-minikube/[minikube], or https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/[kubeadm]that are listed under [.noloc]`Kubernetes` https://kubernetes.io/docs/tasks/tools/[Install Tools]. To simplify and automate the full lifecycle of cluster creation and management, it is much easier to use tools supported by an established [.noloc]`Kubernetes` provider, such as Amazon EKS or Amazon EKS Anywhere.  

In {aws} Cloud, you can create link:eks/[Amazon EKS,type="documentation"] clusters using CLI tools, such as https://eksctl.io/[eksctl], or more declarative tools, such as Terraform (see https://github.com/aws-ia/terraform-aws-eks-blueprints[Amazon EKS Blueprints for Terraform]). You can also create a cluster from the {aws-management-console}. See link:eks/features/[Amazon EKS features,type="marketing"] for a list what you get with Amazon EKS. [.noloc]`Kubernetes` responsibilities that Amazon EKS takes on for you include:  



* *Managed control plane* -- {aws} makes sure that the Amazon EKS cluster is available and scalable because it manages the control plane for you and makes it available across {aws} Availability Zones. 
* *Node management* -- Instead of manually adding nodes, you can have Amazon EKS create nodes automatically as needed, using <<managed-node-groups,Managed Node Groups>> or https://karpenter.sh/[Karpenter]. Managed Node Groups have integrations with [.noloc]`Kubernetes` https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md[Cluster Autoscaling]. Using node management tools, you can take advantage of cost savings, with things like link:AWSEC2/latest/UserGuide/using-spot-instances.html[Spot Instances,type="documentation"] and node consolidation, and availability, using https://karpenter.sh/docs/concepts/scheduling/[Scheduling]features to set how workloads are deployed and nodes are selected. 
* *Cluster networking* -- Using CloudFormation templates, `eksctl` sets up networking between control plane and data plane (node) components in the [.noloc]`Kubernetes` cluster. It also sets up endpoints through which internal and external communications can take place. See link:containers/de-mystifying-cluster-networking-for-amazon-eks-worker-nodes[De-mystifying cluster networking for Amazon EKS worker nodes,type="blog"] for details. Communications between Pods in Amazon EKS is done using <<pod-identities,Amazon EKS Pod Identities>>, which provides a means of letting Pods tap into {aws} cloud methods of managing credentials and permissions. 
* *Add-Ons* -- Amazon EKS saves you from having to build and add software components that are commonly used to support [.noloc]`Kubernetes` clusters. For example, when you create an Amazon EKS cluster from the {aws} Management console, it automatically adds the Amazon EKS <<managing-kube-proxy,kube-proxy>>, <<managing-vpc-cni,Amazon VPC CNI>> plugin for [.noloc]`Kubernetes`, and <<managing-coredns,CoreDNS>> add-ons. See <<eks-add-ons>> for more on these add-ons, including a list of which are available. 

To run your clusters on your own on-premises computers and networks, Amazon offers https://anywhere.eks.amazonaws.com/[Amazon EKS Anywhere]. Instead of the {aws} Cloud being the provider, you have the choice of running Amazon EKS Anywhere on https://anywhere.eks.amazonaws.com/docs/getting-started/vsphere/[VMWare vSphere], https://anywhere.eks.amazonaws.com/docs/getting-started/baremetal/[bare metal] (https://tinkerbell.org[Tinkerbell provider]), https://anywhere.eks.amazonaws.com/docs/getting-started/snow/[Snow], https://anywhere.eks.amazonaws.com/docs/getting-started/cloudstack/[CloudStack], or https://anywhere.eks.amazonaws.com/docs/getting-started/nutanix/[Nutanix] platforms using your own equipment. 

Amazon EKS Anywhere is based on the same https://distro.eks.amazonaws.com/[Amazon EKS Distro] software that is used by Amazon EKS. However, Amazon EKS Anywhere relies on different implementations of the https://cluster-api.sigs.k8s.io/[Kubernetes Cluster API] (CAPI) interface to manage the full lifecycle of the machines in an Amazon EKS Anywhere cluster (such as https://github.com/kubernetes-sigs/cluster-api-provider-vsphere[CAPV] for vSphere and https://github.com/kubernetes-sigs/cluster-api-provider-cloudstack[CAPC] for CloudStack). Because the entire cluster is running on your equipment, you take on the added responsibility of managing the control plane and backing up its data (see etcd later in this document). 

[[cluster-components,cluster-components.title]]
=== Cluster components

[.noloc]`Kubernetes` cluster components are divided into two major areas: control plane and worker nodes. https://kubernetes.io/docs/concepts/overview/components/#control-plane-components[Control Plane Components] manage the cluster and provide access to its APIs. Worker nodes (sometimes just referred to as Nodes) provide the places where the actual workloads are run. https://kubernetes.io/docs/concepts/overview/components/#node-components[Node Components] consist of services that run on each node to communicate with the control plane and run containers. The set of worker nodes for your cluster is referred to as the _Data Plane_. 

[[concepts-control-plane,concepts-control-plane.title]]
==== Control plane

The control plane consists of a set of services that manage the cluster. These services may all be running on a single computer or may be spread across multiple computers. Internally, these are referred to as Control Plane Instances (CPIs). How CPIs are run depends on the size of the cluster and requirements for high availability. As demand increase in the cluster, a control plane service can scale to provide more instances of that service, with requests being load balanced between the instances.  

Tasks that components of the [.noloc]`Kubernetes` control plane performs include:  



* *Communicating with cluster components (API server)* -- The API server (https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/[kube-apiserver]) exposes the [.noloc]`Kubernetes` API so requests to the cluster can be made from both inside and outside of the cluster. In other words, requests to add or change a cluster's objects (Pods, Services, Nodes, and so on) can come from outside commands, such as requests from `kubectl` to run a Pod. Likewise, requests can be made from the API server to components within the cluster, such as a query to the `kubelet` service for the status of a Pod. 
* *Store data about the cluster (``*etcd*`` key value store)* -- The `etcd` service provides the critical role of keeping track of the current state of the cluster. If the `etcd` service became inaccessible, you would be unable to update or query the status of the cluster, though workloads would continue to run for a while. For that reason, critical clusters typically have multiple, load-balanced instances of the `etcd` service running at a time and do periodic backups of the `etcd` key value store in case of data loss or corruption. Keep in mind that, in Amazon EKS, this is all handled for you automatically by default. Amazon EKS Anywhere provides instruction for https://anywhere.eks.amazonaws.com/docs/clustermgmt/etcd-backup-restore/[etcd backup and restore]. See the `etcd`https://etcd.io/docs/v3.5/learning/data_model/[Data Model] to learn how `etcd` manages data. 
* *Schedule Pods to nodes (Scheduler)* -- Requests to start or stop a Pod in [.noloc]`Kubernetes` are directed to the https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/[Kubernetes Scheduler] (https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/[kube-scheduler]). Because a cluster could have multiple nodes that are capable of running the Pod, it is up to the Scheduler to choose which node (or nodes, in the case of replicas) the Pod should run on. If there is not enough available capacity to run the requested Pod on an existing node, the request will fail, unless you have made other provisions. Those provisions could include enabling services such as <<managed-node-groups,Managed Node Groups>> or https://karpenter.sh/[Karpenter] that can automatically start up new nodes to handle the workloads. 
* *Keep components in desired state (Controller Manager)* -- The [.noloc]`Kubernetes` Controller Manager runs as a daemon process (https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/[kube-controller-manager]) to watch the state of the cluster and make changes to the cluster to reestablish the expected states. In particular, there are several controllers that watch over different [.noloc]`Kubernetes` objects, which includes a `statefulset-controller`, `endpoint-controller`, `cronjob-controller`, `node-controller`, and others.
* *Manage cloud resources (Cloud Controller Manager)* -- Interactions between [.noloc]`Kubernetes` and the cloud provider that carries out requests for the underlying data center resources are handled by the https://kubernetes.io/docs/concepts/architecture/cloud-controller/[Cloud Controller Manager] (https://github.com/kubernetes/kubernetes/tree/master/cmd/cloud-controller-manager[cloud-controller-manager]). Controllers managed by the Cloud Controller Manager can include a route controller (for setting up cloud network routes), service controller (for using cloud load balancing services), and node lifecycle controller (to keep nodes in sync with Kubernetes throughout their lifecycles). 


[[worker-nodes-data-plane,worker-nodes-data-plane.title]]
==== Worker Nodes (data plane)

For a single-node [.noloc]`Kubernetes` cluster, workloads run on the same machine as the control plane. However, a more standard configuration is to have one or more separate computer systems (https://kubernetes.io/docs/concepts/architecture/nodes/[Nodes]) that are dedicated to running [.noloc]`Kubernetes` workloads.  

When you first create a [.noloc]`Kubernetes` cluster, some cluster creation tools allow you to configure a certain number nodes to be added to the cluster (either by identifying existing computer systems or by having the provider create new ones). Before any workloads are added to those systems, services are added to each node to implement these features:  



* *Manage each node (`*kubelet*`)* -- The API server communicates with the https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/[kubelet] service running on each node to make sure that the node is properly registered and Pods requested by the Scheduler are running. The kubelet can read the Pod manifests and set up storage volumes or other features needed by the Pods on the local system. It can also check on the health of the locally running containers. 
* *Run containers on a node (container runtime)* -- The https://kubernetes.io/docs/setup/production-environment/container-runtimes/[Container Runtime] on each node manages the containers requested for each Pod assigned to the node. That means that it can pull container images from the appropriate registry, run the container, stop it, and responds to queries about the container. The default container runtime is https://github.com/containerd/containerd/blob/main/docs/getting-started.md[containerd]. As of [.noloc]`Kubernetes` 1.24, the special integration of [.noloc]`Docker` (`dockershim`) that could be used as the container runtime was dropped from [.noloc]`Kubernetes`. While you can still use [.noloc]`Docker` to test and run containers on your local system, to use [.noloc]`Docker` with [.noloc]`Kubernetes` you would now have to https://docs.docker.com/engine/install/#server[Install Docker Engine] on each node to use it with [.noloc]`Kubernetes`.  
* *Manage networking between containers (kube-proxy)* -- To be able to support communication between Pods, [.noloc]`Kubernetes` uses a feature referred to as a https://kubernetes.io/docs/concepts/services-networking/service/[Service] to set up Pod networks that track IP addresses and ports associated with those Pods. The https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/[kube-proxy] service runs on every node to allow that communication between Pods to take place. 


[[extend-clusters,extend-clusters.title]]
=== Extend Clusters

There are some services you can add to [.noloc]`Kubernetes` to support the cluster, but are not run in the control plane. These services often run directly on nodes in the kube-system namespace or in its own namespace (as is often done with third-party service providers). A common example is the CoreDNS service, which provides DNS services to the cluster. Refer to https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster-services/[Discovering builtin services] for information on how to see which cluster services are running in kube-system on your cluster. 

There are different types of add-ons you can consider adding to your clusters. To keep your clusters healthy, you can add <<eks-observe,observability>> features that allow you to do things like logging, auditing, and metrics. With this information, you can troubleshoot problems that occur, often through the same observability interfaces. Examples of these types of services include link:guardduty/latest/ug/runtime-monitoring.html[Amazon GuardDuty,type="documentation"], <<cloudwatch,CloudWatch>>, https://aws-otel.github.io/[{aws} Distro for OpenTelemetry], <<managing-vpc-cni,Amazon VPC CNI>> plugin for [.noloc]`Kubernetes`, and https://grafana.com/docs/grafana-cloud/monitor-infrastructure/kubernetes-monitoring/configuration/config-aws-eks/[Grafana Kubernetes Monitoring]. For <<storage,storage>>, add-ons to Amazon EKS include <<ebs-csi,Amazon Elastic Block Store CSI Driver>> (for adding block storage devices), <<efs-csi,Amazon Elastic File System CSI Driver>> (for adding file system storage), and several third-party storage add-ons (such as <<fsx-ontap,Amazon FSx for NetApp ONTAP CSI driver>>). 

For a more complete list of available Amazon EKS add-ons, see <<eks-add-ons>>. 

[[workloads,workloads.title]]
== Workloads

[.noloc]`Kubernetes` defines a https://kubernetes.io/docs/concepts/workloads/[Workload] as "`an application running on [.noloc]`Kubernetes`.`" That application can consist of a set of microservices run as https://kubernetes.io/docs/reference/glossary/?fundamental=true#term-container[Containers] in https://kubernetes.io/docs/reference/glossary/?fundamental=true#term-pod[Pods], or could be run as a batch job or other type of applications. The job of [.noloc]`Kubernetes` is to make sure that the requests that you make for those objects to be set up or deployed are carried out. As someone deploying applications, you should learn about how containers are built, how Pods are defined, and what methods you can use for deploying them.  

[[containers,containers.title]]
=== Containers

The most basic element of an application workload that you deploy and manage in [.noloc]`Kubernetes` is a  _https://kubernetes.io/docs/concepts/workloads/pods/[Pod]_. A Pod represents a way of holding the components of an application as well as defining specifications that describe the Pod's attributes. Contrast this to something like an RPM or Deb package, which packages together software for a Linux system, but does not itself run as an entity. 

Because the Pod is the smallest deployable unit, it typically holds a single container. However, multiple containers can be in a Pod in cases where the containers are tightly coupled. For example, a web server container might be packaged in a Pod with a https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/[sidecar] type of container that may provide logging, monitoring, or other service that is closely tied to the web server container. In this case, being in the same Pod ensures that for each running instance of the Pod, both containers always run on the same node. Likewise, all containers in a Pod share the same environment, with the containers in a Pod running as though they are in the same isolated host. The effect of this is that the containers share a single IP address that provides access to the Pod and the containers can communicate with each other as though they were running on their own localhost. 

Pod specifications (https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec[PodSpec]) define the desired state of the Pod. You can deploy an individual Pod or multiple Pods by using workload resources to manage https://kubernetes.io/docs/concepts/workloads/pods/#pod-templates[Pod Templates]. Workload resources include https://kubernetes.io/docs/concepts/workloads/controllers/deployment/[Deployments] (to manage multiple Pod Replicas), https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/[StatefulSets] (to deploy Pods that need to be unique, such as database Pods), and https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/[DaemonSets] (where a Pod needs to run continuously on every node). More on those later. 

While a Pod is the smallest unit you deploy, a container is the smallest unit that you build and manage. 

[[building-containers,building-containers.title]]
==== Building Containers

The Pod is really just a structure around one or more containers, with each container itself holding the file system, executables, configuration files, libraries, and other components to actually run the application. Because a company called [.noloc]`Docker` Inc. first popularized containers, some people refer to containers as [.noloc]`Docker` Containers. However, the https://opencontainers.org/[Open Container Initiative] has since defined container runtimes, images, and distribution methods for the industry. Add to that the fact that containers were created from many existing Linux features, others often refer to containers as OCI Containers, Linux Containers, or just Containers. 

When you build a container, you typically start with a [.noloc]`Dockerfile` (literally named that). Inside that Dockerfile, you identify:  



* *A base image* -- A base container image is a container that is typically built from either a minimal version of an operating system's file system (such as https://catalog.redhat.com/software/base-images[Red Hat Enterprise Linux] or https://gallery.ecr.aws/docker/library/ubuntu[Ubuntu]) or a minimal system that is enhanced to provide software to run specific types of applications (such as a https://catalog.redhat.com/software/container-stacks/detail/611c11fabd674341b5c5ed64[nodejs] or https://gallery.ecr.aws/docker/library/python[python] apps). 
* *Application software* -- You can add your application software to your container in much the same way you would add it to a Linux system. For example, in your Dockerfile you can run `npm` and `yarn` to install a Java application or `yum` and `dnf` to install RPM packages. In other words, using a RUN command in a Dockerfile, you can run any command that is available in the file system of your base image to install software or configure software inside of the resulting container image. 
* *Instructions* -- The https://docs.docker.com/reference/dockerfile/[Dockerfile reference] describes the instructions you can add to a Dockerfile when you configure it. These include instructions used to build what is in the container itself (``ADD`` or `COPY` files from the local system), identify commands to execute when the container is run (``CMD`` or `ENTRYPOINT`), and connect the container to the system it runs on (by identifying the `USER` to run as, a local `VOLUME` to mount, or the ports to `EXPOSE`). 

While the `docker` command and service have traditionally been used to build containers (`docker build`), other tools that are available to build container images include https://docs.podman.io/en/stable/markdown/podman-build.1.html[podman] and https://github.com/containerd/nerdctl[nerdctl]. See link:containers/building-better-container-images[Building Better Container Images,type="blog"] or https://docs.docker.com/build/[Overview of Docker Build] to learn about building containers. 

[[storing-containers,storing-containers.title]]
==== Storing Containers

Once you've built your container image, you can store it in a container https://distribution.github.io/distribution/[distribution registry] on your workstation or on a public container registry. Running a private container registry on your workstation allows you to store container images locally, making them readily available to you. 

To store container images in a more public manner, you can push them to a public container registry. Public container registries provide a central location for storing and distributing container images. Examples of public container registries include the link:ecr/[Amazon Elastic Container Registry,type="marketing"], https://quay.io/[Red Hat Quay] registry, and https://hub.docker.com/[Docker Hub] registry. 

When running containerized workloads on Amazon Elastic Kubernetes Service (Amazon EKS) we recommend pulling copies of [.noloc]`Docker` Official Images that are stored in Amazon Elastic Container Registry. Amazon ECR has been storing these images since 2021. You can search for popular container images in the https://gallery.ecr.aws/[Amazon ECR Public Gallery], and specifically for the [.noloc]`Docker` Hub images, you can search the https://gallery.ecr.aws/docker/[Amazon ECR Docker Gallery]. 

[[running-containers,running-containers.title]]
==== Running containers

Because containers are built in a standard format, a container can run on any machine that can run a container runtime (such as [.noloc]`Docker`) and whose contents match the local machine's architecture (such as `x86_64` or `arm`). To test a container or just run it on your local desktop, you can use `docker run` or `podman run` commands to start up a container on the localhost. For [.noloc]`Kubernetes`, however, each worker node has a container runtime deployed and it is up to [.noloc]`Kubernetes` to request that a node run a container.  

Once a container has been assigned to run on a node, the node looks to see if the requested version of the container image already exists on the node. If it doesn't, [.noloc]`Kubernetes` tells the container runtime to pull that container from the appropriate container registry, then run that container locally. Keep in mind that a  _container image_ refers to the software package that is moved around between your laptop, the container registry, and [.noloc]`Kubernetes` nodes. A  _container_ refers to a running instance of that image. 

[[pods,pods.title]]
=== Pods

Once your containers are ready, working with Pods includes configuring, deploying, and making the Pods accessible.  

[[configuring-pods,configuring-pods.title]]
==== Configuring Pods

When you define a Pod, you assign a set of attributes to it. Those attributes must include at least the Pod name and the container image to run. However, there are many other things you want to configure with your Pod definitions as well (see the https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec[PodSpec] page for details on what can go into a Pod). These include: 



* *Storage* -- When a running container is stopped and deleted, data storage in that container will disappear, unless you set up more permanent storage. [.noloc]`Kubernetes` supports many different storage types and abstracts them under the umbrella of https://kubernetes.io/docs/concepts/storage/volumes/[Volumes]. Storage types include https://kubernetes.io/docs/concepts/storage/volumes/#cephfs[CephFS], https://kubernetes.io/docs/concepts/storage/volumes/#nfs[NFS], https://kubernetes.io/docs/concepts/storage/volumes/#iscsi[iSCSI], and others. You can even use a https://kubernetes.io/docs/concepts/storage/volumes/#local[local block device] from the local computer. With one of those storage types available from your cluster, you can mount the storage volume to a selected mount point in your container's file system. A https://kubernetes.io/docs/concepts/storage/persistent-volumes/[Persistent Volume] is one that continues to exist after the Pod is deleted, while an https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/[Ephemeral Volume] is deleted when the Pod is deleted. If your cluster administrator created different https://kubernetes.io/docs/concepts/storage/storage-classes/[StorageClasses] for your cluster, you might have the option for choosing the attributes of the storage you use, such as whether the volume is deleted or reclaimed after use, whether it will expand if more space is needed, and even whether it meets certain performance requirements. 
* *Secrets* -- By making https://kubernetes.io/docs/concepts/configuration/secret/[Secrets] available to containers in Pod specs, you can provide the permissions those containers need to access file systems, data bases, or other protected assets. Keys, passwords, and tokens are among the items that can be stored as secrets. Using secrets makes it so you don't have to store this information in container images, but need only make the secrets available to running containers. Similar to Secrets are https://kubernetes.io/docs/concepts/configuration/configmap/[ConfigMaps]. A `ConfigMap` tends to hold less critical information, such as key-value pairs for configuring a service. 
* *Container resources* -- Objects for further configuring containers can take the form of resource configuration. For each container, you can request the amount of memory and CPU that it can use, as well as place limits of the total amount of those resources that the container can use. See https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/[Resource Management for Pods and Containers] for examples. 
* *Disruptions* -- Pods can be disrupted involuntarily (a node goes down) or voluntarily (an upgrade is desired). By configuring a https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets[Pod disruption budget], you can exert some control over how available your application remains when disruptions occur. See https://kubernetes.io/docs/tasks/run-application/configure-pdb/[Specifying a Disruption Budget] for your application for examples. 
* *Namespaces* -- [.noloc]`Kubernetes` provides different ways to isolate [.noloc]`Kubernetes` components and workloads from each other. Running all the Pods for a particular application in the same https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/[Namespace] is a common way to secure and manage those Pods together. You can create your own namespaces to use or choose to not indicate a namespace (which causes [.noloc]`Kubernetes` to use the `default` namespace). [.noloc]`Kubernetes` control plane components typically run in the https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/[kube-system] namespace. 

The configuration just described is typically gathered together in a YAML file to be applied to the [.noloc]`Kubernetes` cluster. For personal [.noloc]`Kubernetes` clusters, you might just store these YAML files on your local system. However, with more critical clusters and workloads, https://www.eksworkshop.com/docs/automation/gitops/[GitOps] is a popular way to automate storage and updates to both workload and [.noloc]`Kubernetes` infrastructure resources.  

The objects used to gather together and deploy Pod information is defined by one of the following deployment methods. 

[[deploying-pods,deploying-pods.title]]
==== Deploying Pods

The method you would choose for deploying Pods depends on the type of application you plan to run with those Pods. Here are some of your choices:  



* *Stateless applications* -- A stateless application doesn't save a client's session data, so another session doesn't need to refer back to what happened to a previous session. This makes is easier to just replace Pods with new ones if they become unhealthy or move them around without saving state. If you are running a stateless application (such as a web server), you can use a https://kubernetes.io/docs/concepts/workloads/controllers/deployment/[Deployment] to deploy https://kubernetes.io/docs/concepts/workloads/pods/[Pods]and https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/[ReplicaSets]. A ReplicaSet defines how many instances of a Pod that you want running concurrently. Although you can run a ReplicaSet directly, it is common to run replicas directly within a Deployment, to define how many replicas of a Pod should be running at a time. 
* *Stateful applications* -- A stateful application is one where the identity of the Pod and the order in which Pods are launched are important. These applications need persistent storage that is stable and need to be deployed and scaled in a consistent manner. To deploy a stateful application in [.noloc]`Kubernetes`, you can use https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/[StatefulSets]. An example of an application that is typically run as a StatefulSet is a database. Within a StatefulSet, you could define replicas, the Pod and its containers, storage volumes to mount, and locations in the container where data are stored. See https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/[Run a Replicated Stateful Application] for an example of a database being deployed as a ReplicaSet. 
* *Per-node applications* -- There are times when you want to run an application on every node in your [.noloc]`Kubernetes` cluster. For example, your data center might require that every computer run a monitoring application or a particular remote access service. For [.noloc]`Kubernetes`, you can use a https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/[DaemonSet] to ensure that the selected application runs on every node in your cluster. 
* *Applications run to completion* -- There are some applications you want to run to complete a particular task. This could include one that runs monthly status reports or cleans out old data. A https://kubernetes.io/docs/concepts/workloads/controllers/job/[Job] object can be used to set up an application to start up and run, then exit when the task is done. A https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/[CronJob] object lets you set up an application to run at a specific hour, minute, day of the month, month, or day of the week, using a structure defined by the Linux https://man7.org/linux/man-pages/man5/crontab.5.html[crontab] format. 


[[making-applications-accessible-from-the-network,making-applications-accessible-from-the-network.title]]
==== Making applications accessible from the network

With applications often deployed as a set of microservices that moved around to different places, [.noloc]`Kubernetes` needed a way for those microservices to be able to find each other. Also, for others to access an application outside of the [.noloc]`Kubernetes` cluster, [.noloc]`Kubernetes` needed a way to expose that application on outside addresses and ports. These networking-related features are done with Service and Ingress objects, respectively:  



* *Services* -- Because a Pod can move around to different nodes and addresses, another Pod that needed to communicate with the first Pod could find it difficult to find where it is. To solve this problem, [.noloc]`Kubernetes` lets you represent an application as a https://kubernetes.io/docs/concepts/services-networking/service/[Service]. With a Service, you can identify a Pod or set of Pods with a particular name, then indicate what port exposes that application's service from the Pod and what ports another application could use to contact that service. Another Pod within a cluster can simply request a Service by name and [.noloc]`Kubernetes` will direct that request to the proper port for an instance of the Pod running that service.  
* *Ingress* -- https://kubernetes.io/docs/concepts/services-networking/ingress/[Ingress] is what can make applications represented by [.noloc]`Kubernetes` Services available to clients that are outside of the cluster. Basic features of Ingress include a load balancer (managed by Ingress), the Ingress controller, and rules for routing requests from the controller to the Service. There are several https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/[Ingress Controllers] that you can choose from with [.noloc]`Kubernetes`.  


[[next-steps,next-steps.title]]
== Next steps

Understanding basic [.noloc]`Kubernetes` concepts and how they relate to Amazon EKS will help you navigate both the link:eks/[Amazon EKS documentation,type="documentation"] and https://kubernetes.io/docs[Kubernetes documentation] to find the information you need to manage Amazon EKS clusters and deploy workloads to those clusters. To begin using Amazon EKS, choose from the following:



* <<getting-started-eksctl,Create a simple cluster>>
* <<create-cluster,Create a more complex cluster>>
* <<sample-deployment,Deploy a sample application>>
* <<eks-managing,Explore ways of managing your cluster>>
