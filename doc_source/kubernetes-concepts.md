# Kubernetes concepts<a name="kubernetes-concepts"></a>

Amazon Elastic Kubernetes Service \(Amazon EKS\) is an AWS managed service based on the open source [https://kubernetes.io/](https://kubernetes.io/)project\. While there are things you need to know about how the Amazon EKS service integrates with AWS Cloud \(particularly when you first create an Amazon EKS cluster\), once it’s up and running, you use your Amazon EKS cluster in much that same way as you would any other Kubernetes cluster\. So to begin managing Kubernetes clusters and deploying workloads, you need at least a basic understanding of Kubernetes concepts\. 

 This page divides Kubernetes concepts into three sections: Why Kubernetes, Clusters, and Workloads\. The first section describes the value of running a Kubernetes service, in particular as a managed service like Amazon EKS\. The Workloads section covers how Kubernetes applications are built, stored, run, and managed\. The Clusters section lays out the different components that make up Kubernetes clusters and what your responsibilities are for creating and maintaining Kubernetes clusters\. 

**Topics**
+ [Why Kubernetes?](#why-kubernetes)
+ [Clusters](#clusters)
+ [Workloads](#workloads)
+ [Next steps](#next-steps)

 As you go through this content, links will lead you to further descriptions of Kubernetes concepts in both Amazon EKS and Kubernetes documentation, in case you want to take deep dives into any of the topics we cover here\. For details about how Amazon EKS implements Kubernetes control plane and compute features, see [Amazon EKS architecture](https://docs.aws.amazon.com/eks/latest/userguide/eks-architecture.html)\. 

## Why Kubernetes?<a name="why-kubernetes"></a>

 Kubernetes was designed to improve availability and scalability when running mission\-critical, production\-quality containerized applications\. Rather than just running Kubernetes on a single machine \(although that is possible\), Kubernetes achieves those goals by allowing you to run applications across sets of computers that can expand or contract to meet demand\. Kubernetes includes features that make it easier for you to: 
+  Deploy applications on multiple machines \(using containers deployed in Pods\) 
+  Monitor container health and restart failed containers 
+  Scale containers up and down based on load 
+  Update containers with new versions 
+  Allocate resources between containers 
+  Balance traffic across machines 

 Having Kubernetes automate these types of complex tasks allows an application developers to focus on building and improving their application workloads, rather than worrying about infrastructure\. The developer typically creates configuration files, formatted as YAML files, that describe the desired state of the application\. This could include which containers to run, resource limits, number of Pod replicas, CPU/memory allocation, affinity rules, and more\. 

### Attributes of Kubernetes<a name="attributes-of-kubernetes"></a>

 To achieve its goals, Kubernetes has the following attributes: 
+  **Containerized** \- Kubernetes is a container orchestration tool\. To use Kubernetes, you must first have your applications containerized\. Depending on the type of application, this could be as a set of *microservices,* as batch jobs or in other forms\. Then, your applications can take advantage of a Kubernetes workflow that encompasses a huge ecosystem of tools, where containers can be stored as [images in a container registry](https://kubernetes.io/docs/concepts/containers/images/#multi-architecture-images-with-image-indexes), deployed to a Kubernetes [cluster](https://kubernetes.io/docs/concepts/architecture/), and run on an available [node](https://kubernetes.io/docs/concepts/architecture/nodes/)\. You can build and test individual containers on your local computer with Docker or another [container runtime](https://kubernetes.io/docs/setup/production-environment/container-runtimes/), before deploying them to your Kubernetes cluster\. 
+  **Scalable** \- If the demand for your applications exceeds the capacity of the running instances of those applications, Kubernetes is able to scale up\. As needed, Kubernetes can tell if applications require more CPU or memory and respond by either automatically expanding available capacity or using more of existing capacity\. Scaling can be done at the Pod level, if there is enough compute available to just run more instances of the application \([horizontal Pod autoscaling](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)\), or at the node level, if more nodes need to be brought up to handle the increased capacity \([Cluster Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler) or [Karpenter](https://karpenter.sh/)\)\. As capacity is no longer needed, these services can delete unnecessary Pods and shut down unneeded nodes\. 
+  **Available** \- If an application or node becomes unhealthy or unavailable, Kubernetes can move running workloads to another available node\. You can force the issue by simply deleting a running instance of a workload or node that’s running your workloads\. The bottom line here is that workloads can be brought up in other locations if they can no longer run where they are\. 
+  **Declarative** \- Kubernetes uses active reconciliation to constantly check that the state that you declare for you cluster matches the actual state\. By applying [Kubernetes objects](https://kubernetes.io/docs/concepts/overview/working-with-objects/) to a cluster, typically through YAML\-formatted configuration files, you can, for example, ask to start up the workloads you want to run on your cluster\. You can later change the configurations to do something like use a later version of a container or allocate more memory\. Kubernetes will do what it needs to do to establish the desired state\. This can include bringing nodes up or down, stopping and restarting workloads, or pulling updated containers\. 
+  **Composable** \- Because an application typically consists of multiple components, you want to be able to manage a set of these components \(often represented by multiple containers\) together\. While Docker Compose offers a way to do this directly with Docker, the Kubernetes [Kompose](http://kompose.io/)command can help you do that with Kubernetes\. See [Translate a Docker Compose File to Kubernetes Resources](https://kubernetes.io/docs/tasks/configure-pod-container/translate-compose-kubernetes/) for an example of how to do this\. 
+  **Extensible** \- Unlike proprietary software, the open source Kubernetes project is designed to be open to you extending Kubernetes any way that you like to meet your needs\. APIs and configuration files are open to direct modifications\. Third\-parties are encouraged to write their own [Controllers](https://kubernetes.io/docs/concepts/architecture/controller/), to extend both infrastructure and end\-user Kubernetes featrues\. [Webhooks](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/) let you set up cluster rules to enforce policies and adapt to changing conditions\. For more ideas on how to extend Kubernetes clusters, see [Extending Kubernetes](https://kubernetes.io/docs/concepts/extend-kubernetes/)\. 
+  **Portable** \- Many organizations have standardized their operations on Kubernetes because it allows them to manage all of their application needs in the same way\. Developers can use the same pipelines to build and store containerized applications\. Those applications can then be deployed to Kubernetes clusters running on\-premises, in clouds, on point\-of\-sales terminals in restaurants, or on IOT devices dispersed across company’s remote sites\. Its open source nature makes it possible for people to develop these special Kubernetes distributions, along will tools needed to manage them\. 

### Managing Kubernetes<a name="managing-kubernetes"></a>

 Kubernetes source code is freely available, so with your own equipment you could install and manage Kubernetes yourself\. However, self\-managing Kubernetes requires deep operational expertise and takes time and effort to maintain\. For those reasons, most people deploying production workloads choose a cloud provider \(such as Amazon EKS\) or on\-premises provider \(such as Amazon EKS Anywhere\) with its own tested Kubernetes distribution and support of Kubernetes experts\. This allows you to offload much of the undifferentiated heavy lifting needed to maintain your clusters, including: 
+  **Hardware** \- If you don’t have hardware available to run Kubernetes per your requirements, a cloud provider such as AWS Amazon EKS can save you on upfront costs\. With Amazon EKS, this means that you can consume the best cloud resources offered by AWS, including computer instances \(Amazon Elastic Compute Cloud\), your own private environment \(Amazon VPC\), central identity and permissions management \(IAM\), and storage \(Amazon EBS\)\. AWS manages the computers, networks, data centers, and all the other physical components needed to run Kubernetes\. Likewise, you don’t have to plan your datacenter to handle the maximum capacity on your highest\-demand days\. For Amazon EKS Anywhere, or other on premises Kubernetes clusters, you are responsible for managing the infrastructure used in your Kubernetes deployments, but you can still rely on AWS to help you keep Kubernetes up to date\. 
+  **Control plane management** \- Amazon EKS manages the security and availability of the AWS\-hosted Kubernetes control plane, which is responsible for scheduling containers, managing the availability of applications, and other key tasks, so you can focus on your application workloads\. If your cluster breaks, AWS should have the means to restore your cluster to a running state\. For Amazon EKS Anywhere, you would manage the control plane yourself\. 
+  **Tested upgrades** \- When you upgrade your clusters, you can rely on Amazon EKS or Amazon EKS Anywhere to provide tested versions of their Kubernetes distributions\. 
+  **Add\-ons** \- There are hundreds of projects built to extend and work with Kubernetes that you can add to your cluster’s infrastructure or use to aid the running of your workloads\. Instead of building and managing those add\-ons yourself, AWS provides [Amazon EKS Add\-ons](https://docs.aws.amazon.com/eks/latest/userguide/eks-add-ons.html) that you can use with your clusters\. Amazon EKS Anywhere provides [Curated Packages](https://anywhere.eks.amazonaws.com/docs/packages/) that include builds of many popular open source projects\. So you don’t have to build the software yourself or manage critical security patches, bug fixes, or upgrades\. Likewise, if the defaults meet your needs, it’s typical for very little configuration of those add\-ons to be needed\. See [Extend Clusters](#extend-clusters) for details on extending your cluster with add\-ons\. 

### Kubernetes in action<a name="kubernetes-in-action"></a>

 The following diagram shows key activities you would do as a Kubernetes Admin or Application Developer to create and use a Kubernetes cluster\. In the process, it illustrates how Kubernetes components interact with each other, using the AWS cloud as the example of the underlying cloud provider\. 

![\[A Kubernetes cluster in action.\]](http://docs.aws.amazon.com/eks/latest/userguide/images/k8sinaction.png)

 A Kubernetes Admin creates the Kubernetes cluster using a tool specific to the type of provider on which the cluster will be built\. This example uses the AWS cloud as the provider, which offers the managed Kubernetes service called Amazon EKS\. The managed service automatically allocates the resources needed to create the cluster, including creating two new Virtual Private Clouds \(Amazon VPCs\) for the cluster, setting up networking, mapping Kubernetes permissions into those to manage assets in the cloud, seeing that the control plane services have places to run, and allocating zero or more Amazon EC2 instances as Kubernetes nodes for running workloads\. AWS manages one Amazon VPC itself for the control plane, while the other Amazon VPC contains the customer nodes that run workloads\. 

 Many of the Kubernetes Admin’s tasks going forward are done using Kubernetes tools such as kubectl\. That tool makes requests for services directly to the cluster’s control plane\. The ways that queries and changes are made to the cluster are then very similar to the ways you would do them on any Kubernetes cluster\. 

 An application developer wanting to deploy workloads to this cluster can perform several tasks\. The developer needs to build the application into one or more container images, then push those images to a container registry that is accessible to the Kubernetes cluster\. AWS offers the Amazon Elastic Container Registry \(Amazon ECR\) for that purpose\. 

 To run the application, the developer can create YAML\-formatted configuration files that tell the cluster how to run the application, including which containers to pull from the registry and how to wrap those containers in Pods\. The control plane \(scheduler\) schedules the containers to one or more nodes and the container runtime on each node actually pulls and runs the needed containers\. The developer can also set up an application load balancer to balance traffic to available containers running on each node and expose the application so it is available on a public network to the outside world\. With that all done, someone wanting to use the application can connect to the application endpoint to access it\. 

 The following section go through details of each of these features, from the perspective of Kubernetes Clusters and Workloads\. 

## Clusters<a name="clusters"></a>

 If your job is to start and manage Kubernetes clusters, you should know how Kubernetes clusters are created, enhanced, managed, and deleted\. You should also know what the components are that make up a cluster and what you need to do to maintain those components\. 

 Tools for managing clusters handle the overlap between the Kubernetes services and the underlying hardware provider\. For that reason, automation of these tasks tend to be done by the Kubernetes provider \(such as Amazon EKS or Amazon EKS Anywhere\) using tools that are specific to the provider\. For example, to start an Amazon EKS cluster you can use `eksctl create cluster`, while for Amazon EKS Anywhere you can use `eksctl anywhere create cluster`\. Note that while these commands create a Kubernetes cluster, they are specific to the provider and are not part of the Kubernetes project itself\. 

### Cluster creation and management tools<a name="cluster-creation-and-management-tools"></a>

 The Kubernetes project offers tools for creating a Kubernetes cluster manually\. So if you want to install Kubernetes on a single machine, or run the control plane on a machine and add nodes manually, you can use CLI tools like [kind](https://kind.sigs.k8s.io/), [minikube](https://kubernetes.io/docs/tutorials/hello-minikube/), or [kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/)that are listed under Kubernetes [Install Tools](https://kubernetes.io/docs/tasks/tools/)\. To simplify and automate the full lifecycle of cluster creation and management, it is much easier to use tools supported by an established Kubernetes provider, such as Amazon EKS or Amazon EKS Anywhere\. 

 In AWS Cloud, you can create [Amazon EKS](https://docs.aws.amazon.com/eks/) clusters using CLI tools, such as [eksctl](https://eksctl.io/), or more declarative tools, such as Terraform \(see [Amazon EKS Blueprints for Terraform](https://github.com/aws-ia/terraform-aws-eks-blueprints)\)\. You can also create a cluster from the AWS Management Console\. See [Amazon EKS features](https://aws.amazon.com/eks/features/) for a list what you get with Amazon EKS\. Kubernetes responsibilities that Amazon EKS takes on for you include: 
+  **Managed control plane** \- AWS makes sure that the Amazon EKS cluster is available and scalable because it manages the control plane for you and makes it available across AWS Availability Zones\. 
+  **Node management** \- Instead of manually adding nodes, you can have Amazon EKS create nodes automatically as needed, using [Managed Node Groups](https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html) or [Karpenter](https://karpenter.sh/)\. Managed Node Groups have integrations with Kubernetes [Cluster Autoscaling](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md)\. Using node management tools, you can take advantage of cost savings, with things like [Spot Instances](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html) and node consolidation, and availability, using [Scheduling](https://karpenter.sh/docs/concepts/scheduling/)features to set how workloads are deployed and nodes are selected\. 
+  **Cluster networking** \- Using CloudFormation templates, `eksctl` sets up networking between control plane and data plane \(node\) components in the Kubernetes cluster\. It also sets up endpoints through which internal and external communications can take place\. See [De\-mystifying cluster networking for Amazon EKS worker nodes](https://aws.amazon.com/blogs/containers/de-mystifying-cluster-networking-for-amazon-eks-worker-nodes/) for details\. Communications between Pods in Amazon EKS is done using [Amazon EKS Pod Identities](https://docs.aws.amazon.com/eks/latest/userguide/pod-identities.html), which provides a means of letting Pods tap into AWS cloud methods of managing credentials and permissions\. 
+  **Add\-Ons** \- Amazon EKS saves you from having to build and add software components that are commonly used to support Kubernetes clusters\. For example, when you create an Amazon EKS cluster from the AWS Management console, it automatically adds the Amazon EKS [kube\-proxy](https://docs.aws.amazon.com/eks/latest/userguide/managing-kube-proxy.html), [Amazon VPC CNI](https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html) plugin for Kubernetes, and [CoreDNS](https://docs.aws.amazon.com/eks/latest/userguide/managing-coredns.html) add\-ons\. See [Amazon EKS add\-ons](https://docs.aws.amazon.com/eks/latest/userguide/eks-add-ons.html) for more on these add\-ons, including a list of which are available\. 

 To run your clusters on your own on\-premises computers and networks, Amazon offers [Amazon EKS Anywhere](https://anywhere.eks.amazonaws.com/)\. Instead of the AWS Cloud being the provider, you have the choice of running Amazon EKS Anywhere on [VMWare vSphere](https://anywhere.eks.amazonaws.com/docs/getting-started/vsphere/), [bare metal](https://anywhere.eks.amazonaws.com/docs/getting-started/baremetal/) \([Tinkerbell provider](https://tinkerbell.org)\), [Snow](https://anywhere.eks.amazonaws.com/docs/getting-started/snow/), [CloudStack](https://anywhere.eks.amazonaws.com/docs/getting-started/cloudstack/), or [Nutanix](https://anywhere.eks.amazonaws.com/docs/getting-started/nutanix/) platforms using your own equipment\. 

 Amazon EKS Anywhere is based on the same [Amazon EKS Distro](https://distro.eks.amazonaws.com/) software that is used by Amazon EKS\. However, Amazon EKS Anywhere relies on different implementations of the [Kubernetes Cluster API](https://cluster-api.sigs.k8s.io/) \(CAPI\) interface to manage the full lifecycle of the machines in an Amazon EKS Anywhere cluster \(such as [CAPV](https://github.com/kubernetes-sigs/cluster-api-provider-vsphere) for vSphere and [CAPC](https://github.com/kubernetes-sigs/cluster-api-provider-cloudstack) for CloudStack\)\. Because the entire cluster is running on your equipment, you take on the added responsibility of managing the control plane and backing up its data \(see etcd later in this document\)\. 

### Cluster components<a name="cluster-components"></a>

 Kubernetes cluster components are divided into two major areas: control plane and worker nodes\. [Control Plane Components](https://kubernetes.io/docs/concepts/overview/components/#control-plane-components) manage the cluster and provide access to its APIs\. Worker nodes \(sometimes just referred to as Nodes\) provide the places where the actual workloads are run\. [Node Components](https://kubernetes.io/docs/concepts/overview/components/#node-components) consist of services that run on each node to communicate with the control plane and run containers\. The set of worker nodes for you cluster is referred to as the *Data Plane*\. 

#### Control plane<a name="control-plane"></a>

 The control plane consists of a set of services that manage the cluster\. These services may all be running on a single computer or may be spread across multiple computers\. Internally, these are referred to as Control Plane Instances \(CPIs\)\. How CPIs are run depends on the size of the cluster and requirements for high availability\. As demand increase in the cluster, a control plane service can scale to provide more instances of that service, with requests being load balanced between the instances\. 

 Tasks that components of the Kubernetes control plane performs include: 
+  **Communicating with cluster components \(API server\)** \- The API server \([kube\-apiserver](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/)\) exposes the Kubernetes API so requests to the cluster can be made from both inside and outside of the cluster\. In other words, requests to add or change a cluster’s objects \(Pods, Services, Nodes, and so on\) can come from outside commands, such as requests from `kubectl` to run a Pod\. Likewise, requests can be made from the API server to components within the cluster, such as a query to the `kubelet` service for the status of a Pod\. 
+  **Store data about the cluster \(etcd key value store\)** \- The etcd service provides the critical role of keeping track of the current state of the cluster\. If the etcd service became inaccessible, you would be unable to update or query the status of the cluster, though workloads would continue to run for a while\. For that reason, critical clusters typically have multiple, load\-balanced instances of the etcd service running at a time and do periodic backups of the etcd key value store in case of data loss or corruption\. Keep in mind that, in Amazon EKS, this is all handled for you automatically by default\. Amazon EKS Anywhere provides instruction for [etcd backup and restore](https://anywhere.eks.amazonaws.com/docs/clustermgmt/etcd-backup-restore/)\. See the etcd [Data Model](https://etcd.io/docs/v3.5/learning/data_model/) to learn how etcd manages data\. 
+  **Schedule Pods to nodes \(Scheduler\)** \- Requests to start or stop a Pod in Kubernetes are directed to the [Kubernetes Scheduler](https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/) \([kube\-scheduler](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/)\)\. Because a cluster could have multiple nodes that are capable of running the Pod, it is up to the Scheduler to choose which node \(or nodes, in the case of replicas\) the Pod should run on\. If there is not enough available capacity to run the requested Pod on an existing node, the request will fail, unless you have made other provisions\. Those provisions could include enabling services such as [Managed Node Groups](https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html) or [Karpenter](https://karpenter.sh/) that can automatically start up new nodes to handle the workloads\. 
+  **Keep components in desired state \(Controller Manager\)** \- The Kubernetes Controller Manager runs as a daemon process \([kube\-controller\-manager](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/)\) to watch the state of the cluster and make changes to the cluster to reestablish the expected states\. In particular, there are several controllers that watch over different Kubernetes objects, which includes a node\-lifecycle\-controller, statefulset\-controller, endpoint\-controller, cronjob\-controller, and others\. 
+  **Manage cloud resources \(Cloud Controller Manager\)** \- Interactions between Kubernetes and the cloud provider that carries out requests for the underlying data center resources are handled by the [Cloud Controller Manager](https://kubernetes.io/docs/concepts/architecture/cloud-controller/) \([cloud\-controller\-manager](https://github.com/kubernetes/kubernetes/tree/master/cmd/cloud-controller-manager)\)\. Controllers managed by the Cloud Controller Manager can include a route controller \(for setting up cloud network routes\), service controller \(for using cloud load balancing services\), and node controller \(for using cloud APIs to keep Kubernetes nodes in sync with cloud nodes\)\. 

#### Worker Nodes \(data plane\)<a name="worker-nodes-data-plane"></a>

 For a single\-node Kubernetes cluster, workloads run on the same machine as the control plane\. However, a more normal configuration is to have one or more separate computer systems \([Nodes](https://kubernetes.io/docs/concepts/architecture/nodes/)\) that is dedicated to running Kubernetes workloads\. 

 When you first create a Kubernetes cluster, some cluster creation tools allow you to configure a certain number nodes to be added to the cluster \(either by identifying existing computer systems or by having the provider create new ones\)\. Before any workloads are added to those systems, services are added to each node to implement these features: 
+  **Manage each node \(kubelet\)** \- The API server communicates with the [kubelet](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/) service running on each node to make sure that the node is properly registered and Pods requested by the Scheduler are running\. The kubelet can read the Pod manifests and set up storage volumes or other features needed by the Pods on the local system\. It can also check on the health of the locally running containers\. 
+  **Run containers on a node \(container runtime\)** \- The [Container Runtime](https://kubernetes.io/docs/setup/production-environment/container-runtimes/) on each node manages the containers requested for each Pod assigned to the node\. That means that it can pull container images from the appropriate registry, run the container, stop it, and responds to queries about the container\. The default container runtime is [containerd](https://github.com/containerd/containerd/blob/main/docs/getting-started.md)\. As of Kubernetes 1\.24, the special integration of Docker \(Dockershim\) that could be used as the container runtime was dropped from Kubernetes\. While you can still use Docker to test and run containers on your local system, to use Docker with Kubernetes you would now have to [Install Docker Engine](https://docs.docker.com/engine/install/#server) on each node to use it with Kubernetes\. 
+  **Manage networking between containers \(kube\-proxy\)** \- To be able to support communication between Pods using Services, Kubernetes needed a way to set up Pod networks to track IP addresses and ports associated with those Pods\. The [kube\-proxy](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/) service runs on every node to allow that communication between Pods to take place\. 

### Extend Clusters<a name="extend-clusters"></a>

 There are some services you can add to Kubernetes to support the cluster, but are not run in the control plane\. These services often run directly on nodes in the kube\-system namespace or in its own namespace \(as is often done with third\-party service providers\)\. A common example is the CoreDNS service, which provides DNS services to the cluster\. Refer to [Discovering builtin services](https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster-services/) for information on how to see which cluster services are running in kube\-system on your cluster\. 

 There are different types of add\-ons you can consider adding to your clusters\. To keep your clusters healthy, you can add [observability](https://docs.aws.amazon.com/eks/latest/userguide/eks-observe.html) features that allow you to do things like logging, auditing, and metrics\. With this information, you can troubleshoot problems that occur, often through the same observability interfaces\. Examples of these types of services include [Amazon GuardDuty](https://docs.aws.amazon.com/guardduty/latest/ug/runtime-monitoring.html), [CloudWatch](https://docs.aws.amazon.com/eks/latest/userguide/cloudwatch.html), [AWS Distro for OpenTelemetry](https://aws-otel.github.io/), [Amazon VPC CNI](https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html) plugin for Kubernetes, and [Grafana Kubernetes Monitoring](https://grafana.com/docs/grafana-cloud/monitor-infrastructure/kubernetes-monitoring/configuration/config-aws-eks/)\. For [storage](https://docs.aws.amazon.com/eks/latest/userguide/storage.html), add\-ons to Amazon EKS include [Amazon Elastic Block Store CSI Driver](https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html) \(for adding block storage devices\), [Amazon Elastic File System CSI Driver](https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html) \(for adding file system storage\), and several third\-party storage add\-ons \(such as [Amazon FSx for NetApp ONTAP CSI driver](https://docs.aws.amazon.com/eks/latest/userguide/fsx-ontap.html)\)\. 

 For a more complete list of available Amazon EKS add\-ons, see [Amazon EKS add\-ons](https://docs.aws.amazon.com/eks/latest/userguide/eks-add-ons.html)\. 

## Workloads<a name="workloads"></a>

 Kubernetes defines a [Workload](https://kubernetes.io/docs/concepts/workloads/) as “an application running on Kubernetes\.” That application can consist of a set of microservices run as [Containers](https://kubernetes.io/docs/reference/glossary/?fundamental=true#term-container) in [Pods](https://kubernetes.io/docs/reference/glossary/?fundamental=true#term-pod), or could be run as a batch job or other type of applications\. The job of Kubernetes is to make sure that the requests that you make for those objects to be set up or deployed are carried out\. As someone deploying applications, you should learn about how containers are built, how Pods are defined, and what methods you can use for deploying them\. 

### Containers<a name="containers"></a>

 The most basic element of an application workload that you deploy and manage in Kubernetes is a *[Pod](https://kubernetes.io/docs/concepts/workloads/pods/)*\. A Pod represents a way of holding the components of an application as well as defining specifications that describes the Pod’s attributes\. Contrast this to something like an RPM or Deb package, which packages together software for a Linux system, but does not itself run as an entity\. 

 Because the Pod is the smallest deployable unit, it typically holds a single container\. However, multiple containers can be in a Pod in cases where the containers are tightly coupled\. For example, a web server container might be packaged in a Pod with a [sidecar](https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/) type of container that may provide logging, monitoring, or other service that is closely tied to the web server container\. In this case, being in the same Pod ensures that for each running instance of the Pod, both containers always run on the same node\. Likewise, all containers in a Pod share the same environment, with the containers in a Pod running as though they are in the same isolated host\. The effect of this is that the containers share a single IP address that provides access to the Pod and the containers can communicate with each other as though they were running on their own localhost\. 

 Pod specifications \([PodSpec](https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec)\) define the desired state of the Pod\. You can deploy an individual Pod or multiple Pods by using workload resources to manage [Pod Templates](https://kubernetes.io/docs/concepts/workloads/pods/#pod-templates)\. Workload resources include [Deployments](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) \(to manage multiple Pod Replicas\), [StatefulSets](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/) \(to deploy Pods that need to be unique, such as database Pods\), and [DaemonSets](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/) \(where a Pod needs to run continuously on every node\)\. More on those later\. 

 While a Pod is the smallest unit you deploy, a container is the smallest unit that you build and manage\. 

#### Building Containers<a name="building-containers"></a>

 The Pod is really just a structure around one or more containers, with each container itself holding the file system, executables, configuration files, libraries, and other components to actually run the application\. Because a company called Docker Inc\. first popularized containers, some people refer to containers as Docker Containers\. However, the [Open Container Initiative](https://opencontainers.org/) has since defined container runtimes, images, and distribution methods for the industry\. Add to that the fact that containers were created from many existing Linux features, others often refer to containers as OCI Containers, Linux Containers, or just Containers\. 

 When you build a container, you typically start with a Dockerfile \(literally named that\)\. Inside that Dockerfile, you identify: 
+  **A base image** \- A base container image is a container that is typically built from either a minimal version of an operating system’s file system \(such as [Red Hat Enterprise Linux](https://catalog.redhat.com/software/base-images) or [Ubuntu](https://gallery.ecr.aws/docker/library/ubuntu)\) or a minimal system that is enhanced to provide software to run specific types of applications \(such as a [nodejs](https://catalog.redhat.com/software/container-stacks/detail/611c11fabd674341b5c5ed64) or [python](https://gallery.ecr.aws/docker/library/python) apps\)\. 
+  **Application software** \- You can add your application software to your container in much the same way you would add it to a Linux system\. For example, in your Dockerfile you can run `npm` and `yarn` to install a Java application or `yum` and `dnf` to install RPM packages\. In other words, using a RUN command in a Dockerfile, you can run any command that is available in the file system of your base image to install software or configure software inside of the resulting container image\. 
+  **Instructions** \- The [Dockerfile reference](https://docs.docker.com/reference/dockerfile/) describes the instructions you can add to a Dockerfile when you configure it\. These include instructions used to build what is in the container itself \(`ADD` or `COPY` files from the local system\), identify commands to execute when the container is run \(`CMD` or `ENTRYPOINT`\), and connect the container to the system it runs on \(by identifying the `USER` to run as, a local `VOLUME` to mount, or the ports to `EXPOSE`\)\. 

 While the `docker` command and service have traditionally been used to build containers \(`docker build`\), other tools that are available to build container images include [podman](https://docs.podman.io/en/stable/markdown/podman-build.1.html) and [nerdctl](https://github.com/containerd/nerdctl)\. See [Building Better Container Images](https://aws.amazon.com/blogs/containers/building-better-container-images/) or [Build with Docker](https://docs.docker.com/build/guide/) to learn about building containers\. 

#### Storing Containers<a name="storing-containers"></a>

 Once you’ve built your container image, you can store it in a container [distribution registry](https://distribution.github.io/distribution/) on your workstation or on a public container registry\. Running a private container registry on your workstation allows you to store container images locally, making them readily available to you\. 

 To store container images in a more public manner, you can push them to a public container registry\. Public container registries provide a central location for storing and distributing container images\. Examples of public container registries include the [Amazon Elastic Container Registry](https://aws.amazon.com/ecr/), [Red Hat Quay](https://quay.io/) registry, and [Docker Hub](https://hub.docker.com/) registry\. 

 When running containerized workloads on Amazon Elastic Kubernetes Service \(Amazon EKS\) we recommend pulling copies of Docker Official Images that are stored in Amazon Elastic Container Registry\. AWS Amazon ECR has been storing these images since 2021\. You can search for popular container images in the [Amazon ECR Public Gallery](https://gallery.ecr.aws/), and specifically for the Docker Hub images, you can search the [Amazon ECR Docker Gallery](https://gallery.ecr.aws/docker/)\. 

#### Running containers<a name="running-containers"></a>

 Because containers are built in a standard format, a container can run on any machine that can run a container runtime \(such as Docker\) and whose contents match the local machine’s architecture \(such as `x86_64` or `arm`\)\. To test a container or just run it on your local desktop, you can use `docker run` or `podman run` commands to start up a container on the localhost\. For Kubernetes, however, each worker node has a container runtime deployed and it is up to Kubernetes to request that a node run a container\. 

 Once a container has been assigned to run on a node, the node looks to see if the requested version of the container image already exists on the node\. If it doesn’t, Kubernetes tells the container runtime to pull that container from the appropriate container registry, then run that container locally\. Keep in mind that a *container image* refers to the software package that is moved around between your laptop, the container registry, and Kubernetes nodes\. A *container* refers to a running instance of that image\. 

### Pods<a name="pods"></a>

 Once your containers are ready, working with Pods includes configuring, deploying, and making the Pods accessible\. 

#### Configuring Pods<a name="configuring-pods"></a>

 When you define a Pod, you assign a set of attributes to it\. Those attributes must include at least the Pod name and the container image to run\. However, there are many other things you want to configure with your Pod definitions as well \(see the [PodSpec](https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec) page for details on what can go into a Pod\)\. These include: 
+  **Storage** \- When a running container is stopped and deleted, data storage in that container will disappear, unless you set up more permanent storage\. Kubernetes supports many different storage types and abstracts them under the umbrella of [Volumes](https://kubernetes.io/docs/concepts/storage/volumes/)\. Storage types include [CephFS](https://kubernetes.io/docs/concepts/storage/volumes/#cephfs), [NFS](https://kubernetes.io/docs/concepts/storage/volumes/#nfs), [iSCSI](https://kubernetes.io/docs/concepts/storage/volumes/#iscsi), and others\. You can even use a [local block device](https://kubernetes.io/docs/concepts/storage/volumes/#local) from the local computer\. With one of those storage types available from your cluster, you can mount the storage volume to a selected mount point in your container’s filesystem\. A [Persistent Volume](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is one that continues to exist after the Pod is deleted, while an [Ephemeral Volume](https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/) is deleted when the Pod is deleted\. If your cluster administrator created different [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) for your cluster, you might have the option for choosing the attributes of the storage you use, such as whether the volume is deleted or reclaimed after use, whether it will expand if more space is needed, and even whether whether it meets certain performance requirements\. 
+  **Secrets** \- By making [Secrets](https://kubernetes.io/docs/concepts/configuration/secret/) available to containers in Pod specs, you can provide the permissions those containers need to access file systems, data bases, or other protected assets\. Keys, passwords, and tokens are among the items that can be stored as secrets\. Using secrets make it so you don’t have to store this information in container images, but need only make the secrets available to running containers\. Similar to Secrets are [ConfigMaps](https://kubernetes.io/docs/concepts/configuration/configmap/)\. A ConfigMap tends to hold less critical information, such as key\-value pairs for configuring a service\. 
+  **Container resources** \- Objects for further configuring containers can take the form of resource configuration\. For each container, you can request the amount of memory and CPU that it can use, as well as place limits of the total amount of those resources that the container can use\. See [Resource Management for Pods and Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/) for examples\. 
+  **Disruptions** \- Pods can be disrupted involuntarily \(a node goes down\) or voluntarily \(an upgrade is desired\)\. By configuring a [Pod disruption budget](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets), you can exert some control over how available your application remains when disruptions occur\. See [Specifying a Disruption Budget](https://kubernetes.io/docs/tasks/run-application/configure-pdb/) for your application for examples\. 
+  **Namespaces** \- Kubernetes provides different ways to isolate Kubernetes components and workloads from each other\. Running all the Pods for a particular application in the same [Namespace](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/) is a common way to secure and manage those Pods together\. You can create your own namespaces to use or choose to not indicate a namespace \(which causes Kubernetes to use the `default` namespace\)\. Kubernetes control plane components typically run in the [kube\-system](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/) namespace\. 

 The configuration just described is typically gathered together in a YAML file to be applied to the Kubernetes cluster\. For personal Kubernetes clusters, you might just store these YAML files on your local system\. However, with more critical clusters and workloads, [GitOps](https://www.eksworkshop.com/docs/automation/gitops/) is a popular way to automate storage and updates to both workload and Kubernetes infrastructure resources\. 

 The objects used to gather together and deploy Pod information is defined by one of the following deployment methods\. 

#### Deploying Pods<a name="deploying-pods"></a>

 The method you would choose for deploying Pods depends on the type of application you plan to run with those Pods\. Here are some of your choices: 
+  **Stateless applications** \- A stateless application doesn’t save a client’s session data, so another session doesn’t need to refer back to what happened to a previous session\. This makes is easier to just replace Pods with new ones if they become unhealthy or move them around without saving state\. If you are running a stateless application \(such as a web server\), you can use a [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) to deploy [Pods](https://kubernetes.io/docs/concepts/workloads/pods/)and [ReplicaSets](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/)\. A ReplicaSet defines how many instances of a Pod that you want running concurrently\. Although you can run a ReplicaSet directly, it is common to run replicas directly within a Deployment, to define how many replicas of a Pod should be running at a time\. 
+  **Stateful applications** \- A stateful application is one where the identity of the Pod and the order in which Pods are launched are important\. These applications need persistent storage that is stable and need to be deployed and scaled in a consistent manner\. To deploy a stateful application in Kubernetes, you can use [StatefulSets](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/)\. An example of an application that is typically run as a StatefulSet is a database\. Within a StatefulSet, you could define replicas, the Pod and its containers, storage volumes to mount, and locations in the container where data are stored\. See [Run a Replicated Stateful Application](https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/) for an example of a database being deployed as a ReplicaSet\. 
+  **Per\-node** **applications** \- There are times when you want to run an application on every node in your Kubernetes cluster\. For example, your data center might require that every computer run a monitoring application or a particular remote access service\. For Kubernetes, you can use a [DaemonSet](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/) to ensure that the selected application runs on every node in your cluster\. 
+  **Applications run to completion** \- There are some applications you want to run to complete a particular task\. This could include one that runs monthly status reports or cleans out old data\. A [Job](https://kubernetes.io/docs/concepts/workloads/controllers/job/) object can be used to set up an application to start up and run, then exit when the task is done\. A [CronJob](https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/) object lets you set up an application to run at a specific hour, minute, day of the month, month, or day of the week, using a structure defined by the Linux [crontab](https://man7.org/linux/man-pages/man5/crontab.5.html) format\. 

#### Making applications accessible from the network<a name="making-applications-accessible-from-the-network"></a>

 With applications often deployed as a set of microservices that moved around to different places, Kubernetes needed a way for those microservices to be able to find each other\. Also, for others to access an application outside of the Kubernetes cluster, Kubernetes needed a way to expose that application on outside addresses and ports\. These networking\-related features are done with Service and Ingress objects, respectively: 
+  **Services** \- Because a Pod can move around to different nodes and addresses, another Pod that needed to communicate with the first Pod could find it difficult to find where it is\. To solve this problem, Kubernetes lets you represent an application as a [Service](https://kubernetes.io/docs/concepts/services-networking/service/)\. With a Service, you can identify a Pod or set of Pods with a particular name, then indicate what port exposes that application’s service from the Pod and what ports another application could use to contact that service\. Another Pod within a cluster can simply request a Service by name and Kubernetes will direct that request to the proper port for an instance of the Pod running that service\. 
+  **Ingress** \- [Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/) is what can make applications represented by Kubernetes Services available to clients that are outside of the cluster\. Basic features of Ingress include a load balancer \(managed by Ingress\), the Ingress controller, and rules for routing requests from the controller to the Service\. There are several [Ingress Controllers](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/) that you can choose from with Kubernetes\. 

## Next steps<a name="next-steps"></a>

 Understanding basic Kubernetes concepts and how they relate to Amazon EKS will help you navigate both the [Amazon EKS documentation](https://docs.aws.amazon.com/eks/) and [Kubernetes documentation](https://kubernetes.io/docs) to find the information you need to manage Amazon EKS clusters and deploy workloads to those clusters\. To begin using Amazon EKS, choose from the following: 
+  [Create a simple cluster](https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html) 
+  [Create a more complex cluster](https://docs.aws.amazon.com/eks/latest/userguide/create-cluster.html) 
+  [Deploy a sample application](https://docs.aws.amazon.com/eks/latest/userguide/sample-deployment.html) 
+  [Explore ways of managing your cluster](https://docs.aws.amazon.com/eks/latest/userguide/eks-managing.html) 